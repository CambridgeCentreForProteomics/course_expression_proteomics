{
  "hash": "2cfe49ac116395a7f6aca04ba80b526a",
  "result": {
    "markdown": "---\ntitle: Statistical analysis\nbibliography: course_refs.bib\neditor_options: \n  chunk_output_type: console\n---\n\n\n::: {.callout-tip}\n#### Learning Objectives\n\n* Acknowledge the availability of different R/Bioconductor packages for carrying out differential expression (abundance) analyses\n* Using the `limma` package, design a statistical model to test for differentially abundant proteins between two or more conditions\n* Be able to interpret the output of a statistical model and annotate the results with user-defined significance thresholds\n* Produce volcano plots to visualise the results of differential expression analyses\n\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](figs/flow_chart/flow_chart.010.png){fig-align='center' width=90%}\n:::\n:::\n\n\n\n\n\n## Differential expression analysis \n\nHaving cleaned our data, aggregated from PSM to protein level, completed a\nlog2 transformation and normalised the data, we are now ready to carry out\nstatistical analysis. The aim of this section is to answer the question:\n*\"Which proteins show a significant change in abundance between cell cycle stages?\"*.\n\nOur null hypothesis is:\n**(H0)** The change in abundance for a protein between cell cycle stages is 0.\n\nOur alternative hypothesis is:\n**(H1)** The change in abundance for a protein between cell cycle stages is greater than 0.\n\n\n## Selecting a statistical test\n\nThere are a few aspects of our data that we need to consider prior to deciding\nwhich statistical test to apply.\n\n* The protein abundances in this data are not normally distributed (i.e., they\ndo not follow a Gaussian distribution). However, they are approximately normal \nfollowing a log2 transformation.\n* The cell cycle is not expected to have a large impact on biological \nvariability. We can assume that the majority of the proteome is the same between \nsamples.\n* The samples are independent not paired. For example, M_1 is not derived from \nthe same cells as G1_1 and DS_1.\n\nThe first two points relate to two key assumptions that are made when carrying out \n**parametric** statistical tests. Parametric tests provide greater statistical \npower but assume that the input data has a normal distribution and equal variance.\nIf these two assumptions are not met then it is not appropriate to use parametric \ntesting. If we wanted to check whether the log2 protein abundances were truly \nGaussian we could have applied a Shapiro-Wilk test or Kolmogorov-Smirnov test.\nSimilarly, to check for equal variance a Levene test could be used. All of these\ntests are easily done within R but will not be covered in this course. Here, we\nwill use a form of t-test since we know that these assumptions have been met.\n\nMany different R packages can be used to carry out differential expression\n(abundance) analysis on proteomics data. Here we will use `limma`, a package\nthat is widely used for omics analysis and has several models that allow\ndifferential abundance to be assessed in multifactorial experiments. For single \ncomparison analyses (i.e., comparing protein abundance between two groups) we \nrecommend `limma`'s **empirical Bayes moderated t-test**. A simple example of\nthe empirical Bayes moderated t-test is provided in @Hutchings2023. For analyses\nconsidering multiple group comparisons an **empirical Bayes moderated ANOVA test**\ncan be applied. Limma will automatically select a t-test or ANOVA test depending\nupon how many groups (i.e., conditions) we pass to our model.\n\n\n### What is a t-test?\n\nA t-test is a parametric statistical test used to compare the mean values of **two**\ngroups, essentially asking whether the difference between means is significantly\ngreater than 0. The output of such a test includes several parameters:\n\n* **t-value** = provides a ratio between the size of the difference between means and the variation between samples within condition. The further away from 0 that a t-value lies, the more likely it is to represent a significant difference between means (large difference between conditions, small variation within conditions)\n* **degrees of freedom** = the number of observations minus the number of independent variables (*n* - 1)\n* **p-value** = the probability of achieving the t-value under the null hypothesis i.e., by chance\n\n\n### What is an Analysis of Variance (ANOVA) test? \n\nAn Analysis of Variance (ANOVA) test is also a parametric test used to compare\nmean values, but ANOVA considers whether means significantly differ between \n**three or more** groups. Essentially, ANOVA tests are the multi-comparison \nequivalent of a t-test. The output of an ANOVA test includes several parameters:\n\n* **F-value** = provides a ratio of the variation between sample means (of different conditions) and the variation within the samples (of a single condition). The higher the F-value, the more likely it is that the protein has a significantly different mean abundance (large variation between means in different conditions, small variation within conditions)\n* **degrees of freedom** = the number of observations minus the number of independent variables (*n* - 1)\n* **p-value** = the probability of achieving the F-value under the null hypothesis i.e., by chance\n\nHere, we will apply an ANOVA test to each of our proteins to ask whether the\ndifference in the mean abundance of a protein between cell cycle stages is \nsignificantly different from 0. We use an ANOVA rather than a t-test because we\nhave three different conditions of interest: cells in M-phase, G1-phase and \ndesynchronised cells.\n\n\n### What does the empirical Bayes part mean?\n\nWhen carrying out high throughput omics experiments we not only have a\npopulation of samples but also a population of features - here we have several\nthousand proteins. Whilst we do not have time to go into detail about this,\nconsidering our parameters of interest (mean, variation and difference in means)\nacross the entire population of proteins provides us with some prior knowledge\nof what to expect. This prior knowledge facilitates empirical Bayesian methods\n(a mixture between frequentist and Bayesian statistics), a much more powerful\napproach than looking at the data one protein at a time.\n\nEssentially, the empirical Bayes method borrows information across features\n(proteins) and shifts the per-protein variance estimates towards an expected\nvalue based on the variance estimates of other proteins with a similar\nabundance. This results in greater statistical power for detecting significant\nabundance changes for proteins with higher variation and reduces the number of\nfalse positives that arise from proteins with small variances (where normally\neven a small abundance change could be considered significant). For more detail\nabout empirical Bayes methods see\n[here](https://online.stat.psu.edu/stat555/node/40/).\n\nOverall, empirical Bayes provides a way to increase the statistical power and\nreduce false positives within our differential abundance analysis.\n\n\n## Defining the statistical model \n\nBefore we apply our empirical Bayes moderated ANOVA test, we first need to set\nup a model. To define the model design we use `model.matrix`. A **model matrix**,\nalso called a design matrix, is a matrix in which rows represent individual\nsamples and columns correspond to explanatory variables, in our case the cell\ncycle stages. Simply put, the model design is determined by how samples are\ndistributed across conditions.\n\n\n### With or without an intercept\n\nWhen investigating the effect of a single explanatory variable, a design matrix\ncan be created using either `model.matrix(~variable)` or `model.matrix(~0 + variable)`.\nThe difference between these two options is that the first will create a model \nthat includes an intercept term whilst the second will exclude any intercept\nterm. If the explantory variable is a factor, both models with and without an\nintercept are equivalent. If, however, the explanatory variable is a continuous\nvariable, the two models are then fundamentally different.\n\nIn this experiment we consider our explanatory variable (cell cycle stage) to \nbe categorical, making it of `factor` class in R. When modelling factor\nexplanatory variables, models with and without an intercept are equivalent. \nEither option is appropriate for factor explanatory variables, but design matrices\nwithout an intercept value tend to be easier to interpret. For further guidance\non generating design matrices for covariates or continuous explanatory \nvariables, see [A guide to creating design matrices for gene expression experiments](https://bioconductor.org/packages/devel/workflows/vignettes/RNAseq123/inst/doc/designmatrices.html#design-and-contrast-matrices).\n\nWe subset our log2 normalised protein level data since this will be the\ninput for our statistical test. For simplicity we also remove the 1st sample, \nwhich was a single pre-treatment control. Without any replicates this condition\ncannot be considered in a statistical framework.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Extract protein list from our QFeatures object - remove sample 1\nall_proteins <- cc_qf[[\"log_norm_proteins\"]][, -1]\n\n## Ensure that conditions are stored as levels of a factor \n## Explicitly define level order by cell cycle stage\nall_proteins$condition <- factor(all_proteins$condition, \n                                 levels = c(\"Desynch\", \"M\", \"G1\"))\n```\n:::\n\n\nNext, we define the model matrix without any intercept term.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Design a matrix containing all factors that we wish to model\nm_design <- model.matrix(~ 0 + all_proteins$condition)\ncolnames(m_design) <- levels(all_proteins$condition)\n\n## Verify\nm_design\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Desynch M G1\n1       0 1  0\n2       0 1  0\n3       0 1  0\n4       0 0  1\n5       0 0  1\n6       0 0  1\n7       1 0  0\n8       1 0  0\n9       1 0  0\nattr(,\"assign\")\n[1] 1 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$`all_proteins$condition`\n[1] \"contr.treatment\"\n```\n:::\n:::\n\n\nWe also define **contrasts**. The contrasts represent which comparisons are of \ninterest to us. This is important since we are not directly interested in the\nparameter (mean) estimates for each group but rather the differences in these \nparameter (mean) estimates *between* groups. The `makeContrasts` function\nis used by passing the name we wish to give each contrast and how this contrast\nshould be calculated using column names from the design matrix. We also pass the\n`levels` argument to tell R where the column names come from i.e., which design\nmatrix the contrasts are being applied to. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Specify contrasts of interest\ncontrasts <- makeContrasts(G1_M = G1 - M, \n                           M_Des = M - Desynch, \n                           G1_Des = G1 - Desynch,\n                           levels = m_design)\n\n## Verify\ncontrasts\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         Contrasts\nLevels    G1_M M_Des G1_Des\n  Desynch    0    -1     -1\n  M         -1     1      0\n  G1         1     0      1\n```\n:::\n:::\n\n\n\n## Running an empirical Bayes moderated test using `limma`\n\nAfter we have specified the design matrix and contrasts we wish to make, the \nnext step is to apply the statistical model. Since we have three conditions, \nthis model will be an ANOVA model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Fit linear model using the design matrix and desired contrasts\nfit_model <- lmFit(object = assay(all_proteins), design = m_design)\nfit_contrasts <- contrasts.fit(fit = fit_model, contrasts = contrasts)\n```\n:::\n\n\nThe initial model has now been applied to each of the proteins in our data. \nWe now update the model using the `eBayes` function. When we do this we include\ntwo other arguments: `trend = TRUE` and `robust = TRUE` @Phipson2016 @Smyth2004.\n\n* `trend` - takes a logical value of `TRUE` or `FALSE` to indicate whether an \nintensity-dependent trend should be allowed for the prior variance (i.e., the \npopulation level variance prior to empirical Bayes moderation). This means that \nwhen the empirical Bayes moderation is applied the protein variances are not \nsqueezed towards a global mean but rather towards an intensity-dependent trend.\n* `robust` - takes a logical value of `TRUE` or `FALSE` to indicate whether the \nparameter estimation of the priors should be robust against outlier sample \nvariances.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Update the model using the limma eBayes algorithm\nfinal_model <- eBayes(fit = fit_contrasts, \n                      trend = TRUE,\n                      robust = TRUE)\n```\n:::\n\n\n\n### Accessing the model results\n\nThe `topTable` function extracts a table of the top-ranked proteins from our \nfitted linear model. By default, `topTable` outputs a table of the top 10 ranked \nproteins, that is the 10 proteins with the highest log-odds of being differentially \nabundant. To get the results for all of our proteins we use the `number = Inf` \nargument. Let's give this a go.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Format results\nlimma_results <- topTable(fit = final_model,   \n                          coef = NULL, \n                          adjust.method = \"BH\",    # Method for multiple hypothesis testing\n                          number = Inf) %>%        # Print results for all proteins\n  rownames_to_column(\"Protein\") \n\n## Verify\nhead(limma_results)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Protein      G1_M      M_Des     G1_Des    AveExpr        F      P.Value\n1  Q9NQW6 -2.049869  0.6975787 -1.3522904 -0.0584327 572.2372 2.541829e-11\n2  Q9ULW0 -2.163739  0.8367536 -1.3269854  0.2986946 550.9383 3.087468e-11\n3  Q15004 -1.511759 -1.2577330 -2.7694921  0.2064893 453.5226 9.273989e-11\n4  P49454 -1.806650  0.9809888 -0.8256617 -0.1204004 409.6874 1.406254e-10\n5  O14965 -2.030033  0.8176069 -1.2124263 -0.1360124 393.9557 1.717643e-10\n6  Q9BW19 -2.273902  1.0139601 -1.2599415 -0.4465866 390.5437 1.795663e-10\n     adj.P.Val\n1 5.904782e-08\n2 5.904782e-08\n3 1.144735e-07\n4 1.144735e-07\n5 1.144735e-07\n6 1.144735e-07\n```\n:::\n:::\n\n\nAs expected, we see the two key parameters of an ANOVA output - the *F-value* \n(`F`) and *p-value* (`P.Value`). We also see an *adjusted p-value* (`adj.P.Val`).\nThis refers to p-value adjustment that must be done following multiple hypothesis\ntesting.\n\n### Multiple hypothesis testing and correction\n\nUsing the linear model defined above, we have carried out many statistical \ncomparisons. We have three comparisons made per protein, of which we have\n3825. \n\nMultiple testing describes the process of separately testing each null\nhypothesis i.e., carrying out many statistical tests at a time each to test a\ndifferent hypothesis. Here we have carried out \n11475 hypothesis tests. If we \nwere to use the typical p < 0.05 significance threshold for each test we would\naccept a 5% chance of incorrectly rejecting the null hypothesis *per test*.\nTherefore, for every 100 tests that we carry out we expect an average of 5 false\npositives.\n\nIf we do not account for the fact that we have carried out multiple hypothesis\nthen we risk including false positives in our data. Many methods exist to\ncorrect for multiple hypothesis testing and these mainly fall into two\ncategories:\n\n1. Control of the Family-Wise Error Rate (FWER)\n2. Control of the False Discovery Rate (FDR)\n\nAbove we used the \"BH\", or Benjamini-Hochberg procedure, to control the FDR.\nThis accounts for multiple hypothesis testing *per protein* and *per contrast*\nto give us an overall view of the data.\n\n::: {.callout-tip}\n#### The False Discovery Rate\nThe False Discovery Rate (FDR) defines the fraction of false discoveries that we\nare willing to tolerate in our list of differential proteins. For example, an\nFDR threshold of 0.05 means that around 5% of the differential proteins will be\nfalse positives. It is up to you to decide what this threshold should be, but\nconventionally people use 0.01 or 0.05.\n:::\n\n### Diagnostic plots to verify suitability of our statistical model\n\nAs with all statistical analysis, it is crucial to do some quality control and\nto check that the statistical test that has been applied was indeed appropriate\nfor the data. As mentioned above, statistical tests typically come with several\nassumptions. To check that these assumptions were met and that our model was\nsuitable, we create some diagnostic plots.\n\nFirst we plot a histogram of the raw p-values (not BH-adjusted p-values).\nThis can be done by passing our results data into standard `ggplot2` plotting\nfunctions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlimma_results %>%\n  as_tibble() %>%\n  ggplot(aes(x = P.Value)) + \n  geom_histogram()\n```\n\n::: {.cell-output-display}\n![](06_statistical_analysis_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nThe histogram we have plotted shows an anti-conservative distribution, which is\ngood. The flat distribution across the bottom corresponds to null p-values which\nare distributed approximately uniformly between 0 and 1. The peak close to 0 \ncontains our significantly changing proteins, a combination of true positives \nand false positives.\n\nOther examples of how a p-value histogram could look are shown below. Whilst in\nsome experiments a uniform p-value distribution may arise due to an absence of\nsignificant alternative hypotheses, other distribution shapes can indicate that\nsomething was wrong with the model design or statistical test. For more detail on\nhow to interpret p-value histograms there is a great [blog](http://varianceexplained.org/statistics/interpreting-pvalue-histogram/) \nby David Robinson.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Examples of p-value histograms.](figs/phist_shapes.png){fig-align='center' width=100%}\n:::\n:::\n\n\nThe second plot that we generate is an SA plot to display the residual standard\ndeviation (sigma) versus log abundance for each protein to which our model was\nfitted. We can use the `plotSA` function to do this.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplotSA(fit = final_model,\n       cex = 0.5,\n       xlab = \"Average log2 abundance\")\n```\n\n::: {.cell-output-display}\n![](06_statistical_analysis_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nIt is recommended that an SA plot be used as a routine diagnostic plot when\napplying a limma-trend pipeline. From the SA plot we can visualise the\nintensity- dependent trend that has been incorporated into our linear model. It\nis important to verify that the trend line fits the data well. If we had not\nincluded the `trend = TRUE` argument in our `eBayes` function, then we would\ninstead see a straight horizontal line that does not follow the trend of the\ndata. Further, the plot also colours any outliers in red. These are the outliers\nthat are only detected and excluded when using the `robust = TRUE` argument.\n\n\n### Interpreting the output of our statistical model\n\nHaving checked that the model we fitted was appropriate for the data, we can now\ntake a look at the results of our test using the `topTable` and `decideTests` \nfunctions.\n\n#### Interpreting the overall ANOVA output\nAs we saw above, `topTable` will give us the overall output of our ANOVA model.\nWe previously used this function to generate our `limma_results` without specifying\nany value for the `coef` argument. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(limma_results)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Protein      G1_M      M_Des     G1_Des    AveExpr        F      P.Value\n1  Q9NQW6 -2.049869  0.6975787 -1.3522904 -0.0584327 572.2372 2.541829e-11\n2  Q9ULW0 -2.163739  0.8367536 -1.3269854  0.2986946 550.9383 3.087468e-11\n3  Q15004 -1.511759 -1.2577330 -2.7694921  0.2064893 453.5226 9.273989e-11\n4  P49454 -1.806650  0.9809888 -0.8256617 -0.1204004 409.6874 1.406254e-10\n5  O14965 -2.030033  0.8176069 -1.2124263 -0.1360124 393.9557 1.717643e-10\n6  Q9BW19 -2.273902  1.0139601 -1.2599415 -0.4465866 390.5437 1.795663e-10\n     adj.P.Val\n1 5.904782e-08\n2 5.904782e-08\n3 1.144735e-07\n4 1.144735e-07\n5 1.144735e-07\n6 1.144735e-07\n```\n:::\n:::\n\n\nInterpreting the output of `topTable` for a multi-contrast model:\n\n* `G1_M`, `M_Des` and `G1_Des` = the estimated log2FC for each model contrast\n* `AveExpr` = the average log abundance of the protein across samples\n* `F` = eBayes moderated F-value. Interpreted in the same way as a normal F-value (see above).\n* `P.Value` = Unadjusted p-value\n* `adj.P.Val` = FDR-adjusted p-value (adjusting across both proteins and contrasts - global adjustment) \n\nWe have used the ANOVA test to ask *\"Does this protein show a significant change in abundance between cell cycle stages?\"*\nfor each protein. \n\nOur null hypothesis is:\n**(H0)** The change in abundance for a protein between cell cycle stages is 0. \nIn other words, the null hypothesis for each protein is that the mean abundance\nin M-phase = the mean abundance in G1-phase = the mean abundance in desynchronised \ncells. So, `G1_M` = `M_Des` = `G1_Des` = 0.\n\nOur alternative hypothesis is:\n**(H1)** The change in abundance for a protein between cell cycle stages is \ngreater than 0. \n\nFrom our output we can see that some of our proteins have high F-values and \nlow adjusted p-values (below any likely threshold of significance). These \nadjusted p-values may tell us that a protein has a significantly different \nabundance across cell cycle stages. However, we cannot tell from this output\nfrom which contrast this significance is derived. Does a protein have a\nsignificantly different abundance between M- and G1-phase, or M-phase and \ndesynchronised cells, or G1-phase and desynchronised cells, or even multiple of\nthese comparisons? \n\n\n#### Interpreting the results of a single contrast\nWe can look at individual contrasts by passing the contrast name to the `coef`\nargument in the `topTable` function. For example, let's look at the pairwise\ncomparison between M-phase and desynchronised cells. We use the `topTable` \nfunction to get the results of the `\"M_Des\"` contrast. We use the argument \n`confint = TRUE` so that the our output reports the 95% confidence interval of\nthe calculated log2FC. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nM_Desynch_results <- topTable(fit = final_model, \n                              coef = \"M_Des\", \n                              number = Inf,\n                              adjust.method = \"BH\",\n                              confint = TRUE) %>%\n  rownames_to_column(\"protein\")\n\n## Verify\nhead(M_Desynch_results)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  protein     logFC       CI.L      CI.R    AveExpr         t      P.Value\n1  P31350 -3.045183 -3.3264149 -2.763952 -0.5756422 -24.03475 2.236285e-10\n2  O60732 -2.058507 -2.2727461 -1.844267  0.3551997 -21.32766 7.478665e-10\n3  O43583 -1.348484 -1.4959908 -1.200977  0.3797734 -20.27672 1.144324e-09\n4  O00622  1.278311  1.1302938  1.426329 -0.3781623  19.15527 2.033050e-09\n5  O43663  1.071216  0.9295235  1.212909  0.1547363  16.76849 7.744863e-09\n6  Q96PU5 -1.580262 -1.7911478 -1.369377 -0.1603462 -16.63308 9.030420e-09\n     adj.P.Val        B\n1 8.553790e-07 14.21630\n2 1.430295e-06 13.15685\n3 1.459014e-06 12.77438\n4 1.944104e-06 12.24499\n5 4.670537e-06 10.98014\n6 4.670537e-06 10.83491\n```\n:::\n:::\n\n\nInterpreting the output of `topTable` for a single contrast:\n\n* `logFC` = the fold change between the mean log abundance in group A and the mean log abundance in group B\n* `CI.L` = the left limit of the 95% confidence interval for the reported log2FC\n* `CI.R` = the right limit of the 95% confidence interval for the reported log2FC\n* `AveExpr` = the average log abundance of the protein across samples\n* `t` = t-statistic derived from the original ANOVA test (not a t-test)\n* `P.Value` = Unadjusted p-value\n* `adj.P.Val` = FDR-adjusted p-value (adjusted across proteins but not multiple contrasts)\n* `B` = B-statistic representing the log-odds that a protein is differentially abundant between conditions\n\nThis time the output of `topTable` contains a t-statistic rather than an F-value.\nThis is because we only told the function to compare two conditions, so the \ncorresponding t-statistic from our ANOVA test is reported. Importantly, however, \nthe p-value adjustment in this case only accounts for multiple tests across our\n3825 proteins, not the three different contrasts/comparisons\nwe used the data for. As a result, we could over-estimate the number of\nstatistically significant proteins within this contrast. although this this \neffect is only likely to become problematic when we have a larger number of \ncontrasts to account for. \n\n\n#### Interpreting the results of all contrasts\nTo understand the impact of adjusting for multiple hypothesis testing across \nour comparisons, we can use the `decideTests` function. This function provides\na matrix of values -1, 0 and +1 to indicate whether a protein is significantly\ndownregulated, unchanged or significantly upregulated in a given contrast. For\nthe function to determine significance we have to provide a p-value adjustment\nmethod and threshold, here we use the standard Benjamini-Hochberg procedure for\nFDR adjustment and set a threshold of `adj.P.Value < 0.01` for significance.\n\nThe `decideTests` function also takes an argument called `method`. This argument\nspecifies whether p-value adjustment should account only for multiple hypothesis\ntests across proteins (`\"separate\"`) or across both proteins and contrasts \n(`\"global\"`). \n\nLet's first look at the results when we apply the `\"separate\"` method i.e., \nconsider each contrast separately.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndecideTests(object = final_model,\n            adjust.method = \"BH\", \n            p.value = 0.01, \n            method = \"separate\") %>%\n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       G1_M M_Des G1_Des\nDown    377   422     70\nNotSig 3177  2930   3722\nUp      271   473     33\n```\n:::\n:::\n\n\nFrom this table we can see the number of significantly changing proteins per\ncontrast. For the `M_Des` comparison the total number of significantly changing\nproteins is 895. This should be the same as the number of proteins\nwith an adjusted p-value < 0.01 in our `M_Desynch_results` object. Let's check.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nM_Desynch_results %>%\n  as_tibble() %>%\n  filter(adj.P.Val < 0.01) %>% \n  nrow()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 895\n```\n:::\n:::\n\n\nHowever, if we use the `\"global\"` method for p-value adjustment and therefore\nadjust for both per protein and per contrast hypotheses we may see slightly\nfewer significant proteins in our `M_Des` contrast.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndecideTests(object = final_model,\n            adjust.method = \"BH\", \n            p.value = 0.01, \n            method = \"global\") %>%\n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       G1_M M_Des G1_Des\nDown    355   373    115\nNotSig 3223  3040   3639\nUp      247   412     71\n```\n:::\n:::\n\n\nUnfortunately, there is no way to specify global p-value adjustment accounting\nfor all contrasts when using `topTable` to look at a single contrast. Instead, \nwe can merge the results from our globally adjusted significance summary \n(generated using `decideTests` with `method = \"global\"`) with the results of \nour overall ANOVA test (generated using `topTable` with `coef = NULL`). We \ndemonstrate how to do this in the code below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Determine global significance using decideTests\nglobal_sig <- as.data.frame(decideTests(object = final_model, \n                          adjust.method = \"BH\", \n                          p.value = 0.01, \n                          method = \"global\")) %>%\n  rownames_to_column(\"protein\")\n\n\n## Change column names to avoid conflict when binding\ncolnames(global_sig) <- paste0(\"sig_\", colnames(global_sig))\n\n## Add the results of global significance test to overall ANOVA results\nlimma_results <- dplyr::left_join(limma_results, global_sig, by = c(\"Protein\" = \"sig_protein\"))\n```\n:::\n\n\nWe now have three additional column, one per contrast, called `sig_G1_M`, `sig_M_Des`,\nand `sig_G1_Des`. These columns contain -1, 0 or 1 meaning that the protein is\nsignificantly downregulated, non-significant or significantly upregulated in the\ngiven contrast.\n\n\n#### Adding user-defined significance thresholds \n\nFor the remainder of this workshop we will consider only the `M_Des` single\ncontrast and use the results of `topTable` with `coef = \"M_Des\"`. We currently\nhave these results stored in an object called `M_Desynch_results`.\n\nThe output of our statistical test will provide us with key information for\neach protein, including its p-value, BH-adjusted p-value (here, across proteins\nbut not contrasts) and logFC. However, it is up to us to decide what we consider \nto be significant. The first parameter to consider is the `adj.P.Val` threshold\nthat we wish to apply - 0.05 and 0.01 are both common in proteomics. The second \nparameter which is sometimes used to define significance is the `logFC`. This is\nmainly for the purpose of deciding on hits to follow-up, since it is easier to \nvalidate larger abundance changes than those which are consistent but subtle. \n\nHere we are going to define significance based on an `adj.P.Val` < 0.01. We can\nadd a column to our results to indicate significance as well as the direction of\nchange. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Add direction and significance information\nM_Desynch_results <- \n  M_Desynch_results %>%\n  as_tibble() %>%\n  mutate(direction = ifelse(logFC > 0, \"up\", \"down\"),\n         significance = ifelse(adj.P.Val < 0.01, \"sig\", \"not.sig\"))\n\n\n## Verify\nhead(M_Desynch_results)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 11\n  protein logFC   CI.L  CI.R AveExpr     t  P.Value   adj.P.Val     B direction\n  <chr>   <dbl>  <dbl> <dbl>   <dbl> <dbl>    <dbl>       <dbl> <dbl> <chr>    \n1 P31350  -3.05 -3.33  -2.76  -0.576 -24.0 2.24e-10 0.000000855  14.2 down     \n2 O60732  -2.06 -2.27  -1.84   0.355 -21.3 7.48e-10 0.00000143   13.2 down     \n3 O43583  -1.35 -1.50  -1.20   0.380 -20.3 1.14e- 9 0.00000146   12.8 down     \n4 O00622   1.28  1.13   1.43  -0.378  19.2 2.03e- 9 0.00000194   12.2 up       \n5 O43663   1.07  0.930  1.21   0.155  16.8 7.74e- 9 0.00000467   11.0 up       \n6 Q96PU5  -1.58 -1.79  -1.37  -0.160 -16.6 9.03e- 9 0.00000467   10.8 down     \n# ℹ 1 more variable: significance <chr>\n```\n:::\n:::\n\n\n\n## Visualising the results of our statistical model\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](figs/flow_chart/flow_chart.011.png){fig-align='center' width=90%}\n:::\n:::\n\n\n\nThe final step in any statistical analysis is to visualise the results. This is\nimportant for ourselves as it allows us to check that the data looks as \nexpected. \n\nThe most common visualisation used to display the results of expression\nproteomics experiments is a volcano plot. This is a scatterplot that shows\nstatistical significance (p-values) against the magnitude of fold change.\nOf note, when we plot the statistical significance we use the raw unadjusted\np-value (`-log10(P.Value)`). This is because it is better to plot the basic\ndata in its raw form than any derived value (the adjusted p-value is derived\nfrom each p-value using the BH-method of correction). The process of FDR\ncorrection can result in some points that previously had distinct p-values \nhaving the same adjusted p-value. Further, different methods of correction will\ngenerate different adjusted p-values, making the comparison and interpretation\nof values more difficult.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nM_Desynch_results %>%\n  ggplot(aes(x = logFC, y = -log10(P.Value), fill = significance)) +\n  geom_point(shape = 21, stroke = 0.25, size = 3) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](06_statistical_analysis_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\nThere are several ways in which we can export this volcano plot from R. One\noption is to use the `ggsave` function immediately after our plotting code. The \ndefault for this function is to save the last plot displayed.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_results %>%\n  ggplot(aes(x = logFC, y = -log10(P.Value), fill = result)) +\n  geom_point(shape = 21, stroke = 0.25, size = 3) +\n  theme_bw()\nggsave(filename = \"volcano_M_vs_Desynch.png\", device = \"png\")\n```\n:::\n\n\n\n:::{.callout-exercise}\n#### Challenge: Volcano plots\n\n{{< level 2 >}}\n\n\nRe-generate your table of results defining significance based on an adjusted \nP-value < 0.05 and a log2 fold-change of > 1.\n\n::: {.callout-answer collapse=true}\n\nFirst let's regenerate the results adding a column for the log2 fold change\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_results <- \n  M_Desynch_results %>%\n  as_tibble() %>%\n  mutate(direction = ifelse(logFC > 0, \"up\", \"down\"),\n         significance = ifelse(adj.P.Val < 0.05, \"sig\", \"not.sig\"),\n         lfc = ifelse(logFC > 1 | logFC < -1, \"sig\", \"not.sig\"),\n         result = ifelse(significance == \"sig\" & lfc == \"sig\", \"sig\", \"not.sig\"))\n```\n:::\n\n\nNow let's plot the results\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_results %>%\n  ggplot(aes(x = logFC, y = -log10(P.Value), fill = result)) +\n  geom_point(shape = 21, stroke = 0.25, size = 3) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](06_statistical_analysis_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\n\n:::\n:::\n\n\n\n\n\n\n\n::: {.callout-tip}\n#### Key Points\n\n- The `limma` package provides a statistical pipeline for the analysis of differential expression (abundance) experiments\n- Empirical Bayes moderation involves borrowing information across proteins to squeeze the per-protein variance estimates towards an expected value based on the behaviour of other proteins with similar abundances. This method increases the statistical power and reduces the number of false positives. \n- Since proteomics data typically shows an intensity-dependent trend, it is recommended to apply empirical Bayes moderation with `trend = TRUE` and `robust = TRUE`. This approach can be validated by plotting an SA plot.\n- Significance thresholds are somewhat arbitary and must be selected by the user. However, correction must be carried out for multiple hypothesis testing so significance thresholds should be based on adjusted p-values rather than raw p-values. Users may also threshold significance based on a log fold-change value too.\n- The results of differential expression and abundance analyses are often summarised on volcano plots.\n:::\n\n\n\n## References {-}\n",
    "supporting": [
      "06_statistical_analysis_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}