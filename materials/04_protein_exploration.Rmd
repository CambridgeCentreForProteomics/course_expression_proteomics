---
title: Exploration and visualisation of protein data
bibliography: course_refs.bib
editor_options: 
  chunk_output_type: console
---

::: {.callout-tip}
#### Learning Objectives

* Know how to determine the number of PSMs, peptides, proteins and protein groups (i.e., master proteins) in an assay of a `QFeatures` object
* Understand what the `.n` column corresponds to when using the `aggregateFeatures` function to aggregate features
* Be able to use the `subsetByFeature` function to get data across all levels for a feature of interest
* Appreciate the use of Principal Component Analysis (PCA) for visualising key factors that contribute to sample variation 
* Complete PCA using the `prcomp` function from the `stats` package

:::


```{r, eval=TRUE, include=FALSE}
library("QFeatures")
library("ggplot2")
library("stringr")
library("dplyr")
library("tibble")
library("NormalyzerDE")
library("corrplot")
library("Biostrings")
library("limma")
library("statmod")
library("org.Hs.eg.db")
library("clusterProfiler")
library("enrichplot")
library("patchwork")
load("output/lesson03.rda", verbose = TRUE)
```


Before we carry out statistical analysis to determine which of our proteins 
show significantly differential abundance across conditions (cell cycle stages),
we first want to do some exploration of the protein level data. This includes
determining some information that may be required for reporting and publication
purposes as well as information corresponding to quality control.


## Determining the dimensions of our final protein data

Given that we started from the PSM level and did extensive data cleaning, 
filtering and management of missing data, it would be useful to know how much 
data we have left. We may want to know how many PSMs, peptides and proteins
the `log_norm_proteins` assay contains, given that this is the data to which
statistical analysis will be applied.

We can easily find the number master proteins by printing our `QFeatures` object

```{r}
cc_qf
```

We can see we have `r nrow(cc_qf[["log_norm_proteins"]])` master proteins, each
representing a protein group.

```{r}
cc_qf[["log_norm_proteins"]] %>%
  nrow()
```


:::{.callout-exercise}
#### Challenge 1: Final PSM, peptide and protein count
{{< level 2 >}}

Determine how many PSMs, peptides and proteins were lost during processing of
the raw data to our final protein list?

::: {.callout-answer collapse=true}

We started with,

```{r}
psm_count <- cc_qf[["psms_raw"]] %>% nrow()

peptide_count <- 
  cc_qf[["psms_raw"]] %>%
  rowData() %>%
  as_tibble() %>%
  pull(Sequence) %>%
  unique() %>%
  length() 

prot_count <- 
  cc_qf[["psms_raw"]] %>%
  rowData() %>%
  as_tibble() %>%
  pull(Master.Protein.Accessions) %>%
  unique() %>%
  length() 

message(psm_count, " PSMs, ", 
        peptide_count, " peptides and ", 
        prot_count, " proteins")
```

After filtering we have,

```{r}
psm_final <- cc_qf[["psms_filtered"]] %>% nrow()

peptide_final <- 
  cc_qf[["psms_filtered"]] %>%
  rowData() %>%
  as_tibble() %>%
  pull(Sequence) %>%
  unique() %>%
  length() 

prot_final <- 
  cc_qf[["psms_filtered"]] %>%
  rowData() %>%
  as_tibble() %>%
  pull(Master.Protein.Accessions) %>%
  unique() %>%
  length() 

message(psm_final, " PSMs, ", 
        peptide_final, " peptides and ", 
        prot_final, " proteins")
```

During the course of data processing we have lost,

```{r}
message(psm_count - psm_final, " PSMs, ", 
        peptide_count - peptide_final, " peptides and ", 
        prot_count - prot_final, " proteins")
```

:::
:::

## The `.n` column created by `aggregateFeatures`

If we look at the names of the columns within our `"log_proteins"` and 
`"log_norm_proteins"` assays we see that there is a column called `.n`. This 
column was not present in the `"log_imputed_psms"`.

```{r}
## Check columns in the log normalised protein assay
cc_qf[["log_norm_proteins"]] %>%
  rowData() %>%
  names()
```

The `.n` column is created during the aggregation process that is completed via
the `aggregateFeatures` function. This column stores information about how many
child features were aggregated into each parent feature. Since we aggregated
directly from the PSMs (child) to protein (parent), the `.n` column tells us how
many PSMs we have in support of each protein in the final dataset.

```{r}
cc_qf[["log_norm_proteins"]] %>%
  rowData() %>%
  as_tibble() %>%
  pull(.n) %>%
  table()
```

```{r, eval=TRUE, include=FALSE}
.support <- 
  cc_qf[["log_norm_proteins"]] %>%
  rowData() %>%
  as_tibble() %>%
  pull(.n) %>%
  table()
```

The output tells us that we have `r .support[1]` proteins with `r names(.support[1])` 
PSMs (single peptide hits), `r .support[2]` proteins with support from 
`r names(.support[2])` PSMs, and so forth.

:::{.callout-exercise}
#### Challenge 2: Visualisation of PSM support
{{< level 2 >}}

1. Using the information we have in the `.n` column create a graph to visualise
PSM support. 

2. What is the maximum number of PSMs we have available for one given protein? 
What is the most common number of PSMs available for supporting a given protein?

::: {.callout-answer collapse=true}

**Task 1: Graph to visualise PSM support**

Several answers,

```{r}
cc_qf[["log_norm_proteins"]] %>%
  rowData() %>%
  as_tibble() %>%
  ggplot(aes(x = .n)) +
  geom_histogram(binwidth = 1)
```

NOTE: Could also visualise with a lollipop or bar plot?

**Task 2: PSM support extremes**

```{r}
cc_qf[["log_norm_proteins"]] %>%
  rowData() %>%
  as_tibble() %>%
  pull(.n) %>%
  table()  
```

From the above output we see the most common number of PSMs available to support
a given protein is 1. Single peptide hits most frequently occur in the data. 

We have one protein which has 413 PSMs for one given protein. 

:::
:::


We can check the sum of the PSMs which contribute to the protein level data
add up to the total number of PSMs used in aggreagtion. We can see from printing
the object summary `cc_qf` we have `r nrow(cc_qf)[["log_imputed_psms]])` PSMs. 

Let's sum the PSMs from `.n`,

```{r}
psms <- cc_qf[["log_norm_proteins"]] %>%
  rowData() %>%
  as_tibble() %>%
  pull(.n) %>%
  sum()
```

NOTE: add challenge back in here where we get users to aggregate PSMs to 
peptides and then get them to verify the number of peptides we have. We need
this assay later on when we subset by feature.


```{r}
cc_qf <- aggregateFeatures(cc_qf,
                           i = "log_imputed_psms",
                           fcol = "Sequence",
                           name = "log_peptides",
                           fun = MsCoreUtils::robustSummary,
                           na.rm = TRUE)

cc_qf <- aggregateFeatures(cc_qf, 
                           i = "log_peptides",
                           fcol = "Master.Protein.Accessions",
                           name = "log_proteins_2",
                           fun = MsCoreUtils::robustSummary,
                           na.rm = TRUE)
```


<!-- **Challenge** -->
<!-- Now we know how many master proteins are in our final dataset and how many PSMs -->
<!-- support the quantification of these proteins. Determine how many peptide  -->
<!-- sequences are in support of the final master proteins. -->

<!-- 1. Use the `aggregateFeatures` function to aggregate from log imputed PSM level to log peptide and then log peptide to log protein -->
<!-- 2. Calculate the sum of the `.n` column in the log peptide and log protein assays - think about what these values mean in each instance -->

<!-- **Below is the solution - change to eval=TRUE, include=FALSE** -->

<!-- ```{r, warning = FALSE} -->
<!-- ## Aggregate from PSM to peptide -->
<!-- cc_qf <- aggregateFeatures(cc_qf,  -->
<!--                            i = "log_imputed_psms",  -->
<!--                            fcol = "Sequence", -->
<!--                            name = "log_peptides", -->
<!--                            fun = MsCoreUtils::robustSummary, -->
<!--                            na.rm = TRUE) -->

<!-- ## Aggregate from peptide to protein -->
<!-- cc_qf <- aggregateFeatures(cc_qf,  -->
<!--                            i = "log_peptides",  -->
<!--                            fcol = "Master.Protein.Accessions", -->
<!--                            name = "log_proteins_2", -->
<!--                            fun = MsCoreUtils::robustSummary, -->
<!--                            na.rm = TRUE) -->

<!-- ## Sum of .n in peptide-level (i.e., total PSMs) -->
<!-- cc_qf[["log_peptides"]] %>% -->
<!--   rowData() %>% -->
<!--   as_tibble() %>% -->
<!--   pull(.n) %>% -->
<!--   sum() -->

<!-- ## Sum of .n in protein level (i.e., total peptides) -->
<!-- cc_qf[["log_proteins_2"]] %>% -->
<!--   rowData() %>% -->
<!--   as_tibble() %>% -->
<!--   pull(.n) %>% -->
<!--   sum() -->
<!-- ``` -->


<!-- **Challenge**  -->
<!-- Sometimes we also want to know the dimensions of the raw data. For example, if  -->
<!-- we want to report how many PSMs or peptides were in support of a protein's -->
<!-- *identification* rather than its *quantitation* then using the raw data is  -->
<!-- more appropriate.  -->

<!-- Look at the names of the `rowData` colums in the `"psms_raw"` assay. Using the -->
<!-- `unique()` function, determine how many PSMs, peptide sequences and master -->
<!-- protein accessions were identified from the raw data. -->

<!-- **Below is the solution! need to hide** -->

<!-- ```{r} -->
<!-- cc_qf[["psms_raw"]] %>%  -->
<!--   nrow() -->

<!-- cc_qf[["psms_raw"]] %>%  -->
<!--   rowData() %>% -->
<!--   as_tibble() %>% -->
<!--   pull(Sequence) %>% -->
<!--   unique() %>% -->
<!--   length() -->

<!-- cc_qf[["psms_raw"]] %>%  -->
<!--   rowData() %>% -->
<!--   as_tibble() %>% -->
<!--   pull(Master.Protein.Accessions) %>% -->
<!--   unique() %>% -->
<!--   length() -->
<!-- ``` -->


## The `subsetByFeature` function

As well as determining the dimensions of our entire dataset, both in its raw
state and its final state, sometimes we may wish to find out information about 
a specific feature e.g., a protein of interest. The `QFeatures` infrastructure
provides a convenient function called `subsetByFeature` to extract all data 
levels corresponding to a particular feature.

The `subsetByFeature` function take a `QFeatures` object as its input and an 
additional argument specifying one or more features of interest. The output is
then a new `QFeatures` object with only data corresponding to the specified 
features.

Let's take a look at O43583.

```{r}
O43583 <- subsetByFeature(cc_qf, "O43583")

O43583
```

From this we can see that the O43583 protein is supported by 4 peptides derived
from 5 PSMs.

We can use our new `QFeatures` object to create a plot which displays how the
PSM data was aggregated to protein for this particular feature. To do so, we
extract the assays of interest from our `"O43583"` `QFeatures` object and pass
to the `longFormat` function which will covert the subset `QFeatures` object
to a long format `DataFrame`. We can then use the standard `ggplot2` functions
to visualise the processing of this protein.

```{r, warning = FALSE}
O43583[, , c("log_imputed_psms", "log_peptides", "log_proteins")] %>%
  longFormat() %>%
  as_tibble() %>%
  mutate(assay_order = factor(assay, 
                              levels = c("log_imputed_psms", "log_peptides", "log_proteins"))) %>%
  ggplot(aes(x = colname, y = value, colour = assay)) + 
  geom_point() +
  geom_line(aes(group = rowname)) +
  facet_wrap(~assay_order)
```

Other useful functions that we do not have time to cover today include
`subsetByAssay`, `subsetByColData`, `subsetByColumn`, `subsetByFilter`, 
`subsetByRow`, `subsetByOverlap`, and many more. To find out more about these 
functions you can execute a single question mark (`?`) followed by the function name. 
If you have the `QFeatures` package installed you should be able to access a help
and information page for the function of interest.

For example: 

```{r, eval = FALSE}
?subsetByAssay
```


## Principal Component Analysis (PCA)

The final protein level exploration that we will do is Principal Component
Analysis (PCA).

PCA is a statistical method that can be applied to condense complex data from
large data tables into a smaller set of summary indices, termed principal 
components. This process of dimensionality reduction makes it easier to 
understand the variation observed in a dataset, both how much variation there is
and what the primary factors driving the variation are. This is particularly 
important for multivariate datasets in which experimental factors can contribute
differentially or cumulatively to variation in the observed data. PCA allows us
to observe any trends, clusters and outliers within the data thereby helping to
uncover the relationships between observations and variables.


### PCA uses eigendecomposition - eigenvalues and eigenvectors

Eigendecomposition is a concept in linear algebra whereby a data matrix is 
represented in terms of **eigenvalues** and **eigenvectors**. 




### Completing PCA with `prcomp`

To carry out PCA on our data we will use the `prcomp` function from the `stats`
package. We first extract the quantitative matrix (assay) corresponding to the
log normalised protein level data. To make this matrix compatible with `prcomp`
we also need to transpose the data such that the samples become rows and 
proteins become columns. This is easily achieved using the `t` function. 

Our protein data does not contain due to the decision to impute. However, if
there were any missing values in the data, these would need to be removed using 
`filterNA` to facilitate compatibility with PCA.


#### Scaling and centering

When we *scale and center* a dataset we are centering a variable to a mean of 0
and scaling its standard deviation to 1. It is generally advisable to both center
and scale the data. 


```{r}
protein_pca <- cc_qf[["log_norm_proteins"]] %>%
  assay() %>%
# filterNA() %>%
  t() %>%
  prcomp(scale = TRUE, center = TRUE)

summary(protein_pca)
```


We now have a simplified representation of our quantitative data in the form
of principle components (PC). The `prcomp` function outputs a list of 5 different
information sources, each of which can be accessed using the `$` sign nomenclature.

1. `sdev` - holds the standard deviation values for each of the principle components
2. `rotation` - a matrix which contains each of our proteins as a row and the corresponding PC values as columns
3. `x` - a matrix which contains each of our samples as a row and the corresponding PC values as columns
4. `center` - if `center = TRUE` then contains the centering values, otherwise `FALSE`
5. `scale` - if `scale = TRUE` then contains the scaling values, otherwise `FALSE`

We now want to plot each of our samples in PCA space. To do this we will use the
`protein_pca$x` data.

```{r}
head(protein_pca$x)
```

Typically a 2D PCA plot will display PC1 and PC2, since these are the PCs that
explain the most variation within the dataset.

```{r}
protein_pca$x %>%
  as_tibble() %>%
  ggplot(aes(x = PC1, y = PC2)) +
  geom_point(size = 3) + 
  theme_bw()
```

It is generally advisable to colour each point based on all possible explanatory
variables that may have contributed to the observed variation. In our case we
only have one - the cell cycle stage. To colour the points based on this condition
we can use the tidyverse `mutate` function to add a column defining the condition
of each sample and then use `colour = condition` within our ggplot aesthetics.


```{r}
protein_pca$x %>%
  as_tibble() %>%
  mutate(condition = cc_qf[["log_norm_proteins"]]$condition) %>%
  ggplot(aes(x = PC1, y = PC2, colour = condition)) +
  geom_point(size = 3) + 
  theme_bw()
```

This PCA plot shows clear clustering of samples based on their condition, which
is what we would hope to see. This indicates that the observed variation could 
indeed be explained by cell cycle stage.

For more complicated multivariate experiments all possible explanatory 
variables should be visualised. For example, if multiple batches of samples 
have been prepared separately or several TMTplexes were used, these factors
should be visualised (e.g., by colour) on the PCA plot to see whether they 
are contributing the the observed variation. If the samples do cluster based on
unwanted factors such as batch or TMTplex, additional normalisation may be 
required.


::: {.callout-tip}
#### Key Points

- The `.n` column created by `aggregateFeatures` is a useful way to trace how many child features have been aggregated into a single parent feature
- The `subsetByFeature` function can be used to generate a `QFeatures` object with all levels of data corresponding to one or more features of interest
- Principal Component Analysis (PCA) is a dimensionality reduction method that can be used to visualise the relationship between explanatory variables and observed data. If samples cluster together based on a particular factor, this indicates that the factor

:::

## References {-}