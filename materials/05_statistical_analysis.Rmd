---
title: Statistical analysis
---

::: {.callout-tip}
#### Learning Objectives

* Acknowledge the availability of different R/Bioconductor packages for carrying out differential expression (abundance) analyses
* Using the `Limma` package, design a statistical model to test for differentially abundant proteins between two conditions
* Be able to interpret the output of a statistical model and annotate the results with user-defined significance thresholds
* Produce volcano plots and MA plots to visualise the results of differential expression analyses
* Export key results from R using `write_csv` and figures using `ggsave`

:::

```{r, eval=TRUE, include=FALSE}
library("QFeatures")
library("ggplot2")
library("stringr")
library("dplyr")
library("tibble")
library("NormalyzerDE")
library("corrplot")
library("Biostrings")
library("limma")
library("statmod")
library("org.Hs.eg.db")
library("clusterProfiler")
library("enrichplot")
load("output/lesson03.rda", verbose = TRUE)
```


## Differential expression analysis 

Having cleaned our data, aggregated from PSM to protein level, completed a
log2 transformation and normalised the data, we are now ready to carry out
statistical analysis. The aim of this section is to answer the question:
*"Which proteins show a significant change in abundance between cell cycle stages?"*.

Our null hypothesis is:
**(H0)** The change in abundance for a protein between cell cycle stages is 0.

Our alternative hypothesis is:
**(H1)** The change in abundance for a protein between cell cycle stages is greater than 0.


## Selecting a statistical test

There are a few aspects of our data that we need to consider prior to deciding
which statistical test to apply.

* The protein abundances in this data are not normally distributed (i.e., they do not follow a Gaussian distribution). However, they are approximately normal following a log2 transformation.
* The cell cycle is not expected to have a large impact on biological variability. We can assume that the majority of the proteome is the same between samples.
* The samples are independent not paired. For example, M_1 is not derived from the same cells as G1_1 and DS_1.

The first two points relate to two key assumptions that are made when carrying out 
**parametric** statistical tests. Parametric tests provide greater statistical 
power but assume that the input data has a normal distribution and equal variance.
If these two assumptions are not met then it is not appropriate to use parametric 
testing. If we wanted to check whether the log2 protein abundances were truly 
Gaussian we could have applied a Shapiro-Wilk test or Kolmogorov-Smirnov test.
Similarly, to check for equal variance a Levene test could be used. All of these
tests are easily done within R but will not be covered in this course. Here, we
will use a form of t-test since we know that these assumptions have been met.

Many different R packages can be used to carry out differential expression 
(abundance) analysis on proteomics data. Here we will use `limma`, a package that
is widely used for omics analysis and has several models that allow differential
abundance to be assessed in multifactorial experiments. Specifically, we are 
going to apply `limma`'s **empirical Bayes moderated t-test**.


### What is a t-test?

A t-test is a parametric statistical test used to compare the mean values of two
groups, essentially asking whether the difference between means is significantly
greater than 0. The output of such a test includes several parameters:

* **t-value** = provides a ratio between the size of the difference between means and the variation between samples within condition. The further away from 0 that a t-value lies, the more likely it is to represent a significant difference between means (large difference between conditions, small variation within conditions)
* **degrees of freedom** = the number of observations minus the number of independent variables (*n* - 1)
* **p-value** = the probability of achieving the t-value under the null hypothesis i.e., by chance

Here, we will apply a  t-test to each of our proteins to ask whether the
difference in the mean abundance of a protein between cell cycle stages is 
significantly different from 0.


### What does the empirical Bayes part mean?

When carrying out high throughput omics experiments we not only have a population
of samples but also a population of features - here we have several thousand 
proteins. Whilst we do not have time to go into detail about this, considering our 
parameters of interest (mean, variation and difference in means) across the
entire population of proteins provides us with some prior knowledge of what to 
expect. This prior knowledge facilitates empirical Bayesian methods (a mixture
between frequentist and Bayesian statistics), a much more powerful approach than
looking at the data one protein at a time.

Essentially, the empirical Bayes method borrows information across features 
(proteins) and shifts the per-protein variance estimates towards an expected
value based on the variance estimates of other proteins with a similar abundance.
This results in greater statistical power for detecting significant abundance
changes for proteins with higher variation and reduces the number of false 
positives that arise from proteins with small variances (where normally even a
small abundance change could be considered significant). For more detail 
about empirical Bayes methods see [here](https://online.stat.psu.edu/stat555/node/40/).

Overall, empirical Bayes provides a way to increase the statistical power and
reduce false positives within our differential abundance analysis.


## Defining the statistical model 

Before we apply our empirical Bayes moderated t-test, we first need to set up a 
model. To define the model design we use `model.matrix`. A **model matrix**, also
called a design matrix, is a matrix in which rows represent individual samples and 
columns correspond to explanatory variables, in our case the cell cycle stages.
Simply put, the model design is determined by how samples are distributed across
conditions. 


### With or without an intercept

When investigating the effect of a single explanatory variable, a design matrix
can be created using either `model.matrix(~variable)` or `model.matrix(~0 + variable)`.
The difference between these two options is that the first will create a model 
that includes an intercept term whilst the second will exclude any intercept
term. If the explantory variable is a factor, both models with and without an
intercept are equivalent. If, however, the explanatory variable is a covariate, 
the two models are then fundamentally different. 

In this experiment we consider our explanatory variable (cell cycle stage) to 
be categorical, making it of `factor` class in R. When modelling factor
explanatory variables, models with and without an intercept are equivalent. 
Either option is appropriate for factor explanatory variables, but design matrices
without an intercept value tend to be easier to interpret. For further guidance
on generating design matrices for covariates or continuous explanatory 
variables, see [A guide to creating design matrices for gene expression experiments](https://bioconductor.org/packages/devel/workflows/vignettes/RNAseq123/inst/doc/designmatrices.html#design-and-contrast-matrices).

We subset our log2 normalised protein level data since this will be the
input for our statistical test. For simplicity we also remove the 1st sample, 
which was a single pre-treatment control. Without any replicates this condition
cannot be considered in a statistical framework.

```{r}
## Extract protein list from our QFeatures object - remove sample 1
all_proteins <- cc_qf[["log_norm_proteins"]][, -1]

## Ensure that conditions are stored as levels of a factor - check level order
all_proteins$condition <- factor(all_proteins$condition, levels = c("Desynch",
                                                                    "M",
                                                                    "G1"))

```

Next, we define the model matrix without any intercept term.

```{r}
## Design a matrix containing all factors that we wish to model
m_design <- model.matrix(~ 0 + all_proteins$condition)
colnames(m_design) <- levels(all_proteins$condition)

## Verify
m_design
```

We also define **contrasts**. The contrasts represent which comparisons or 
questions are of interest to us. This is importance since we are not directly 
interested in the parameter (mean) estimates for each group but rather the differences
in these parameter (mean) estimates between groups. The `makeContrasts` function
is used by passing the name we wish to give each contrast and how this contrast
should be calculated using column names from the design matrix. We also pass the
`levels` argument to tell R where the column names come from i.e., which design
matrix the contrasts are being applied to.


```{r}
## Specify contrasts of interest
contrasts <- makeContrasts(M_G1 = G1 - M, 
                           M_Des = Desynch - M, 
                           G1_Des = Desynch - G1,
                           levels = m_design)

## Verify
contrasts
```


## Running an empirical Bayes moderated t-test using `Limma`

After we have specified the design matrix and contrasts we wish to make, the 
next step is to apply the statistical model. 

```{r}
## Fit linear model using the design matrix and desired contrasts
fit_model <- lmFit(assay(all_proteins), m_design)
fit_contrasts <- contrasts.fit(fit_model, contrasts)
```

The initial model has now been applied to each of the proteins in our data. 
We now update the model using the `eBayes` function. When we do this we include
two other arguments: `trend = TRUE` and `robust = TRUE` @Phipson2016 @Smyth2004.

* `trend` - takes a logical value of `TRUE` or `FALSE` to indicate whether an intensity-dependent trend should be allowed for the prior variance (i.e., the population level variance prior to empirical Bayes moderation). This means that when the empirical Bayes moderation is applied the protein variances are not squeezed towards a global mean but rather towards an intensity-dependent trend.
* `robust` - takes a logical value of `TRUE` or `FALSE` to indicate whether the parameter estimation of the priors should be robust against outlier sample variances.


```{r}
## Update the model using the limma eBayes algorithm
fit_contrasts <- eBayes(fit_contrasts, 
                        trend = TRUE,
                        robust = TRUE)
```

Finally, we can take a look out the output of our differential abundance testing.
The `topTable` function allows us to look at the results of our linear model. 
This includes a result of the test for each individual protein.

```{r}
## Format results
limma_results <- topTable(fit_contrasts,
                          adjust.method = "BH",
                          number = Inf) %>%
  rownames_to_column("Protein") 

## Verify
head(limma_results)
```

### Multiple hypothesis testing and correction

Here we see another argument appear, the `adjust.method = "BH`. This refers to 
p-value adjustment that must be done following multiple hypothesis testing. 

Multiple testing describes the process of separately testing each null hypothesis
i.e., carrying out many statistical tests at a time each to test a different
hypothesis. Here we have carried out `r nrow(limma_results)` hypothesis tests.
If we were to use the typical p < 0.05 significance threshold for each test we 
would accept a 5% chance of incorrectly rejecting the null hypothesis *per test*.
Therefore, for every 100 tests that we carry out we expect an average of 5 false
positives.

If we do not account for the fact that we have carried out multiple hypothesis
then we risk including false positives in our data. Many methods exist to 
correct for multiple hypothesis testing and these mainly fall into two categories:

1. Control of the Family-Wise Error Rate (FWER)
2. Control of the False Discovery Rate (FDR)

Above we used the "BH", or Benjamini-Hochberg procedure, to control the FDR. We
will later use the FDR-adjusted p-value to define statistical significance in 
our data.


### Diagnostic plots to verify suitability of our statistical model

As with all statistical analysis, it is crucial to do some quality control and
to check that the statistical test that has been applied was indeed appropriate
for the data. As mentioned above, statistical tests typically come with several
assumptions. To check that these assumptions were met and that our model was
suitable, we create some diagnostic plots.

First we plot a histogram of the raw p-values (not the BH-adjusted p-values). 
This can be done by passing our results data into standard `ggplot2` plotting
functions.

```{r}
limma_results %>%
  as_tibble() %>%
  ggplot(aes(x = P.Value)) + 
  geom_histogram()
```

The histogram we have plotted shows an anti-conservative distribution, which is
good. The flat distribution across the bottom corresponds to null p-values which
are distributed approximately uniformly between 0 and 1. The peak close to 0 
contains our significantly changing proteins, a combination of true positives 
and false positives.

Other examples of how a p-value histogram could look are shown below. Whilst in
some experiments a uniform p-value distribution may arise due to an absence of
significant alternative hypotheses, other distribution shapes can indicate that
something was wrong with the model design or statistical test. For more detail on
how to interpret p-value histograms there is a great [blog](http://varianceexplained.org/statistics/interpreting-pvalue-histogram/) 
by David Robinson.


```{r p-value_hist, echo = FALSE, fig.cap = "Examples of p-value histograms.", fig.align = "center", out.width = "100%"}
knitr::include_graphics("figs/phist_shapes.png", error = FALSE)
```






```{r}
plotSA(fit_contrasts,
       cex = 0.5,
       xlab = "Average log2 abundance")
```

* p-value histogram 
* SA plot


### Interpreting the output of our statisical model

Having checked that the model we fitted was appropriate for the data, we can
now take a look at the results of our test.

```{r}
decideTests(fit_contrasts, adjust.method = "BH", p.value = 0.01) %>%
  summary()
```


For the remainder of the workshop we will focus on the statistical comparison 
between M-phase and the desynchronised cells. We use the `topTable` function to
get the results of the `"M_Des"` contrast.


```{r}
M_Desynch_results <- topTable(fit_contrasts, coef = "M_Des", number = Inf, adjust.method = "BH") %>%
  rownames_to_column("protein")

## Verify
head(M_Desynch_results)
```

Interpreting the output of `topTable`:

* `logFC` = the fold change between the mean log abundance in group A and the mean log abundance in group B
* `AveExpr` = the average log abundance of the protein across samples
* `t` = eBayes moderated t-statistic. Interpreted in the same way as a normal t-statistic (see above).
* `P.Value` = Unadjusted p-value
* `adj.P.Val` = FDR-adjusted p-value 
* `B` = B-statistic representing the log-odds that a protein is differentially abundant between conditions


#### Adding user-defined significance thresholds 

The output of out statistical test will provide us with key information for
each protein, including its p-value, BH-adjusted p-value and logFC. However, it
is up to us to decide what we consider to be significant. The first parameter 
to consider is the `adj.P.Val` threshold that we wish to apply - 0.05 and 0.01 
are both common in proteomics. The second parameter which is sometimes used to
define significance is the `logFC`. This is mainly for the purpose of deciding
on hits to follow-up, since it is easier to validate larger abundance changes
than those which are consistent but subtle. 

Here we are going to define significance based on an `adj.P.Val` < 0.01. We can
add a column to our results to indicate significance as well as the direction of
change. 


```{r}
## Add direction and significance information
M_Desynch_results <- M_Desynch_results %>%
  as_tibble() %>%
  mutate(direction = ifelse(logFC > 0, "up", "down"),
        significance = ifelse(adj.P.Val < 0.01, "sig", "not.sig"))


## Verify
head(M_Desynch_results)
```

```{r, include=FALSE}
## Save results
save(M_Desynch_results, all_proteins, file = "output/lesson05.rda")
```


### Visualising the results of our statistical model

The final step in any statistical analysis is to visualise the results. This is
important for ourselves as it allows us to check that the data looks as 
expected. 

The most common visualisation used to display the results of expression
proteomics experiments is a volcano plot. This is a scatterplot that shows
statistical significance (p-values) against the magnitude of fold change.
Of note, when we plot the statistical significance we use the raw unadjusted
p-value (`-log10(P.Value)`). This is because it is better to plot the basic
data in its raw form than any derived value (the adjusted p-value is derived
from each p-value using the BH-method of correction). The process of FDR
correction can result in some points that previously had distinct p-values 
having the same adjusted p-value. Further, different methods of correction will
generate different adjusted p-values, making the comparison and interpretation
of values more difficult.


```{r}
M_Desynch_results %>%
  ggplot(aes(x = logFC, y = -log10(P.Value), fill = significance)) +
  geom_point(shape = 21, stroke = 0.25, size = 3) +
  theme_bw()
```


::: {.callout-tip}
#### Key Points

- Last section of the page is a bulleted summary of the key points
:::
