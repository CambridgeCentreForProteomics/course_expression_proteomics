---
title: Statistical analysis
bibliography: course_refs.bib
editor_options: 
  chunk_output_type: console
---

::: {.callout-tip}
#### Learning Objectives

* Acknowledge the availability of different R/Bioconductor packages for carrying out differential expression (abundance) analyses
* Using the `limma` package, design a statistical model to test for differentially abundant proteins between two or more conditions
* Be able to interpret the output of a statistical model and annotate the results with user-defined significance thresholds
* Produce volcano plots to visualise the results of differential expression analyses

:::

```{r, eval=TRUE, include=FALSE}
library("QFeatures")
library("ggplot2")
library("stringr")
library("dplyr")
library("tibble")
library("NormalyzerDE")
library("corrplot")
library("Biostrings")
library("limma")
library("statmod")
library("org.Hs.eg.db")
library("clusterProfiler")
library("enrichplot")
load("output/lesson_6.rda", verbose = TRUE)
```


## Differential expression analysis 

Having cleaned our data, aggregated from PSM to protein level, completed a
log2 transformation and normalised the data, we are now ready to carry out
statistical analysis. The aim of this section is to answer the question:
*"Which proteins show a significant change in abundance between cell cycle stages?"*.

Our null hypothesis is:
**(H0)** The change in abundance for a protein between cell cycle stages is 0.

Our alternative hypothesis is:
**(H1)** The change in abundance for a protein between cell cycle stages is greater than 0.


## Selecting a statistical test

There are a few aspects of our data that we need to consider prior to deciding
which statistical test to apply.

* The protein abundances in this data are not normally distributed (i.e., they
do not follow a Gaussian distribution). However, they are approximately normal 
following a log2 transformation.
* The cell cycle is not expected to have a large impact on biological 
variability. We can assume that the majority of the proteome is the same between 
samples.
* The samples are independent not paired. For example, M_1 is not derived from 
the same cells as G1_1 and DS_1.

The first two points relate to two key assumptions that are made when carrying out 
**parametric** statistical tests. Parametric tests provide greater statistical 
power but assume that the input data has a normal distribution and equal variance.
If these two assumptions are not met then it is not appropriate to use parametric 
testing. If we wanted to check whether the log2 protein abundances were truly 
Gaussian we could have applied a Shapiro-Wilk test or Kolmogorov-Smirnov test.
Similarly, to check for equal variance a Levene test could be used. All of these
tests are easily done within R but will not be covered in this course. Here, we
will use a form of t-test since we know that these assumptions have been met.

Many different R packages can be used to carry out differential expression
(abundance) analysis on proteomics data. Here we will use `limma`, a package
that is widely used for omics analysis and has several models that allow
differential abundance to be assessed in multifactorial experiments. For single 
comparison analyses (i.e., comparing protein abundance between two groups) we 
recommend `limma`'s **empirical Bayes moderated t-test**. A simple example of
the empirical Bayes moderated t-test is provided in @Hutchings2023. For analyses
considering multiple group comparisons an **empirical Bayes moderated ANOVA test**
can be applied. Limma will automatically select a t-test or ANOVA test depending
upon how many groups (i.e., conditions) we pass to our model.


### What is a t-test?

A t-test is a parametric statistical test used to compare the mean values of **two**
groups, essentially asking whether the difference between means is significantly
greater than 0. The output of such a test includes several parameters:

* **t-value** = provides a ratio between the size of the difference between means and the variation between samples within condition. The further away from 0 that a t-value lies, the more likely it is to represent a significant difference between means (large difference between conditions, small variation within conditions)
* **degrees of freedom** = the number of observations minus the number of independent variables (*n* - 1)
* **p-value** = the probability of achieving the t-value under the null hypothesis i.e., by chance


### What is an Analysis of Variance (ANOVA) test? 

An Analysis of Variance (ANOVA) test is also a parametric test used to compare
mean values, but ANOVA considers whether means significantly differ between 
**three or more** groups. Essentially, ANOVA tests are the multi-comparison 
equivalent of a t-test. The output of an ANOVA test includes several parameters:

* **F-value** = provides a ratio of the variation between sample means (of different conditions) and the variation within the samples (of a single condition). The higher the F-value, the more likely it is that the protein has a significantly different mean abundance (large variation between means in different conditions, small variation within conditions)
* **degrees of freedom** = the number of observations minus the number of independent variables (*n* - 1)
* **p-value** = the probability of achieving the F-value under the null hypothesis i.e., by chance

Here, we will apply an ANOVA test to each of our proteins to ask whether the
difference in the mean abundance of a protein between cell cycle stages is 
significantly different from 0. We use an ANOVA rather than a t-test because we
have three different conditions of interest: cells in M-phase, G1-phase and 
desynchronised cells.


### What does the empirical Bayes part mean?

When carrying out high throughput omics experiments we not only have a
population of samples but also a population of features - here we have several
thousand proteins. Whilst we do not have time to go into detail about this,
considering our parameters of interest (mean, variation and difference in means)
across the entire population of proteins provides us with some prior knowledge
of what to expect. This prior knowledge facilitates empirical Bayesian methods
(a mixture between frequentist and Bayesian statistics), a much more powerful
approach than looking at the data one protein at a time.

Essentially, the empirical Bayes method borrows information across features
(proteins) and shifts the per-protein variance estimates towards an expected
value based on the variance estimates of other proteins with a similar
abundance. This results in greater statistical power for detecting significant
abundance changes for proteins with higher variation and reduces the number of
false positives that arise from proteins with small variances (where normally
even a small abundance change could be considered significant). For more detail
about empirical Bayes methods see
[here](https://online.stat.psu.edu/stat555/node/40/).

Overall, empirical Bayes provides a way to increase the statistical power and
reduce false positives within our differential abundance analysis.


## Defining the statistical model 

Before we apply our empirical Bayes moderated ANOVA test, we first need to set
up a model. To define the model design we use `model.matrix`. A **model matrix**,
also called a design matrix, is a matrix in which rows represent individual
samples and columns correspond to explanatory variables, in our case the cell
cycle stages. Simply put, the model design is determined by how samples are
distributed across conditions.


### With or without an intercept

When investigating the effect of a single explanatory variable, a design matrix
can be created using either `model.matrix(~variable)` or `model.matrix(~0 + variable)`.
The difference between these two options is that the first will create a model 
that includes an intercept term whilst the second will exclude any intercept
term. If the explantory variable is a factor, both models with and without an
intercept are equivalent. If, however, the explanatory variable is a covariate, 
the two models are then fundamentally different. 

In this experiment we consider our explanatory variable (cell cycle stage) to 
be categorical, making it of `factor` class in R. When modelling factor
explanatory variables, models with and without an intercept are equivalent. 
Either option is appropriate for factor explanatory variables, but design matrices
without an intercept value tend to be easier to interpret. For further guidance
on generating design matrices for covariates or continuous explanatory 
variables, see [A guide to creating design matrices for gene expression experiments](https://bioconductor.org/packages/devel/workflows/vignettes/RNAseq123/inst/doc/designmatrices.html#design-and-contrast-matrices).

We subset our log2 normalised protein level data since this will be the
input for our statistical test. For simplicity we also remove the 1st sample, 
which was a single pre-treatment control. Without any replicates this condition
cannot be considered in a statistical framework.

```{r}
## Extract protein list from our QFeatures object - remove sample 1
all_proteins <- cc_qf[["log_norm_proteins"]][, -1]

## Ensure that conditions are stored as levels of a factor 
## Explicitly define level order by cell cycle stage
all_proteins$condition <- factor(all_proteins$condition, 
                                 levels = c("Desynch", "M", "G1"))

```

Next, we define the model matrix without any intercept term.

```{r}
## Design a matrix containing all factors that we wish to model
m_design <- model.matrix(~ 0 + all_proteins$condition)
colnames(m_design) <- levels(all_proteins$condition)

## Verify
m_design
```

We also define **contrasts**. The contrasts represent which comparisons are of 
interest to us. This is important since we are not directly interested in the
parameter (mean) estimates for each group but rather the differences in these 
parameter (mean) estimates *between* groups. The `makeContrasts` function
is used by passing the name we wish to give each contrast and how this contrast
should be calculated using column names from the design matrix. We also pass the
`levels` argument to tell R where the column names come from i.e., which design
matrix the contrasts are being applied to. 


```{r}
## Specify contrasts of interest
contrasts <- makeContrasts(M_G1 = G1 - M, 
                           M_Des = M - Desynch, 
                           G1_Des = G1 - Desynch,
                           levels = m_design)

## Verify
contrasts
```


## Running an empirical Bayes moderated test using `limma`

After we have specified the design matrix and contrasts we wish to make, the 
next step is to apply the statistical model. Since we have three conditions, 
this model will be an ANOVA model.

```{r}
## Fit linear model using the design matrix and desired contrasts
fit_model <- lmFit(object = assay(all_proteins), design = m_design)
fit_contrasts <- contrasts.fit(fit = fit_model, contrasts = contrasts)
```

The initial model has now been applied to each of the proteins in our data. 
We now update the model using the `eBayes` function. When we do this we include
two other arguments: `trend = TRUE` and `robust = TRUE` @Phipson2016 @Smyth2004.

* `trend` - takes a logical value of `TRUE` or `FALSE` to indicate whether an 
intensity-dependent trend should be allowed for the prior variance (i.e., the 
population level variance prior to empirical Bayes moderation). This means that 
when the empirical Bayes moderation is applied the protein variances are not 
squeezed towards a global mean but rather towards an intensity-dependent trend.
* `robust` - takes a logical value of `TRUE` or `FALSE` to indicate whether the 
parameter estimation of the priors should be robust against outlier sample 
variances.


```{r}
## Update the model using the limma eBayes algorithm
final_model <- eBayes(fit = fit_contrasts, 
                      trend = TRUE,
                      robust = TRUE)
```


### Accessing the model results

The `topTable` function extracts a table of the top-ranked proteins from our 
fitted linear model. By default, `topTable` outputs a table of the top 10 ranked 
proteins, that is the 10 proteins with the highest log-odds of being differentially 
abundant. To get the results for all of our proteins we use the `number = Inf` 
argument. Let's give this a go.


```{r}
## Format results
limma_results <- topTable(fit = final_model,   
                          coef = NULL, 
                          adjust.method = "BH",    # Method for multiple hypothesis testing
                          number = Inf) %>%        # Print results for all proteins
  rownames_to_column("Protein") 

## Verify
head(limma_results)
```

As expected, we see the two key parameters of an ANOVA output - the *F-value* 
(`F`) and *p-value* (`P.Value`). We also see an *adjusted p-value* (`adj.P.Val`).
This refers to p-value adjustment that must be done following multiple hypothesis
testing.

### Multiple hypothesis testing and correction

Using the linear model defined above, we have carried out many statistical 
comparisons. We have three comparisons made per protein, of which we have
`r nrow(limma_results)`. 

Multiple testing describes the process of separately testing each null
hypothesis i.e., carrying out many statistical tests at a time each to test a
different hypothesis. Here we have carried out `r nrow(limma_results) * 3`
hypothesis tests. If we were to use the typical p < 0.05 significance threshold
for each test we would accept a 5% chance of incorrectly rejecting the null
hypothesis *per test*. Therefore, for every 100 tests that we carry out we
expect an average of 5 false positives.

If we do not account for the fact that we have carried out multiple hypothesis
then we risk including false positives in our data. Many methods exist to
correct for multiple hypothesis testing and these mainly fall into two
categories:

1. Control of the Family-Wise Error Rate (FWER)
2. Control of the False Discovery Rate (FDR)

Above we used the "BH", or Benjamini-Hochberg procedure, to control the FDR.
This accounts for multiple hypothesis testing *per protein* and *per contrast*
to give us an overall view of the data.



### Diagnostic plots to verify suitability of our statistical model

As with all statistical analysis, it is crucial to do some quality control and
to check that the statistical test that has been applied was indeed appropriate
for the data. As mentioned above, statistical tests typically come with several
assumptions. To check that these assumptions were met and that our model was
suitable, we create some diagnostic plots.

First we plot a histogram of the raw p-values (not BH-adjusted p-values).
This can be done by passing our results data into standard `ggplot2` plotting
functions.

```{r, message=FALSE, warning=FALSE}
limma_results %>%
  as_tibble() %>%
  ggplot(aes(x = P.Value)) + 
  geom_histogram()
```

The histogram we have plotted shows an anti-conservative distribution, which is
good. The flat distribution across the bottom corresponds to null p-values which
are distributed approximately uniformly between 0 and 1. The peak close to 0 
contains our significantly changing proteins, a combination of true positives 
and false positives.

Other examples of how a p-value histogram could look are shown below. Whilst in
some experiments a uniform p-value distribution may arise due to an absence of
significant alternative hypotheses, other distribution shapes can indicate that
something was wrong with the model design or statistical test. For more detail on
how to interpret p-value histograms there is a great [blog](http://varianceexplained.org/statistics/interpreting-pvalue-histogram/) 
by David Robinson.


```{r p-value_hist, echo = FALSE, fig.cap = "Examples of p-value histograms.", fig.align = "center", out.width = "100%"}
knitr::include_graphics("figs/phist_shapes.png", error = FALSE)
```

The second plot that we generate is an SA plot to display the residual standard
deviation (sigma) versus log abundance for each protein to which our model was
fitted. We can use the `plotSA` function to do this.

```{r}
plotSA(fit = final_model,
       cex = 0.5,
       xlab = "Average log2 abundance")
```

It is recommended that an SA plot be used as a routine diagnostic plot when
applying a limma-trend pipeline. From the SA plot we can visualise the
intensity- dependent trend that has been incorporated into our linear model. It
is important to verify that the trend line fits the data well. If we had not
included the `trend = TRUE` argument in our `eBayes` function, then we would
instead see a straight horizontal line that does not follow the trend of the
data. Further, the plot also colours any outliers in red. These are the outliers
that are only detected and excluded when using the `robust = TRUE` argument.


### Interpreting the output of our statistical model

Having checked that the model we fitted was appropriate for the data, we can now
take a look at the results of our test using the `topTable` and `decideTests` 
functions.

#### Interpreting the overall ANOVA output
As we saw above, `topTable` will give us the overall output of our ANOVA model.
We previously used this function to generate our `limma_results` without specifying
any value for the `coef` argument. 

```{r}
head(limma_results)
```

Interpreting the output of `topTable` for a multi-contrast model:

* `M_G1`, `M_Des` and `G1_Des` = the estimated log2FC for each model contrast
* `AveExpr` = the average log abundance of the protein across samples
* `F` = eBayes moderated F-value. Interpreted in the same way as a normal F-value (see above).
* `P.Value` = Unadjusted p-value
* `adj.P.Val` = FDR-adjusted p-value (adjusting across both proteins and contrasts - global adjustment) 

We have used the ANOVA test to ask *"Does this protein show a significant change in abundance between cell cycle stages?"*
for each protein. 

Our null hypothesis is:
**(H0)** The change in abundance for a protein between cell cycle stages is 0. 
In other words, the null hypothesis for each protein is that the mean abundance
in M-phase = the mean abundance in G1-phase = the mean abundance in desynchronised 
cells. So, `M_G1` = `M_Des` = `G1_Des` = 0.

Our alternative hypothesis is:
**(H1)** The change in abundance for a protein between cell cycle stages is 
greater than 0. 

From our output we can see that some of our proteins have high F-values and 
low adjusted p-values (below any likely threshold of significance). These 
adjusted p-values may tell us that a protein has a significantly different 
abundance across cell cycle stages. However, we cannot tell from this output
from which contrast this significance is derived. Does a protein have a
significantly different abundance between M- and G1-phase, or M-phase and 
desynchronised cells, or G1-phase and desynchronised cells, or even multiple of
these comparisons? 


#### Interpreting the results of a single contrast
We can look at individual contrasts by passing the contrast name to the `coef`
argument in the `topTable` function. For example, let's look at the pairwise
comparison between M-phase and desynchronised cells. We use the `topTable` 
function to get the results of the `"M_Des"` contrast. We use the argument 
`confint = TRUE` so that the our output reports the 95% confidence interval of
the calculated log2FC. 

```{r}
M_Desynch_results <- topTable(fit = final_model, 
                              coef = "M_Des", 
                              number = Inf,
                              adjust.method = "BH",
                              confint = TRUE) %>%
  rownames_to_column("protein")

## Verify
head(M_Desynch_results)
```

Interpreting the output of `topTable` for a single contrast:

* `logFC` = the fold change between the mean log abundance in group A and the mean log abundance in group B
* `CI.L` = the left limit of the 95% confidence interval for the reported log2FC
* `CI.R` = the right limit of the 95% confidence interval for the reported log2FC
* `AveExpr` = the average log abundance of the protein across samples
* `t` = t-statistic derived from the original ANOVA test (not a t-test)
* `P.Value` = Unadjusted p-value
* `adj.P.Val` = FDR-adjusted p-value (adjusted across proteins but not multiple contrasts)
* `B` = B-statistic representing the log-odds that a protein is differentially abundant between conditions

This time the output of `topTable` contains a t-statistic rather than an F-value.
This is because we only told the function to compare two conditions, so the 
corresponding t-statistic from our ANOVA test is reported. Importantly, however, 
the p-value adjustment in this case only accounts for multiple tests across our
`r nrow(limma_results)` proteins, not the three different contrasts/comparisons
we used the data for. As a result, we could over-estimate the number of
statistically significant proteins within this contrast. although this this 
effect is only likely to become problematic when we have a larger number of 
contrasts to account for. 


#### Interpreting the results of all contrasts
To understand the impact of adjusting for multiple hypothesis testing across 
our comparisons, we can use the `decideTests` function. This function provides
a matrix of values -1, 0 and +1 to indicate whether a protein is significantly
downregulated, unchanged or significantly upregulated in a given contrast. For
the function to determine significance we have to provide a p-value adjustment
method and threshold, here we use the standard Benjamini-Hochberg procedure for
FDR adjustment and set a threshold of `adj.P.Value < 0.01` for significance.

The `decideTests` function also takes an argument called `method`. This argument
specifies whether p-value adjustment should account only for multiple hypothesis
tests across proteins (`"separate"`) or across both proteins and contrasts 
(`"global"`). 

Let's first look at the results when we apply the `"separate"` method i.e., 
consider each contrast separately.

```{r}
decideTests(object = final_model,
            adjust.method = "BH", 
            p.value = 0.01, 
            method = "separate") %>%
  summary()
```

From this table we can see the number of significantly changing proteins per
contrast. For the `M_Des` comparison the total number of significantly changing
proteins is `r 422 + 473`. This should be the same as the number of proteins
with an adjusted p-value < 0.01 in our `M_Desynch_results` object. Let's check.

```{r}
M_Desynch_results %>%
  as_tibble() %>%
  filter(adj.P.Val < 0.01) %>% 
  nrow()
```

However, if we use the `"global"` method for p-value adjustment and therefore
adjust for both per protein and per contrast hypotheses we may see slightly
fewer significant proteins in our `M_Des` contrast.

```{r}
decideTests(object = final_model,
            adjust.method = "BH", 
            p.value = 0.01, 
            method = "global") %>%
  summary()
```

Unfortunately, there is no way to specify global p-value adjustment accounting
for all contrasts when using `topTable` to look at a single contrast. Instead, 
we can merge the results from our globally adjusted significance summary 
(generated using `decideTests` with `method = "global"`) with the results of 
our overall ANOVA test (generated using `topTable` with `coef = NULL`). We 
demonstrate how to do this in the code below.

```{r}
## Determine global significance using decideTests
global_sig <- as.data.frame(decideTests(object = final_model, 
                          adjust.method = "BH", 
                          p.value = 0.01, 
                          method = "global")) %>%
  rownames_to_column("protein")


## Change column names to avoid conflict when binding
colnames(global_sig) <- paste0("sig_", colnames(global_sig))

## Add the results of global significance test to overall ANOVA results
limma_results <- dplyr::left_join(limma_results, global_sig, by = c("Protein" = "sig_protein"))
```

We now have three additional column, one per contrast, called `sig_M_G1`, `sig_M_Des`,
and `sig_G1_Des`. These columns contain -1, 0 or 1 meaning that the protein is
significantly downregulated, non-significant or significantly upregulated in the
given contrast.


#### Adding user-defined significance thresholds 

For the remainder of this workshop we will consider only the `M_Des` single
contrast and use the results of `topTable` with `coef = "M_Des"`. We currently
have these results stored in an object called `M_Desynch_results`.

The output of our statistical test will provide us with key information for
each protein, including its p-value, BH-adjusted p-value (here, across proteins
but not contrasts) and logFC. However, it is up to us to decide what we consider 
to be significant. The first parameter to consider is the `adj.P.Val` threshold
that we wish to apply - 0.05 and 0.01 are both common in proteomics. The second 
parameter which is sometimes used to define significance is the `logFC`. This is
mainly for the purpose of deciding on hits to follow-up, since it is easier to 
validate larger abundance changes than those which are consistent but subtle. 

Here we are going to define significance based on an `adj.P.Val` < 0.01. We can
add a column to our results to indicate significance as well as the direction of
change. 


```{r}
## Add direction and significance information
M_Desynch_results <- 
  M_Desynch_results %>%
  as_tibble() %>%
  mutate(direction = ifelse(logFC > 0, "up", "down"),
         significance = ifelse(adj.P.Val < 0.01, "sig", "not.sig"))


## Verify
head(M_Desynch_results)
```


### Visualising the results of our statistical model

The final step in any statistical analysis is to visualise the results. This is
important for ourselves as it allows us to check that the data looks as 
expected. 

The most common visualisation used to display the results of expression
proteomics experiments is a volcano plot. This is a scatterplot that shows
statistical significance (p-values) against the magnitude of fold change.
Of note, when we plot the statistical significance we use the raw unadjusted
p-value (`-log10(P.Value)`). This is because it is better to plot the basic
data in its raw form than any derived value (the adjusted p-value is derived
from each p-value using the BH-method of correction). The process of FDR
correction can result in some points that previously had distinct p-values 
having the same adjusted p-value. Further, different methods of correction will
generate different adjusted p-values, making the comparison and interpretation
of values more difficult.


```{r}
M_Desynch_results %>%
  ggplot(aes(x = logFC, y = -log10(P.Value), fill = significance)) +
  geom_point(shape = 21, stroke = 0.25, size = 3) +
  theme_bw()
```

There are several ways in which we can export this volcano plot from R. One
option is to use the `ggsave` function immediately after our plotting code. The 
default for this function is to save the last plot displayed.

```{r, eval = FALSE}
my_results %>%
  ggplot(aes(x = logFC, y = -log10(P.Value), fill = result)) +
  geom_point(shape = 21, stroke = 0.25, size = 3) +
  theme_bw()
ggsave(filename = "volcano_M_vs_Desynch.png", device = "png")
```


:::{.callout-exercise}
#### Challenge: Volcano plots
{{< level 2 >}}
Re-generate your table of results defining significance based on an adjusted 
P-value < 0.05 and a log2 fold-change of > 1.

::: {.callout-answer collapse=true}

First let's regenerate the results adding a column for the log2 fold change

```{r}
my_results <- 
  M_Desynch_results %>%
  as_tibble() %>%
  mutate(direction = ifelse(logFC > 0, "up", "down"),
         significance = ifelse(adj.P.Val < 0.05, "sig", "not.sig"),
         lfc = ifelse(logFC > 1 | logFC < -1, "sig", "not.sig"),
         result = ifelse(significance == "sig" & lfc == "sig", "sig", "not.sig"))

```

Now let's plot the results

```{r}
my_results %>%
  ggplot(aes(x = logFC, y = -log10(P.Value), fill = result)) +
  geom_point(shape = 21, stroke = 0.25, size = 3) +
  theme_bw()
```


:::
:::

```{r, include=FALSE}
## Save results
save(M_Desynch_results, all_proteins, file = "output/lesson_7.rda")
```



::: {.callout-tip}
#### Key Points

- The `limma` package provides a statistical pipeline for the analysis of differential expression (abundance) experiments
- Empirical Bayes moderation involves borrowing information across proteins to squeeze the per-protein variance estimates towards an expected value based on the behaviour of other proteins with similar abundances. This method increases the statistical power and reduces the number of false positives. 
- Since proteomics data typically shows an intensity-dependent trend, it is recommended to apply empirical Bayes moderation with `trend = TRUE` and `robust = TRUE`. This approach can be validated by plotting an SA plot.
- Significance thresholds are somewhat arbitary and must be selected by the user. However, correction must be carried out for multiple hypothesis testing so significance thresholds should be based on adjusted p-values rather than raw p-values. Users may also threshold significance based on a log fold-change value too.
- The results of differential expression and abundance analyses are often summarised on volcano plots.
:::



## References {-}
