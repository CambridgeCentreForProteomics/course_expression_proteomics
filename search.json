[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Analysis of expression proteomics data in R",
    "section": "",
    "text": "Analysis of expression proteomics data in R",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Welcome",
      "Overview"
    ]
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Analysis of expression proteomics data in R",
    "section": "Overview",
    "text": "Overview\nThese materials focus on expression proteomics, which aims to characterise the protein diversity and abundance in a particular system. You will learn about the bioinformatic analysis steps involved when working with these kind of data, in particular several dedicated proteomics Bioconductor packages, part of the R programming language. We will use a real-world dataset obtained from a tandem mass tag (TMT) mass spectrometry experiment. We cover the basic data structures used to store and manipulate protein abundance data, how to do quality control and filtering of the data, as well as several visualisations. Finally, we include statistical analysis of differential abundance across sample groups (e.g. control vs. treated) and further evaluation and biological interpretation of the results via gene ontology analysis.\n\n\n\n\n\n\nTipLearning Objectives\n\n\n\nYou will learn about:\n\nHow mass spectrometry can be used to quantify protein abundance and some of the methods used for peptide quantitation.\nThe bioinformatics steps involved in processing and analysing expression proteomics data.\nHow to assess the quality of your data, deal with missing values and summarise PSM-level (peptide-spectrum match) data to protein-level.\nHow to perform differential expression analysis to compare protein abundances between different groups of samples.\n\n\n\n\nTarget Audience\nProteomics practitioners or data analysts/bioinformaticians that would like to learn how to use R to analyse proteomics data.\n\n\nPrerequisites\n\nBasic understanding of mass spectometry.\n\nWatch this video for an excellent overview.\n\nA working knowledge of R and the tidyverse.\nFamiliarity with other Bioconductor data classes, such as those used for RNA-seq analysis, is useful but not required.\n\n\n\n\nExercises\nExercises in these materials are labelled according to their level of difficulty:\n\n\n\n\n\n\n\nLevel\nDescription\n\n\n\n\n  \nExercises in level 1 are simpler and designed to get you familiar with the concepts and syntax covered in the course.\n\n\n  \nExercises in level 2 combine different concepts together and apply it to a given task.\n\n\n  \nExercises in level 3 require going beyond the concepts and syntax introduced to solve new problems.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Welcome",
      "Overview"
    ]
  },
  {
    "objectID": "index.html#instructors",
    "href": "index.html#instructors",
    "title": "Analysis of expression proteomics data in R",
    "section": "Instructors",
    "text": "Instructors\nThis workshop will be run by:\n\nLisa Breckels - Cambridge Centre for Proteomics, University of Cambridge\nTom Smith - MRC Laboratory of Molecular Biology, Cambridge\nAlistair Hines - Cambridge Centre for Proteomics, University of Cambridge\nOliver Crook - Kavli Institute for NanoScience Discovery, University of Oxford\n\nPrevious instructors include:\n\nCharlotte Hutchings - Cambridge Centre for Proteomics, University of Cambridge\nThomas Krueger - Department of Biochemistry, University of Cambridge\nCharlotte S. Dawson - Cambridge Centre for Proteomics, University of Cambridge",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Welcome",
      "Overview"
    ]
  },
  {
    "objectID": "index.html#authors",
    "href": "index.html#authors",
    "title": "Analysis of expression proteomics data in R",
    "section": "Authors",
    "text": "Authors\n\nAbout the authors:\n\nCharlotte Hutchings  \nAffiliation: Cambridge Centre for Proteomics, Department of Biochemistry, University of Cambridge\nRoles: writing - original draft; conceptualisation; coding\nLisa Breckels  \nAffiliation: Cambridge Centre for Proteomics, Department of Biochemistry, University of Cambridge\nRoles: writing - original draft; conceptualisation; coding\nTom Smith  \nAffiliation: MRC Laboratory of Molecular Biology, Cambridge\nRoles: writing-review-editing; coding\nCharlotte Dawson \nAffiliation: Cambridge Centre for Proteomics, Department of Biochemistry, University of Cambridge. Roles: writing-review-editing; coding",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Welcome",
      "Overview"
    ]
  },
  {
    "objectID": "index.html#citation",
    "href": "index.html#citation",
    "title": "Analysis of expression proteomics data in R",
    "section": "Citation",
    "text": "Citation\n\nPlease cite these materials if:\n\nYou adapted or used any of them in your own teaching.\nThese materials were useful for your research work.\n\nFor example, you can cite us in the methods section of your paper: “We carried out our analyses based on the recommendations in Hutchings and Breckels (2024)”.\nYou can cite these materials as:\n\nHutchings C, Breckels LM (2024) “CambridgeCentreForProteomics/course_expression_proteomics: Analysis of expression proteomics data in R”, https://cambridgecentreforproteomics.github.io/course_expression_proteomics\n\nOr in BibTeX format:\n@Misc{,\n  author = {Charlotte Hutchings and Lisa M Breckels},\n  title = {CambridgeCentreForProteomics/course_expression_proteomics: Analysis of expression proteomics data in R},\n  month = {November},\n  year = {2024},\n  url = {https://cambridgecentreforproteomics.github.io/course_expression_proteomics}\n}\n\nOther key references\nData analysis workflow\n\nHutchings C, Dawson CS, Krueger T, Lilley KS, Breckels LM. A Bioconductor workflow for processing, evaluating, and interpreting expression proteomics data [version 2; peer review: 3 approved]. F1000Research 2024, 12:1402 https://f1000research.com/articles/12-1402/v2\n\nThe QFeatures R/Bioconductor package.\n\nGatto L, Vanderaa C: QFeatures: Quantitative features for mass spectrometry data. R package version 1.12.0. 2023. Reference Source\n\nThe limma R/Bioconductor package\n\nRitchie, M.E., Phipson, B., Wu, D., Hu, Y., Law, C.W., Shi, W., and Smyth, G.K. (2015). limma powers differential expression analyses for RNA-sequencing and microarray studies. Nucleic Acids Research 43(7), e47.\n\nCase-study data\n\nQueiroz, R.M.L., Smith, T., Villanueva, E., Marti-Solano, M., Monti, M., Pizzinga, M., Mirea, D.-M., Ramakrishna, M., Harvey, R.F., Dezi, V., Thomas, G.H., Willis, A.E. & Lilley, K.S. (2019) Comprehensive identification of RNA–protein interactions in any organism using orthogonal organic phase separation (OOPS). Nature Biotechnology. 37 (2), 169–178. doi:10.1038/s41587-018-0001-2.\n\nMass spectrometry-based proteomics:\n\nDupree, E.J., Jayathirtha, M., Yorkey, H., Mihasan, M., Petre, B.A. & Darie, C.C. (2020) A Critical Review of Bottom-Up Proteomics: The Good, the Bad, and the Future of This Field. Proteomes. 8 (3), 14. doi:10.3390/proteomes8030014.\n\n\nObermaier, C., Griebel, A. & Westermeier, R. (2021) Principles of protein labeling techniques. In: A. Posch (ed.). Proteomic Profiling: Methods and Protocols. Methods in Molecular Biology. New York, NY, Springer US. pp. 549–562. doi:10.1007/978-1-0716-1186-9_35.\n\n\nRainer, L.G., Sebastian Gibb, Johannes (n.d.) Chapter 5 Quantitative data | R for Mass Spectrometry. https://rformassspectrometry.github.io/book/sec-quant.html.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Welcome",
      "Overview"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Analysis of expression proteomics data in R",
    "section": "Acknowledgements",
    "text": "Acknowledgements\n\n\nThank you to Hugo Tavares for coordinating this course and his valuable input in developing and testing this material.\nThomas Kruger and Charlotte S. Dawson for their input and guidance writing this material and the f1000 workflow A Bioconductor workflow for processing, evaluating, and interpreting expression proteomics data.\nProf. Kathryn Lilley, group head and director of Cambridge Centre for Proteomics at the Department of Biochemistry, University of Cambridge.\nThe QFeatures and limma R/Bioconductor packages are fundamental to this workflow, please cite them alongside the course if you use this material. Thank you to Laurent Gatto and Christophe Vanderaa for providing exemplary software for proteomics.\nThank you to the R for Mass Spectrometry team for providing excellent material in particular the R for Mass Spectrometry Book by Laurent Gatto, Sebastian Gibb and Johannes Rainer.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Welcome",
      "Overview"
    ]
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Data & Setup",
    "section": "",
    "text": "Data\nThe data used in these materials is provided as a zip file. Download and unzip the folder to your Desktop to follow along with the materials.\nDownload",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Welcome",
      "Data & Setup"
    ]
  },
  {
    "objectID": "setup.html#working-on-your-own-laptop",
    "href": "setup.html#working-on-your-own-laptop",
    "title": "Data & Setup",
    "section": "Working on your own laptop",
    "text": "Working on your own laptop\nIf you wish to work on your own laptop during the course and not the training machines you will need to have the latest stable versions of R, RStudio and Bioconductor including the packages listed below.\n\nR and RStudio\nThis lesson assumes that you have current versions of the following installed on your computer:\n\nThe R software itself, and\nRStudio Desktop.\n\nR and RStudio require separate downloads and installations. R itself refers to the underlying language and computing environment required to write and read this language. RStudio is a graphical integrated development environment which essentially makes running R much easier and more interactive.\n\n\n\n\n\n\n\n\nWindows\nDownload and install all these using default options:\n\nR\nRTools\nRStudio\n\n\n\nMac OS\nDownload and install all these using default options:\n\nR\nRStudio\n\n\n\nLinux\n\nGo to the R installation folder and look at the instructions for your distribution.\nDownload the RStudio installer for your distribution and install it using your package manager.\n\n\n\n\n\nR package installation\nIn this workshop we make use of open-source software from the R Bioconductor (Huber et al. 2015) project. The Bioconductor initiative provides R software packages dedicated to the processing of high-throughput complex biological data. Packages are open-source, well-documented and benefit from an active community of developers.\nDetailed instructions for the installation of Bioconductor packages are documented on the Bioconductor Installation page. The main packages required for this workshop are installed using the code below. Additional packages required for downstream statistics and interpretation are installed as required.\n\nif (!require(\"BiocManager\", quietly = TRUE)) {\n  install.packages(\"BiocManager\")\n}\n\nBiocManager::install(c(\"QFeatures\",\n                       \"NormalyzerDE\",\n                       \"limma\",\n                       \"factoextra\",\n                       \"org.Hs.eg.db\",\n                       \"clusterProfiler\",\n                       \"enrichplot\",\n                       \"patchwork\",\n                       \"tidyverse\",\n                       \"pheatmap\",\n                       \"ggupset\"))\n\nAfter installation, each package must be loaded before it can be used in the R session. This is achieved via the library function. Here we load all packages included in this course.\n\nlibrary(\"QFeatures\")\nlibrary(\"NormalyzerDE\")\nlibrary(\"limma\")\nlibrary(\"factoextra\")\nlibrary(\"org.Hs.eg.db\")\nlibrary(\"clusterProfiler\")\nlibrary(\"enrichplot\")\nlibrary(\"patchwork\")\nlibrary(\"tidyverse\")\nlibrary(\"pheatmap\")\nlibrary(\"ggupset\")",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Welcome",
      "Data & Setup"
    ]
  },
  {
    "objectID": "materials/01_intro.html",
    "href": "materials/01_intro.html",
    "title": "1  The use-case data",
    "section": "",
    "text": "1.1 Exploring changes in protein abundance across stages of the cell cycle\nAs a use case, we will analyse an expression proteomics dataset from Queiroz et al. (2019). The aim of this experiment was to examine proteins involved in the cell cycle (see Figure 1.1). Briefly, cells from a human cell line (U-2 0S) were treated with Nocodazole to inhibit the polymerisation of microtubules and block the transition beyond the prometaphase stage of M-phase. After 18-hours of treatment, the cells were released from cell cycle inhibition by withdrawing Nocodazole. Samples were taken in triplicate at 0, 6 and 23-hours post-withdrawl to generate 3 x M-phase samples, 3 x G1-phase samples and 3 x desynchronised samples. A sample of cells was also taken prior to Nocodazole treatment as an additional control, thus giving a total of 10 samples.\nFigure 1.1: A schematic summary of the experimental design used to generate the use case data.\nTo measure protein abundance via bottom-up mass spectrometry, protein samples were digested to peptides using trypsin (an enzyme which cleaves proteins at the C-terminus of arginine and lysine residues, except where there is an adjacent proline residue). The same quantity of peptide from each sample was then labelled using Tandem Mass Tag (TMT) labels. This allows all of the samples to be combined into a single pooled MS sample, thus reducing technical variability and MS time, as well as ensuring quantification of the same peptides in each samples. The TMT labelling strategy is outlined below in Table 1.1.\nTable 1.1: Sample information and TMT labelling strategy in the use case experiment\n\n\n\n\n\n\nSample Name\nTimepoint\nReplicate\nTag\n\n\n\n\nControl\nPre-treatment\nNA\n126\n\n\nM_1\n0hrs\n1\n127N\n\n\nM_2\n0hrs\n2\n127C\n\n\nM_3\n0hrs\n3\n128N\n\n\nG1_1\n6hrs\n1\n128C\n\n\nG1_2\n6hrs\n2\n129N\n\n\nG1_3\n6hrs\n3\n129C\n\n\nDS_1\n23hrs\n1\n130N\n\n\nDS_2\n23hrs\n2\n130C\n\n\nDS_3\n23hrs\n3\n131",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Expression proteomics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The use-case data</span>"
    ]
  },
  {
    "objectID": "materials/01_intro.html#isobaric-peptide-labelling-using-tandem-mass-tags-tmt",
    "href": "materials/01_intro.html#isobaric-peptide-labelling-using-tandem-mass-tags-tmt",
    "title": "1  The use-case data",
    "section": "1.2 Isobaric peptide labelling using Tandem Mass Tags (TMT)",
    "text": "1.2 Isobaric peptide labelling using Tandem Mass Tags (TMT)\nAs outlined above, the use-case experiment made use of tandem mass tag (TMT) labelling. TMT reagents are isobaric chemical labels that can be covalently linked to peptide amine groups, either at the peptide N-terminus or at lysine (K) residues. These reagents are currently available in kits (referred to as TMTplexes) containing 6 (TMT6plexTM), 10 (TMT10plexTM), 16 (TMTpro 16plexTM), 18 (TMTpro 18plexTM) or 32 (TMTpro 32plexTM) TMT labels. Further, kits can be combined to multiplex up to 35 samples. The isobaric nature of TMT reagents means that all of the labels within a TMTplex have the same overall mass. However, they differ in how this mass is distributed between their two variable regions - the reporter and mass normaliser. The structure of a TMT label is shown below in Figure 1.2.\n\n\n\n\n\n\n\n\nFigure 1.2: The chemical structure of a TMT label.\n\n\n\n\n\nAfter labelling each sample with a different TMT reagent, the same peptide will have an identical mass but be differentially labelled across samples. The samples are then pooled together and run on the MS as a single multiplexed sample. In the MS1 spectra it is not possible to distinguish how much of each peptide (peak) was derived from each sample because all of the labelled peptides still have the same overall mass. During fragmentation, however, a cleavable linker within the TMT label is broken, thereby releasing a section of the label called the reporter ion. The mass of this reporter ion is different between TMT labels due to differences in the distribution of isotopes (C13 and N15) between the mass reporter and mass normalizer regions. As a result, the spectra of fragment ions will contain one peak per reporter ion and the relative intensity of these ions can be used to calculate the relative abundance of the identified peptide across the labelled samples. This is summarised below in Figure 1.3.\n\n\n\n\n\n\n\n\nFigure 1.3: Relative peptide quantitation using TMT labels. Image modified from Thermo Fisher Scientific.\n\n\n\n\n\nThis means that TMT labelling can be used for the relative quantification of peptides (and in turn proteins) across samples. Here we will use the relative reporter ion intensity of our TMT labels to compare abundance between samples taken throughout the cell cycle. If we wanted to carry out absolute quantification of proteins, TMT labelling would not be an appropriate method.\n\n\n\n\n\n\nNoteMS2- vs. MS3-based TMT quantitaion\n\n\n\nOriginally the reporter ion of a TMT reagent was released by fragmentation of the precursor peptide. This meant that the reporter ion signals could be calculated based on MS2 spectra, and hence TMT quantitation was done at the MS2 level. However, different precursor peptides can have very similar m/z values, resulting in their MS1 peaks being very close together. When one of these precursors is selected for fragmentation it is possible for other precursors to be unintentionally co-isolated within the same fragmentation window. This results in co-isolation interference since the reporter ions present at MS2 are not only from the identified peptide but also any other co-isolated peptides.\nMore recently, MS3-based quantitation has been applied to reduce the problem of co-isolation interference and increase quantitation accuracy when using TMT labels Ting et al. (2011). In this approach the initial round of precursor fragmentation only uses a weak fragmentation method (e.g., collision induced dissociation, CID) to ensure that TMT reporter ions are not broken off at MS2. The most abundance fragment ions from MS2 are then selected for a second round of fragmentation, this time using a stronger force (e.g., high-energy collision dissociation) so that the TMT reporter ions are released. Since the MS2 fragment ions derived from the intended precursor peptide are more abundant than the unwanted MS2 fragments from co-isolated peptides, the MS3 fragments and reporter ions provide a much cleaner quantitation of the identified precursor peptide. The MS3 reporter ion quantitation is almost purely from the peptide that we want, not from a mixture of all co-isolated peptides.\nThe specific MS3 data acquisition approach used to generate our data was synchronous precursor selection (SPS)-MS3. In simple terms, the SPS method allows us to select multiple MS2 fragments for additional fragmentation to the MS3 level, rather than just one McAlister et al. (2014).",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Expression proteomics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The use-case data</span>"
    ]
  },
  {
    "objectID": "materials/01_intro.html#identification-search-of-the-raw-ms-data",
    "href": "materials/01_intro.html#identification-search-of-the-raw-ms-data",
    "title": "1  The use-case data",
    "section": "1.3 Identification search of the raw MS data",
    "text": "1.3 Identification search of the raw MS data\nHaving analysed the pooled TMT sample via MS, the raw MS were processed using Proteome Discoverer v3.0 (Thermo Fisher Scientific). This software is one of several that can be used to carry out a database search to identify and quantify peptide sequences, and therefore proteins, from raw MS data. Others include MaxQuant, FragPipe and PEAKS among others.\nBriefly, database searching of MS data involves comparing the observed MS spectra to expected MS spectra generated via in silico digestion of a selected protein database. Since the use-case data used human cells, we provided the human proteome to Proteome Discoverer as the database for searching. We also provided a database of common contaminants that could be present in our sample due to (i) human contamination (e.g., keratin from nails, hair or skin), or (ii) sample preparation (e.g., enzymes used for protein digestion). These two databases are can be found in the course materials and also at https://zenodo.org/records/7837375. The result of such a database search is a list of peptide spectrum matches (PSMs), that is a list of matches between observed and expected spectra. Most software will also aggregate the PSM level data upward and provide output files at all data levels (PSM, peptide and protein). It is up to the user which output file to use for further analysis.\nThe starting point for this workshop and data processing workflow in R is an output file from the identification search. Proteome Discoverer provides each data level output as a separate .txt file. Here, we will use the PSM-level .txt file.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Expression proteomics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The use-case data</span>"
    ]
  },
  {
    "objectID": "materials/01_intro.html#starting-analysis-from-psm-peptide-or-protein-level",
    "href": "materials/01_intro.html#starting-analysis-from-psm-peptide-or-protein-level",
    "title": "1  The use-case data",
    "section": "1.4 Starting analysis from PSM, peptide or protein level",
    "text": "1.4 Starting analysis from PSM, peptide or protein level\nIn general, to allow for maximum control of data quality, normalisation and aggregation it is advisable to begin analysis from the lowest possible data level. This will ensure a greater understanding of the data and facilitate transparency throughout the process.\nFor TMT data we can start our data analysis from the PSM level. However, it is not always possible to start at this level. Some algorithms used to process label-free data may require the data to be analysed from the peptide-level. See Adapting this workflow to label-free proteomics for more details.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Expression proteomics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The use-case data</span>"
    ]
  },
  {
    "objectID": "materials/01_intro.html#summary",
    "href": "materials/01_intro.html#summary",
    "title": "1  The use-case data",
    "section": "1.5 Summary",
    "text": "1.5 Summary\nThe use-case data that we will process and analyse in this workshop is a DDA TMT-labelled bottom-up proteomics dataset. The aim of the experiment is to calculate relative protein abundances between samples. This will then allow us to apply statistical tests to determine whether any proteins have significantly different abundances relative to each other.\nAlthough many of the processing and analysis steps discussed in this course are also applicable to other types of proteomics datasets (particularly label-free DDA bottom-up proteomics), there is no one-size-fits-all workflow for expression proteomics. Some aspects of the workflow would need to be adapted for other types of proteomics experiment. We have provided details on how this workflow could be adapted to label-free data or data processed using MaxQuant rather than Proteome Discoverer.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Expression proteomics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The use-case data</span>"
    ]
  },
  {
    "objectID": "materials/01_intro.html#glossary",
    "href": "materials/01_intro.html#glossary",
    "title": "1  The use-case data",
    "section": "1.6 Glossary",
    "text": "1.6 Glossary\nKey terms that you should understand for this workshop:\n\nPrecursor ion = the original parent ion representing an ionized form of the entire peptide sequence.\nFragment ion = an ion produced by fragmentation of the precursor ion, thus only representing a fraction of the original peptide sequence.\nMS1 spectrum = raw mass spectrum produced by the separation of precursor ions based on their mass-to-charge ratio (m/z). Each peak represents a precursor ion at a particular m/z and with an associated intensity.\nMS2 (MS/MS) spectrum = raw mass spectrum produced by the separation of fragment ions based on their mass-to-charge ratio (m/z). Each peak corresponds to a fragment ion derived from the same precursor ion.\nPeptide spectrum match (PSM) = A match made between a theoretical mass spectrum for a given peptide sequence and an observed experimental spectrum, thus linking a raw mass spectrum to its predicted peptide sequence\nTandem mass tag (TMT) = a type of peptide label which can be used for relative quantification of peptides across samples. Quantification is measured at the MS2 or MS3 level.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Expression proteomics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The use-case data</span>"
    ]
  },
  {
    "objectID": "materials/01_intro.html#references",
    "href": "materials/01_intro.html#references",
    "title": "1  The use-case data",
    "section": "References",
    "text": "References\n\n\n\n\nMcAlister, Graeme C., David P. Nusinow, Mark P. Jedrychowski, Martin Wühr, Edward L. Huttlin, Brian K. Erickson, Ramin Rad, Wilhelm Haas, and Steven P. Gygi. 2014. “MultiNotch MS3 Enables Accurate, Sensitive, and Multiplexed Detection of Differential Expression Across Cancer Cell Line Proteomes.” Analytical Chemistry 86 (14): 7150–58. https://doi.org/10.1021/ac502040v.\n\n\nQueiroz, Rayner M. L., Tom Smith, Eneko Villanueva, Maria Marti-Solano, Mie Monti, Mariavittoria Pizzinga, Dan-Mircea Mirea, et al. 2019. “Comprehensive Identification of RNAprotein Interactions in Any Organism Using Orthogonal Organic Phase Separation (OOPS).” Nature Biotechnology 37 (2): 169–78. https://doi.org/10.1038/s41587-018-0001-2.\n\n\nTing, Lily, Ramin Rad, Steven P Gygi, and Wilhelm Haas. 2011. “MS3 Eliminates Ratio Distortion in Isobaric Multiplexed Quantitative Proteomics.” Nature Methods 8 (11): 937–40. https://doi.org/10.1038/nmeth.1714.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Expression proteomics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The use-case data</span>"
    ]
  },
  {
    "objectID": "materials/02_import_and_infrastructure.html",
    "href": "materials/02_import_and_infrastructure.html",
    "title": "2  Import and infrastructure",
    "section": "",
    "text": "2.1 The data structure\nMS-based quantitative proteomics data is typically represented as a matrix in which the rows represent features (PSMs, peptides or proteins) and the columns contain information about these features, including their quantification measurements across several samples. As we saw before, this type of quantitative proteomics matrix can be generated by using third party softwares to process raw MS data. Moreover, the software usually outputs a quantitative matrix for each data level (e.g., PSM, peptide, peptide groups, protein groups). In this lesson we will explain how we can make use of R/Bioconductor packages with dedicated functions and data structures to store and manipulate quantitative proteomics data.\nWe begin with exporting our data at the lowest data-level. In this DDA TMT experiment this is the PSM level. This .txt file contains for every PSM identified the relative abundance of each PSMs across each sample as well as columns with PSM meta-data.\nWe will be using the Quantitative features for mass spectrometry, or QFeatures, Bioconductor package to import, store, and manipulate our data. Before we import the data into R using QFeatures it is first necessary to understand the structure of a QFeatures object and SummarizedExperiment object.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Expression proteomics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Import and infrastructure</span>"
    ]
  },
  {
    "objectID": "materials/02_import_and_infrastructure.html#the-structure-of-a-summarizedexperiment",
    "href": "materials/02_import_and_infrastructure.html#the-structure-of-a-summarizedexperiment",
    "title": "2  Import and infrastructure",
    "section": "2.2 The structure of a SummarizedExperiment",
    "text": "2.2 The structure of a SummarizedExperiment\nTo simplify the storage of quantitative proteomics data we can store the data as a SummarizedExperiment object (as shown in Figure 2.1).SummarizedExperiment objects can be conceptualised as data containers made up of three different parts:\n\nThe assay - a matrix which contains the quantitative data from a proteomics experiment. Each row represents a feature (a PSM, peptide or protein) and each column contains the quantitative data (measurements) from one experimental sample.\nThe rowData - a table (data frame) which contains all remaining information derived from an identification search (i.e. every column from your identification search output that was not a quantification column). Rows represent features but columns inform about different attributes of the feature (e.g., its sequence, name, modifications).\nThe colData - a table (data frame) to store sample metadata that would not appear in the output of your identification search. This could be for example, the cell line used, which condition, which replicate each sample corresponds to etc. Again, this information is stored in a matrix-like data structure.\n\nFinally, there is also an additional container called metadata which is a place for users to store experimental metadata. For example, this could be which instrument the samples were run on, the operators name, the date the samples were run and so forth. We will focus on populating and understanding the above 3 main containers within a SummarizedExperiment.\n\n\n\n\n\n\n\n\nFigure 2.1: Diagramatic representation of the structure of a SummarizedExperiment object in R (modified from the SummarizedExperiment package)\n\n\n\n\n\nData stored in these three main areas can be easily accessed using the assay(), rowData() and colData() functions, as we will see later.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Expression proteomics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Import and infrastructure</span>"
    ]
  },
  {
    "objectID": "materials/02_import_and_infrastructure.html#the-structure-of-a-qfeatures-object",
    "href": "materials/02_import_and_infrastructure.html#the-structure-of-a-qfeatures-object",
    "title": "2  Import and infrastructure",
    "section": "2.3 The structure of a QFeatures object",
    "text": "2.3 The structure of a QFeatures object\nWhilst a SummarizedExperiment is able to neatly store quantitative proteomics data at a single data level (i.e., PSM, peptide or protein), a typical data analysis workflow requires us to look across multiple levels. For example, it is common to start an analysis with a lower data level and then aggregate upward towards a final protein-level dataset. Doing this allows for greater flexibility and understanding of the processes required to clean and aggregate the data.\nA QFeatures object is essentially a list of SummarizedExperiment objects. However, the main benefit of using a QFeatures object over storing each data level as an independent SummarizedExperiment is that the QFeatures infrastructure maintains explicit links between the SummarizedExperiments that it stores. This allows for maximum traceability when processing data across multiple levels e.g., tracking which PSMs contribute to each peptide and which peptides contribute to a protein (Figure 2.2, modified from the QFeatures vignette with permission).\n\n\n\n\n\n\n\n\nFigure 2.2: Graphic representation of the explicit links between data levels when stored in a QFeatures object.\n\n\n\n\n\n\n\n\n\n\n\nNoteQFeatures nomenclature\n\n\n\nWhen talking about a QFeatures object, each dataset (individual SummarizedExperiment) can be referred to as an set (short for dataset). Previously, each dataset was referred to as an experimental assay, and some older resources may still use this term. However, the experimental assay should not be confused with the quantitative matrix section of a SummarizedExperiment, which is called the assay data.\n\n\nIn order to generate the explicit links between data levels, we need to import the lowest desired data level into a QFeatures object and aggregate upwards within the QFeatures infrastructure using the aggregateFeatures function, as we will later see in this course. If two SummarizedExperiments are generated separately and then added into the same QFeatures object, there will not automatically be links between them. In this case, if links are required, we can manually add links using the addAssayLink() function.\nThe best way to get our head around the QFeatures infrastructure is to import our data into a QFeatures object and start exploring.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Expression proteomics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Import and infrastructure</span>"
    ]
  },
  {
    "objectID": "materials/02_import_and_infrastructure.html#packages-and-working-directory",
    "href": "materials/02_import_and_infrastructure.html#packages-and-working-directory",
    "title": "2  Import and infrastructure",
    "section": "2.4 Packages and working directory",
    "text": "2.4 Packages and working directory\nDuring this course we will use several R/Bioconductor packages.\nLet’s begin by opening RStudio and loading these packages into our working environment.\n\nlibrary(\"QFeatures\")\nlibrary(\"limma\")\nlibrary(\"factoextra\")\nlibrary(\"org.Hs.eg.db\")\nlibrary(\"clusterProfiler\")\nlibrary(\"enrichplot\")\nlibrary(\"patchwork\")\nlibrary(\"tidyverse\")\nlibrary(\"pheatmap\")\nlibrary(\"ggupset\")\nlibrary(\"here\")\n\nSet your working directory to Course_Materials where you will find all the relevant data and R code files for this course. This can be achieved using the setwd function or by going to the menu Session -&gt; Set Working Directory -&gt; Choose Directory, in RStudio. Type list.files into your R console to check you can see the materials.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Expression proteomics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Import and infrastructure</span>"
    ]
  },
  {
    "objectID": "materials/02_import_and_infrastructure.html#importing-data-into-r",
    "href": "materials/02_import_and_infrastructure.html#importing-data-into-r",
    "title": "2  Import and infrastructure",
    "section": "2.5 Importing data into R",
    "text": "2.5 Importing data into R\nThere are several ways in which data can be imported into a QFeatures object. The most straightforward way is to first read the data into R as a data.frame using the read.delim function, and then import this data.frame into a QFeatures object.\nArguments to pass to readQFeatures:\n\nassayData = A data.frame, or any object that can be coerced into a data.frame, holding the data\nquantCols = an index for columns containing quantitative data. Can be a numeric index of the columns or a character vector containing the column names\nname = name we want to give our first experiment, in the QFeatures object\n\nLet’s read in the total proteome dataset from our experiment which is has been output from Proteome Discoverer at the PSM level. The file is called cell_cycle_total_proteome_analysis_PSMs.txt.\nThe first step is to read the data into R as a data.frame. We use the read.delim function.\n\ndf &lt;- read.delim(\"data/cell_cycle_total_proteome_analysis_PSMs.txt\")\n\nBefore we can create a QFeatures object we need to first identify which columns the quantitation data is stored in.\n\ndf %&gt;% names()\n\n [1] \"Checked\"                           \"Tags\"                             \n [3] \"Confidence\"                        \"Identifying.Node.Type\"            \n [5] \"Identifying.Node\"                  \"Search.ID\"                        \n [7] \"Identifying.Node.No\"               \"PSM.Ambiguity\"                    \n [9] \"Sequence\"                          \"Annotated.Sequence\"               \n[11] \"Modifications\"                     \"Number.of.Proteins\"               \n[13] \"Master.Protein.Accessions\"         \"Master.Protein.Descriptions\"      \n[15] \"Protein.Accessions\"                \"Protein.Descriptions\"             \n[17] \"Number.of.Missed.Cleavages\"        \"Charge\"                           \n[19] \"Original.Precursor.Charge\"         \"Delta.Score\"                      \n[21] \"Delta.Cn\"                          \"Rank\"                             \n[23] \"Search.Engine.Rank\"                \"Concatenated.Rank\"                \n[25] \"mz.in.Da\"                          \"MHplus.in.Da\"                     \n[27] \"Theo.MHplus.in.Da\"                 \"Delta.M.in.ppm\"                   \n[29] \"Delta.mz.in.Da\"                    \"Ions.Matched\"                     \n[31] \"Matched.Ions\"                      \"Total.Ions\"                       \n[33] \"Intensity\"                         \"Activation.Type\"                  \n[35] \"NCE.in.Percent\"                    \"MS.Order\"                         \n[37] \"Isolation.Interference.in.Percent\" \"SPS.Mass.Matches.in.Percent\"      \n[39] \"Average.Reporter.SN\"               \"Ion.Inject.Time.in.ms\"            \n[41] \"RT.in.min\"                         \"First.Scan\"                       \n[43] \"Last.Scan\"                         \"Master.Scans\"                     \n[45] \"Spectrum.File\"                     \"File.ID\"                          \n[47] \"Abundance.126\"                     \"Abundance.127N\"                   \n[49] \"Abundance.127C\"                    \"Abundance.128N\"                   \n[51] \"Abundance.128C\"                    \"Abundance.129N\"                   \n[53] \"Abundance.129C\"                    \"Abundance.130N\"                   \n[55] \"Abundance.130C\"                    \"Abundance.131\"                    \n[57] \"Quan.Info\"                         \"Peptides.Matched\"                 \n[59] \"XCorr\"                             \"Number.of.Protein.Groups\"         \n[61] \"Contaminant\"                       \"Percolator.q.Value\"               \n[63] \"Percolator.PEP\"                    \"Percolator.SVMScore\"              \n\n\nWe can see the quantitation data is located in columns 47 - 56 which start with “Abundance…” followed by the identifying TMT tag. Naming of the abundance columns is dependent on the third party software used and version, please make sure you check you identify the relevant column numbers containing the quantitation data in your own data.\nWe will pass this information to the quantCols argument when we use the readQFeatures function. As this is the first data import before any processing so let’s call this data level \"psms_raw\".\n\n## Import data into QF object\ncc_qf &lt;- readQFeatures(assayData = df,\n                       quantCols = 47:56, \n                       name = \"psms_raw\")\n\nChecking arguments.\n\n\nLoading data as a 'SummarizedExperiment' object.\n\n\nFormatting sample annotations (colData).\n\n\nFormatting data as a 'QFeatures' object.\n\n\n\n\n\n\n\n\nNoteFinding help files\n\n\n\nDuring this course we see many new functions. For more information on any R function type ? and the name of the function in the R console to bring up the relevant documentation. Or go to the Help menu in RStudio and Help -&gt; Search R Help. For example, to see the help files and documentation for the readQFeatures function type ?readQFeatures in your RStudio console. The help documentation highlights what structure the input data should be (a vector, dataframe or matrix etc.) as well as which arguments are required or optional. When using more complex functions, such as those which we will use for statistical analysis, it is advisable to consult the help documentation to ensure that you understand what the function’s default parameters are. This will help you to make sure that the function does what you are expecting it to do.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Expression proteomics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Import and infrastructure</span>"
    ]
  },
  {
    "objectID": "materials/02_import_and_infrastructure.html#accessing-information-within-a-qfeatures-object",
    "href": "materials/02_import_and_infrastructure.html#accessing-information-within-a-qfeatures-object",
    "title": "2  Import and infrastructure",
    "section": "2.6 Accessing information within a QFeatures object",
    "text": "2.6 Accessing information within a QFeatures object\nLet’s take a look at our newly created object.\n\ncc_qf\n\nAn instance of class QFeatures (type: bulk) with 1 set:\n\n [1] psms_raw: SummarizedExperiment with 45803 rows and 10 columns \n\n\nWe see that we have created a QFeatures object containing a single SummarizedExperiment called “psms_raw”. There are 45803 rows in the data, representing 45803 PSMs. We also see that there are 10 columns representing our 10 samples.\nWe can access individual SummarizedExperiments within a QFeatures object using standard double bracket nomenclature (this is how you would normally access items of a list in R). As with all indexing we can use the list position or dataset name.\n\n## Indexing using position\ncc_qf[[1]]\n\nclass: SummarizedExperiment \ndim: 45803 10 \nmetadata(0):\nassays(1): ''\nrownames(45803): 1 2 ... 45802 45803\nrowData names(54): Checked Tags ... Percolator.PEP Percolator.SVMScore\ncolnames(10): Abundance.126 Abundance.127N ... Abundance.130C\n  Abundance.131\ncolData names(0):\n\n## Indexing using name\ncc_qf[[\"psms_raw\"]]\n\nclass: SummarizedExperiment \ndim: 45803 10 \nmetadata(0):\nassays(1): ''\nrownames(45803): 1 2 ... 45802 45803\nrowData names(54): Checked Tags ... Percolator.PEP Percolator.SVMScore\ncolnames(10): Abundance.126 Abundance.127N ... Abundance.130C\n  Abundance.131\ncolData names(0):\n\n\nWithin each SummarizedExperiment(SE), the rowData, colData and assay data are accessed using rowData(), colData() and assay(), respectively. Let’s use these functions to explore the data structure further.\n\n2.6.1 The assay container\nLet’s start with the quantitation data of our SE called psms_raw. This is stored in the assay slot.\n\nassay(cc_qf[[\"psms_raw\"]])\n\nYou will see that the R console is populated with the quantitation data and the whole dataset is too big to print to the screen. Let’s use the head command to show only the 6 lines (default) of the quantitation data.\nType into your R console,\n\nhead(assay(cc_qf[[\"psms_raw\"]]))\n\n  Abundance.126 Abundance.127N Abundance.127C Abundance.128N Abundance.128C\n1          22.3           48.6           15.1           26.8           39.0\n2          19.8           31.3           14.1           19.5           32.1\n3           2.4            7.9            1.2            2.3            4.1\n4           2.2             NA             NA             NA             NA\n5           2.2            4.2            2.2            1.6            5.7\n6            NA            1.7             NA             NA             NA\n  Abundance.129N Abundance.129C Abundance.130N Abundance.130C Abundance.131\n1           21.6           41.6           38.7           21.2          34.7\n2           29.2           37.1           41.2           30.0          34.9\n3            1.0            3.6            7.0            3.1           2.7\n4            2.3            3.1             NA            1.7            NA\n5            1.4             NA            3.6            4.0           4.0\n6             NA             NA             NA             NA           2.7\n\n\nThis is equivalent to,\n\ncc_qf[[\"psms_raw\"]] %&gt;% \n  assay() %&gt;% \n  head()\n\n  Abundance.126 Abundance.127N Abundance.127C Abundance.128N Abundance.128C\n1          22.3           48.6           15.1           26.8           39.0\n2          19.8           31.3           14.1           19.5           32.1\n3           2.4            7.9            1.2            2.3            4.1\n4           2.2             NA             NA             NA             NA\n5           2.2            4.2            2.2            1.6            5.7\n6            NA            1.7             NA             NA             NA\n  Abundance.129N Abundance.129C Abundance.130N Abundance.130C Abundance.131\n1           21.6           41.6           38.7           21.2          34.7\n2           29.2           37.1           41.2           30.0          34.9\n3            1.0            3.6            7.0            3.1           2.7\n4            2.3            3.1             NA            1.7            NA\n5            1.4             NA            3.6            4.0           4.0\n6             NA             NA             NA             NA           2.7\n\n\nThe first code chunk uses nested functions and the second uses pipes. Throughout this course for ease of coding and clarity we will use pipes and where appropriate follow the tidyverse style of coding for clarity.\n\ncc_qf[[\"psms_raw\"]] %&gt;% \n  assay() %&gt;% \n  ncol()\n\n[1] 10\n\n\nWe see the assay slot contains 10 quantitative columns corresponding to the 10 samples in our experiment. We see some quantitative values and some missing values, here denotated NA.\n\n\n\n\n\n\nNoteThe dplyr pipe\n\n\n\nIn this course we will frequently use pipes, specifically the dplyr pipe part of the dplyr package and tidyverse. The pipe operator allows you to chain together multiple operations and code complex tasks in a more linear and understandable way, rather than having to nest multiple function calls or write multiple lines of code. For more information see Hadley Wickham’s Tidyverse at https://www.tidyverse.org.\n\n\n\n\n2.6.2 The rowData container\nNow let’s examine the the rowData slot,\n\ncc_qf[[\"psms_raw\"]] %&gt;%\n  rowData() %&gt;%\n  names()\n\n [1] \"Checked\"                           \"Tags\"                             \n [3] \"Confidence\"                        \"Identifying.Node.Type\"            \n [5] \"Identifying.Node\"                  \"Search.ID\"                        \n [7] \"Identifying.Node.No\"               \"PSM.Ambiguity\"                    \n [9] \"Sequence\"                          \"Annotated.Sequence\"               \n[11] \"Modifications\"                     \"Number.of.Proteins\"               \n[13] \"Master.Protein.Accessions\"         \"Master.Protein.Descriptions\"      \n[15] \"Protein.Accessions\"                \"Protein.Descriptions\"             \n[17] \"Number.of.Missed.Cleavages\"        \"Charge\"                           \n[19] \"Original.Precursor.Charge\"         \"Delta.Score\"                      \n[21] \"Delta.Cn\"                          \"Rank\"                             \n[23] \"Search.Engine.Rank\"                \"Concatenated.Rank\"                \n[25] \"mz.in.Da\"                          \"MHplus.in.Da\"                     \n[27] \"Theo.MHplus.in.Da\"                 \"Delta.M.in.ppm\"                   \n[29] \"Delta.mz.in.Da\"                    \"Ions.Matched\"                     \n[31] \"Matched.Ions\"                      \"Total.Ions\"                       \n[33] \"Intensity\"                         \"Activation.Type\"                  \n[35] \"NCE.in.Percent\"                    \"MS.Order\"                         \n[37] \"Isolation.Interference.in.Percent\" \"SPS.Mass.Matches.in.Percent\"      \n[39] \"Average.Reporter.SN\"               \"Ion.Inject.Time.in.ms\"            \n[41] \"RT.in.min\"                         \"First.Scan\"                       \n[43] \"Last.Scan\"                         \"Master.Scans\"                     \n[45] \"Spectrum.File\"                     \"File.ID\"                          \n[47] \"Quan.Info\"                         \"Peptides.Matched\"                 \n[49] \"XCorr\"                             \"Number.of.Protein.Groups\"         \n[51] \"Contaminant\"                       \"Percolator.q.Value\"               \n[53] \"Percolator.PEP\"                    \"Percolator.SVMScore\"              \n\n\nThe columns of our rowData contain non-quantitative information derived from the third party identification search. The exact names of variables will differ between software and change over time, but the key information that we need to know about each feature (here PSMs) will always be here.\nThis information includes the sequence of the peptide to which each PSM corresponds (\"Sequence\") as well as the master protein to which the peptide is assigned (\"Master.Protein.Accessions\"). We will come across more of the variables stored in the rowData when we come to data cleaning and filtering.\nWe again use the head command to print the first 6 rows of the rowData\n\ncc_qf[[\"psms_raw\"]] %&gt;%\n  rowData() %&gt;%\n  head()\n\nDataFrame with 6 rows and 54 columns\n      Checked      Tags  Confidence Identifying.Node.Type Identifying.Node\n  &lt;character&gt; &lt;logical&gt; &lt;character&gt;           &lt;character&gt;      &lt;character&gt;\n1       False        NA        High            Sequest HT    Sequest HT...\n2       False        NA        High            Sequest HT    Sequest HT...\n3       False        NA        High            Sequest HT    Sequest HT...\n4       False        NA        High            Sequest HT    Sequest HT...\n5       False        NA        High            Sequest HT    Sequest HT...\n6       False        NA        High            Sequest HT    Sequest HT...\n    Search.ID Identifying.Node.No PSM.Ambiguity      Sequence\n  &lt;character&gt;           &lt;integer&gt;   &lt;character&gt;   &lt;character&gt;\n1           A                   2 Unambiguou...        REEEMR\n2           A                   2 Unambiguou...     QQNGTASSR\n3           A                   2 Unambiguou...      CHMEENQR\n4           A                   2 Unambiguou...        QACQER\n5           A                   2 Unambiguou...     HSEAATAQR\n6           A                   2 Unambiguou... QQQQQQQHQQ...\n  Annotated.Sequence Modifications Number.of.Proteins Master.Protein.Accessions\n         &lt;character&gt;   &lt;character&gt;          &lt;integer&gt;               &lt;character&gt;\n1             rEEEmR N-Term(TMT...                  2             P49755; Q1...\n2          qQnGTASSR N-Term(TMT...                  1                    Q92917\n3           cHmEENQR N-Term(TMT...                  1                    Q96TC7\n4             qAcQER N-Term(TMT...                  1                    P26358\n5          hSEAATAQR N-Term(TMT...                  1                    Q14103\n6      qQQQQQQHQQ... N-Term(TMT...                  1                    Q6Y7W6\n  Master.Protein.Descriptions Protein.Accessions Protein.Descriptions\n                  &lt;character&gt;        &lt;character&gt;          &lt;character&gt;\n1               Transmembr...      P49755; Q1...        Transmembr...\n2               G-patch do...             Q92917        G-patch do...\n3               Regulator ...             Q96TC7        Regulator ...\n4               DNA (cytos...             P26358        DNA (cytos...\n5               Heterogene...             Q14103        Heterogene...\n6               GRB10-inte...             Q6Y7W6        GRB10-inte...\n  Number.of.Missed.Cleavages    Charge Original.Precursor.Charge Delta.Score\n                   &lt;integer&gt; &lt;integer&gt;                 &lt;integer&gt;   &lt;numeric&gt;\n1                          1         2                         2      0.1696\n2                          0         2                         2      0.1154\n3                          0         3                         3      1.0000\n4                          0         2                         2      0.2849\n5                          0         2                         2      0.4780\n6                          0         3                         3      0.4098\n   Delta.Cn      Rank Search.Engine.Rank Concatenated.Rank  mz.in.Da\n  &lt;numeric&gt; &lt;integer&gt;          &lt;integer&gt;         &lt;integer&gt; &lt;numeric&gt;\n1         0         1                  1                 3   547.776\n2         0         1                  1                 1   589.802\n3         0         1                  1                 1   450.202\n4         0         1                  1                 1   510.758\n5         0         1                  1                 1   600.320\n6         0         1                  1                 1   635.659\n  MHplus.in.Da Theo.MHplus.in.Da Delta.M.in.ppm Delta.mz.in.Da Ions.Matched\n     &lt;numeric&gt;         &lt;numeric&gt;      &lt;numeric&gt;      &lt;numeric&gt;  &lt;character&gt;\n1      1094.55           1094.55          -0.99       -0.00054          0/0\n2      1178.60           1178.60           0.06        0.00003          0/0\n3      1348.59           1348.59          -1.08       -0.00049          0/0\n4      1020.51           1020.51          -1.44       -0.00074          0/0\n5      1199.63           1199.63          -0.33       -0.00020          0/0\n6      1904.96           1904.96          -1.33       -0.00084          0/0\n  Matched.Ions Total.Ions Intensity Activation.Type NCE.in.Percent    MS.Order\n     &lt;integer&gt;  &lt;integer&gt; &lt;numeric&gt;     &lt;character&gt;      &lt;numeric&gt; &lt;character&gt;\n1            0          0  505446.3             CID             NA         MS2\n2            0          0  197059.8             CID             NA         MS2\n3            0          0   76685.6             CID             NA         MS2\n4            0          0  103765.7             CID             NA         MS2\n5            0          0  238514.8             CID             NA         MS2\n6            0          0   76203.7             CID             NA         MS2\n  Isolation.Interference.in.Percent SPS.Mass.Matches.in.Percent\n                          &lt;numeric&gt;                   &lt;integer&gt;\n1                           0.00000                          50\n2                          47.04837                          50\n3                          30.96584                          50\n4                           3.10827                          60\n5                          12.56510                          70\n6                           0.00000                          80\n  Average.Reporter.SN Ion.Inject.Time.in.ms RT.in.min First.Scan Last.Scan\n            &lt;numeric&gt;             &lt;numeric&gt; &lt;numeric&gt;  &lt;integer&gt; &lt;integer&gt;\n1                30.8                    60   39.4997       5816      5816\n2                28.7                    60   39.8252       5902      5902\n3                 3.5                    60   40.0773       5967      5967\n4                 0.9                    60   40.3077       6035      6035\n5                 2.8                    60   40.7688       6151      6151\n6                 0.4                    60   40.8788       6183      6183\n  Master.Scans Spectrum.File     File.ID Quan.Info Peptides.Matched     XCorr\n     &lt;integer&gt;   &lt;character&gt; &lt;character&gt; &lt;logical&gt;        &lt;integer&gt; &lt;numeric&gt;\n1         5810 NocArrest_...        F1.1        NA              104      1.71\n2         5898 NocArrest_...        F1.1        NA              125      1.82\n3         5961 NocArrest_...        F1.1        NA                4      0.94\n4         6031 NocArrest_...        F1.1        NA              105      1.86\n5         6147 NocArrest_...        F1.1        NA              136      1.82\n6         6177 NocArrest_...        F1.1        NA              285      1.22\n  Number.of.Protein.Groups Contaminant Percolator.q.Value Percolator.PEP\n                 &lt;integer&gt; &lt;character&gt;          &lt;numeric&gt;      &lt;numeric&gt;\n1                        2       False          0.0033430       0.030030\n2                        1       False          0.0009278       0.006729\n3                        1       False          0.0014400       0.012330\n4                        1       False          0.0009278       0.006435\n5                        1       False          0.0004399       0.002259\n6                        1       False          0.0006928       0.003668\n  Percolator.SVMScore\n            &lt;numeric&gt;\n1               0.132\n2               0.352\n3               0.260\n4               0.360\n5               0.536\n6               0.453\n\n\n\n\n2.6.3 The colData container\nThe final slot within our SummarizedExperiment is the colData.\n\ncc_qf[[\"psms_raw\"]] %&gt;%\n  colData()\n\nDataFrame with 10 rows and 0 columns\n\n\nWe see this is also a DataFrame structure.\n\ncc_qf[[\"psms_raw\"]] %&gt;%\n  colData() %&gt;% \n  class()\n\n[1] \"DFrame\"\nattr(,\"package\")\n[1] \"S4Vectors\"\n\n\nIt has 10 rows, one per sample, but no columns yet. This is because the sample metadata is not derived from an identification search and does not appear in our output .txt file. If we want to store extra information about our samples, we need to add this to the colData ourselves.\nEach row of this 10 by 0 DataFrame corresponds to some feature of the quantitation channel. The names of these rows are the TMT sample names,\n\ncc_qf[[\"psms_raw\"]] %&gt;%\n  colData() %&gt;% \n  rownames()\n\n [1] \"Abundance.126\"  \"Abundance.127N\" \"Abundance.127C\" \"Abundance.128N\"\n [5] \"Abundance.128C\" \"Abundance.129N\" \"Abundance.129C\" \"Abundance.130N\"\n [9] \"Abundance.130C\" \"Abundance.131\" \n\n\n\n\n2.6.4 Annotating samples by adding meta-data\nThe colData contains information about the samples. We can create a data.frame in R with information regarding the samples, replicates, conditions and TMT tag used or we can read in this information into R from a spreadsheet or .csv.\nLet’s open the file coldata.csv and then read it into R and add it to our QFeatures object. We use the read.csv function and pass the argument row.names = 1 which tells the function that the in the .csv file the row names are located in column 1.\n\n## Read in coldata .csv\nmetadata_df &lt;- read.csv(\"data/samples_meta_data.csv\", row.names = 1)\n\n## View data\nmetadata_df\n\n                sample rep     condition  tag\nAbundance.126  Control  NA Pre-treatment  126\nAbundance.127N     M_1   1             M 127N\nAbundance.127C     M_2   2             M 127C\nAbundance.128N     M_3   3             M 128N\nAbundance.128C    G1_1   1            G1 128C\nAbundance.129N    G1_2   2            G1 129N\nAbundance.129C    G1_3   3            G1 129C\nAbundance.130N    DS_1   1       Desynch 130N\nAbundance.130C    DS_2   2       Desynch 130C\nAbundance.131     DS_3   3       Desynch  131\n\n\nThis .csv file has 10 rows, importantly the rows are annotated using the names of the rowData of our object, e.g. “Abundance.126”, “Abundance.127N” etc. These names must match exactly to the column names of the samples we have in our existing QFeatures object.\nWe can add information from this .csv to our colData\n\n## Annotate colData with the sample data\ncolData(cc_qf) &lt;- metadata_df\n\n## Verify\ncolData(cc_qf)\n\nDataFrame with 10 rows and 4 columns\n                    sample       rep     condition         tag\n               &lt;character&gt; &lt;integer&gt;   &lt;character&gt; &lt;character&gt;\nAbundance.126      Control        NA Pre-treatm...         126\nAbundance.127N         M_1         1             M        127N\nAbundance.127C         M_2         2             M        127C\nAbundance.128N         M_3         3             M        128N\nAbundance.128C        G1_1         1            G1        128C\nAbundance.129N        G1_2         2            G1        129N\nAbundance.129C        G1_3         3            G1        129C\nAbundance.130N        DS_1         1       Desynch        130N\nAbundance.130C        DS_2         2       Desynch        130C\nAbundance.131         DS_3         3       Desynch         131\n\n## Apply this to the first set so that it is carried up\ncolData(cc_qf[[\"psms_raw\"]]) &lt;- colData(cc_qf)\n\nWe can also change the names of our samples to simplify our downstream coding as well as our visualisation. This is done by changing the colnames.\n\n ## Change col names to represent the sample\ncolnames(cc_qf[[\"psms_raw\"]]) &lt;- cc_qf$sample\n\n## Verify\ncolData(cc_qf[[\"psms_raw\"]])\n\nDataFrame with 10 rows and 4 columns\n             sample       rep     condition         tag\n        &lt;character&gt; &lt;integer&gt;   &lt;character&gt; &lt;character&gt;\nControl     Control        NA Pre-treatm...         126\nM_1             M_1         1             M        127N\nM_2             M_2         2             M        127C\nM_3             M_3         3             M        128N\nG1_1           G1_1         1            G1        128C\nG1_2           G1_2         2            G1        129N\nG1_3           G1_3         3            G1        129C\nDS_1           DS_1         1       Desynch        130N\nDS_2           DS_2         2       Desynch        130C\nDS_3           DS_3         3       Desynch         131\n\n\n\n\n\n\n\n\nExerciseExercise 1 - Challenge 1: Accessing information\n\n\n\n\n\n\nLevel: \nExplore the QFeatures object you have just created.\n\nHow many “sets” (SummarizedExperiments) do we currently have in the object?\nHow many PSMs have been identified in the data?\nHow do you access and view the quantitation data i.e. the relative abundance of each PSM across the samples?\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nTask 1\nWhen we type the name of the the object cc_qf a summary of the data is printed to the screen. This summary shows us a summary of the number of sets (SummarizedExperiments) we have in the data.\n\ncc_qf\n\nAn instance of class QFeatures (type: bulk) with 1 set:\n\n [1] psms_raw: SummarizedExperiment with 45803 rows and 10 columns \n\n\nSince a QFeatures object is a list of SummarizedExperiment objects, we can use length(cc_qf) to check how many objects we have inside it\n\nlength(cc_qf)\n\n[1] 1\n\n\nAlternatively, we can also extract the list of experiments directly using\n\nexperiments(cc_qf)\n\nExperimentList class object of length 1:\n [1] psms_raw: SummarizedExperiment with 45803 rows and 10 columns\n\n\n\nTask 2\nSimilarly, the object summary also tells us the number of PSMs. We can also extract the number of rows (PSMs in our case) for a given level (i.e. psms_raw)\n\ncc_qf[[\"psms_raw\"]] %&gt;% \n  nrow()\n\n[1] 45803\n\n\n\nTask 3\nWe can use the assay accessor,\n\ncc_qf[[\"psms_raw\"]] %&gt;% \n  assay()\n\nor equivalently,\n\nassay(cc_qf[[\"psms_raw\"]])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExerciseExercise 2 - Challenge 2: Calculating the number of peptides and proteins in our dataset\n\n\n\n\n\n\nLevel: \nExplore the information stored in the rowData from the Proteome Discoverer search. Look at the first few rows of this data matrix by using the head function and find a column that related to the peptide sequence information.\n\nWhat class is the rowData container? How many rows and columns are in this data structure?\nExtract the rowData and convert it to a tibble or data.frame. Find a column that contains information that relates to the peptide sequence. Pull this information and calculate how many unique peptide sequences we have in the dataset.\n\n\n\nHint\n\nUsing the unique function may be helpful.\n\n\nHow many protein groups (master proteins) have been identified in the dataset?\n\n\n\nHint\n\nAgain, convert the rowData to a tibble or data.frame and find a column that contains information that relates to protein identification.\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nTask 1\n\ncc_qf[[\"psms_raw\"]] %&gt;% \n  rowData() %&gt;% \n  class()\n\n[1] \"DFrame\"\nattr(,\"package\")\n[1] \"S4Vectors\"\n\n\nWe see that the rowData is of class \"DFrame\". This is a type of data frame. Will will convert this to a data.frame or tibble throughout this couse to make use of tidyverse functions.\n\ncc_qf[[\"psms_raw\"]] %&gt;% \n  rowData() %&gt;% \n  dim()\n\n[1] 45803    54\n\n\nWe see we have 45803 PSMs and 54 columns of information relating to each PSM.\n\nTask 2\nThere are many ways to extract the number of peptides from our datasets. The current data level shows the total number of PSMs (peptide spectrum matches). Although PSMs are fragments of identified peptide sequences, there can be many PSMs per unique peptide. The rowData is where we have stored information relating to our samples that are not quantitation data. The column Sequence contains the peptide sequence from which the PSM is is derived.\nIn the following lines of code we, - Extract the rowData - Convert the rowData which is DFrame to either a data.frame or tibble. We could use the as.data.frame or as_tibble functions, respectively, which we will allow us to use pipes. - Extract the column Sequence. - Use the unique function to group together PSMs from the same peptide. - Finally, we use the length command to count the number of peptides.\n\n## Using pipes\ncc_qf[[\"psms_raw\"]] %&gt;%\n  rowData() %&gt;%\n  as_tibble() %&gt;%\n  pull(Sequence) %&gt;%\n  unique() %&gt;%\n  length() \n\n[1] 26738\n\n\nAlternative solution (among many others)\n\n## Without pipes\nrd &lt;- rowData(cc_qf[[\"psms_raw\"]])\ntbl &lt;- unique(rd$Sequence) \nlength(tbl)\n\n[1] 26738\n\n\nWe have 26738 peptides.\n\nTask 3\nInformation regarding the protein groups is found in the Master.Protein.Accessions column.\n\n## Using pipes\ncc_qf[[\"psms_raw\"]] %&gt;%\n  rowData() %&gt;%\n  as_tibble() %&gt;%\n  pull(Master.Protein.Accessions) %&gt;%\n  unique() %&gt;%\n  length() \n\n[1] 5267\n\n\nAlternative solution (among many others)\n\n## Without pipes\ntbl &lt;- unique(rd$Master.Protein.Accessions) \nlength(tbl)\n\n[1] 5267\n\n\nWe have 5267 protein groups represented by 5267 master proteins.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExerciseExercise 3 - Challenge 3: Miscleavages\n\n\n\n\n\n\nLevel: \nOne of the pieces of information given by the Proteome Discoverer software used to produce the TMT data is the number of missed cleavages. This is stored in a rowData column named \"Number.of.Missed.Cleavages\". Can you count how many occurrences of missed cleavages there are in our data?\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\n\ncc_qf[[\"psms_raw\"]] %&gt;%\n  rowData() %&gt;%\n  as_tibble() %&gt;%\n  count(Number.of.Missed.Cleavages)\n\n# A tibble: 3 × 2\n  Number.of.Missed.Cleavages     n\n                       &lt;int&gt; &lt;int&gt;\n1                          0 34988\n2                          1  9709\n3                          2  1106\n\n\nWe see that during the identification search missed cleavages were limited to a maximum of 2 missed cleavage sites. This is typical in bottom-up proteomics.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nThe QFeatures infrastructure provides a convenient and transparent way to store proteomics data across multiple levels.\nA QFeatures object stores each data level as a SummarizedExperiment, also called an set (dataset), with explicit links maintained between features across different sets (e.g., links between all PSMs contributing to a peptide).\nExplicit links between sets are (i) generated automatically when aggregating data using aggregateFeatures or (ii) added manually using the addAssayLinks function, as we will see later on.\nWithin each SummarizedExperiment the feature data is stored within the rowData, quantitative data in the assay, and sample metadata in the colData. These slots are accessible using their corresponding functions.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Expression proteomics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Import and infrastructure</span>"
    ]
  },
  {
    "objectID": "materials/02_import_and_infrastructure.html#references",
    "href": "materials/02_import_and_infrastructure.html#references",
    "title": "2  Import and infrastructure",
    "section": "References",
    "text": "References",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Expression proteomics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Import and infrastructure</span>"
    ]
  },
  {
    "objectID": "materials/03_data_processing.html",
    "href": "materials/03_data_processing.html",
    "title": "3  Data cleaning",
    "section": "",
    "text": "3.1 Creating a copy of the raw data\nNow that we have our PSM level quantitative data stored in a QFeatures object, the next step is to carry out some data cleaning. However, as with all data analyses, it is sensible to keep a copy of our raw data in case we need to refer back to it later.\nTo create a copy of the psms_raw data we can first extract the SummarizedExperiment and then add it back to our QFeatures object using the addAssay function. When we do this, we give the second SummarizedExperiment (our copy) a new name. Here we will call it psms_filtered as by the end of our data cleaning and filtering, this copy will have had the unwanted data removed.\n## Extract a copy of the raw PSM-level data\nraw_data_copy &lt;- cc_qf[[\"psms_raw\"]] \n\n## Re-add the assay to our QFeatures object with a new name\ncc_qf &lt;- addAssay(x = cc_qf, \n                  y = raw_data_copy, \n                  name = \"psms_filtered\")\n\n## Verify\ncc_qf\n\nAn instance of class QFeatures (type: bulk) with 2 sets:\n\n [1] psms_raw: SummarizedExperiment with 45803 rows and 10 columns \n [2] psms_filtered: SummarizedExperiment with 45803 rows and 10 columns\nWe now see that a second SummarizedExperiment has been added to the QFeatures object. When we use addAssay to add to a QFeatures object, the newly created SummarizedExperiment does not automatically have links to the pre-existing SummarizedExperiments.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Expression proteomics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data cleaning</span>"
    ]
  },
  {
    "objectID": "materials/03_data_processing.html#creating-a-copy-of-the-raw-data",
    "href": "materials/03_data_processing.html#creating-a-copy-of-the-raw-data",
    "title": "3  Data cleaning",
    "section": "",
    "text": "NoteAssay links\n\n\n\nQFeatures maintains the hierarchical links between quantitative levels whilst allowing easy access to all data levels for individual features (PSMs, peptides and proteins) of interest. This is fundamental to the QFeatures infrastructure and will be exemplified throughout this course.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Expression proteomics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data cleaning</span>"
    ]
  },
  {
    "objectID": "materials/03_data_processing.html#data-cleaning",
    "href": "materials/03_data_processing.html#data-cleaning",
    "title": "3  Data cleaning",
    "section": "3.2 Data cleaning",
    "text": "3.2 Data cleaning\nNow that we have created a copy of our raw data, we can start the process of data cleaning. We will only remove data from the second copy of our data, the one which we have called \"psms_filtered\". Standard data cleaning steps in proteomics include the removal of features which:\n\nDo not have an associated master protein accession\nHave a (master) protein accession matching to a contaminant protein\nDo not have quantitative data\n\nThe above steps are necessary for all quantitative proteomics datasets, regardless of the experimental goal or data level (PSM or peptide). If a feature cannot be identified and quantified then it does not contribute useful information to our analysis. Similarly, contaminant proteins that are introduced intentionally as reagents (e.g., trypsin) or accidentally (e.g., human keratins) do not contribute to our biological question as they were not originally present in the samples.\nWe also have the option to get rid of any lower quality data by removing those features which:\n\nAre not rank 1\nAre not unambiguous\n\nSince each spectrum can have multiple potential matches (PSMs), the software used for our identification search provides some parameters to help us decide how confident we are in a match. Firstly, each PSM is given a rank based on the probability of it being incorrect - the PSM with the lowest probability of being wrong is allocated rank 1. Secondly, each PSM is assigned a level of ambiguity to tell us whether it was easy to assign with no other options (unambiguous), whether it was selected as the best match from a series of potential matches (selected), or whether it was not possible to distinguish between potential matches (ambiguous). High quality identifications should be both rank 1 and unambiguous. The exact filters we set here would depend on how exploratory or stringent we wish to be.\nFinally, depending upon the experimental question and goal, we may also wish to remove features which:\n\nAre not unique\n\nThe information regarding each of these parameters is present as a column in the output of our identification search, hence is present in the rowData of our QFeatures object.\n\n\n\n\n\n\nNoteThird party software\n\n\n\nThe use-case data was searched using the Proteome Discoverer software (Thermo Fisher Scientific). This is one among several third party softwares available for protein identification. Each software has it’s own naming conventions, formatting and metrics for quantitation and associated metadata. As such, users following this course with their own data should be mindful of this and adapt the proposed filters and steps accordingly. MaxQuant is a free, open-source alternative software for database searches of MS data. We have provided details on the MaxQuant equivalent column names to those used in this course here.\n\n\n\n3.2.1 Non-specific filtering\nTo remove features based on variables within our rowData we make use of the filterFeatures function. This function takes a QFeatures object as its input and then filters this object against a condition based on the rowData (indicated by the ~ operator). If the condition is met and returns TRUE for a feature, this feature will be kept.\nThe filterFeatures function provides the option to apply our filter (i) to the whole QFeatures object and all of its SummarizedExperiments, or (ii) to specific SummarizedExperiments within the QFeatures object. We wish to only filter on the data in our QFeatures object called psms_filtered (and not every dataset) so in the following code chunks we always pass the argument i = \"psms_filtered\".\nLet’s begin by filtering out any PSMs that do not have a master protein. These instances are characterised by an empty character i.e. \"\".\n\n## Before filtering\ncc_qf\n\nAn instance of class QFeatures (type: bulk) with 2 sets:\n\n [1] psms_raw: SummarizedExperiment with 45803 rows and 10 columns \n [2] psms_filtered: SummarizedExperiment with 45803 rows and 10 columns \n\ncc_qf &lt;- cc_qf %&gt;% \n  filterFeatures(~ Master.Protein.Accessions != \"\", i = \"psms_filtered\")\n\n'Master.Protein.Accessions' found in 2 out of 2 assay(s).\n\n## After filtering\ncc_qf\n\nAn instance of class QFeatures (type: bulk) with 2 sets:\n\n [1] psms_raw: SummarizedExperiment with 45803 rows and 10 columns \n [2] psms_filtered: SummarizedExperiment with 45691 rows and 10 columns \n\n\nWe see a message printed to the screen \"Master.Protein.Accessions' found in 2 out of 2 assay(s)\" this is informing us that the column \"Master.Protein.Accessions\" is present in the rowData of the two SummarizedExperiments in our QFeatures object. We can see from looking at our cc_qf object before and after filtering we have lost several hundred PSMs that do not have a master protein.\nAs mentioned above it is standard practice in proteomics to filter MS data for common contaminants. This is done by using a carefully curated, sample-specific contaminant database. In this study the data was searched against the Hao Group’s Protein Contaminant Libraries for DDA and DIA Proteomics Frankenfield et al. (2022) during the database search. The PD software flags PSMs that originate from proteins that match a contaminant and this is recorded in the “Contaminant” column in the PD output and propagated to the rowData of our QFeatures object.\n\n\n\n\n\n\nExerciseExercise 1 - Challenge 1: Filtering for contaminants\n\n\n\n\n\n\nLevel: \nThe Proteome Discoverer software flags a potential contaminant in the Contaminants column found in the rowData of the SummarizedExperiment.\nWe can count how many contaminants there are using,\n\ncc_qf[[\"psms_filtered\"]] %&gt;%  \n  rowData() %&gt;% \n  as_tibble() %&gt;% \n  count(Contaminant)\n\n# A tibble: 2 × 2\n  Contaminant     n\n  &lt;chr&gt;       &lt;int&gt;\n1 False       42751\n2 True         2940\n\n\nExamine the class of this column,\n\ncc_qf[[\"psms_filtered\"]] %&gt;%  \n  rowData() %&gt;% \n  as_tibble() %&gt;% \n  pull(Contaminant) %&gt;% \n  class()\n\n[1] \"character\"\n\n\nCareful: it is not a logical TRUE or FALSE. It is a character and takes two values, \"True\" or \"False\".\n\nUse the filterFeatures function on the psms_filtered dataset and filter out any contaminants which have been flagged as \"True\".\nHow many PSMs are left after this step?\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\n\ncc_qf &lt;- cc_qf %&gt;% \n  filterFeatures(~ Contaminant != \"True\", i = \"psms_filtered\")\n\n'Contaminant' found in 2 out of 2 assay(s).\n\n## After filtering\ncc_qf\n\nAn instance of class QFeatures (type: bulk) with 2 sets:\n\n [1] psms_raw: SummarizedExperiment with 45803 rows and 10 columns \n [2] psms_filtered: SummarizedExperiment with 42751 rows and 10 columns \n\n\nWe now have 42751 PSMs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteMore on contaminants\n\n\n\nIt is also possible to filter against your own contaminants list uploaded into R. It may be that you have a second list or newer list you’d like to search against, or you perhaps did not add a contaminants list to filter against in the identification search (which was performed in third party software). Full details including code can be found in Hutchings et al. (2024).\nNote: Filtering on “Protein.Accessions”, rather than “Master.Protein.Accessions” ensures the removal of PSMs which matched to a protein group containing a contaminant protein, even if the contaminant protein is not the group’s master protein.\n\n\nOne of the next filtering steps is to examine to see if we have any PSMs which lack quantitative data. In outputs derived from Proteome Discoverer this information is included in the “Quan.Info” column where PSMs are annotated as having “NoQuanLabels”. For users who have considered both lysine and N-terminal TMT labels as static modifications, the data should not contain any PSMs without quantitative information. See Hutchings et al. (2024) for how to filter your data if you have TMT modifications set as dynamic.\n\ncc_qf[[\"psms_filtered\"]] %&gt;%\n  rowData() %&gt;% \n  as_tibble() %&gt;%\n  pull(Quan.Info) %&gt;%\n  table()\n\n&lt; table of extent 0 &gt;\n\n\nWe see we have no annotation in this column and no PSMs lacking quantitative information.\n\n\n\n\n\n\nExerciseExercise 2 - Challenge 2: PSM ranking\n\n\n\n\n\n\nLevel: \nSince individual spectra can have multiple candidate PSMs, Proteome Discoverer uses a scoring algorithm to determine the probability of a PSM being incorrect. Once each candidate PSM has been given a score, the one with the lowest score (lowest probability of being incorrect) is allocated rank 1. The PSM with the second lowest probability of being incorrect is rank 2, and so on. For the analysis, we only want rank 1 PSMs to be retained. The majority of search engines, including SequestHT (used in this analysis), also provide their own PSM rank. To be conservative and ensure accurate quantitation, we also only retain PSMs that have a search engine rank of 1.\n\nFind the columns Rank and Search.Engine.Rank in the dataset and tabulate how many PSMs we have at each level\nUse filterFeatures and keep,\n\n\nPSMs with a Rank of 1\nPSMs with a Search.Engine.Rank of 1\nHigh confidence PSMs that have been unambiguously assigned\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nPart 1\nFirst let’s examine the columns in the rowData\n\ncc_qf[[\"psms_filtered\"]] %&gt;%\n  rowData() %&gt;% \n  names()\n\n [1] \"Checked\"                           \"Tags\"                             \n [3] \"Confidence\"                        \"Identifying.Node.Type\"            \n [5] \"Identifying.Node\"                  \"Search.ID\"                        \n [7] \"Identifying.Node.No\"               \"PSM.Ambiguity\"                    \n [9] \"Sequence\"                          \"Annotated.Sequence\"               \n[11] \"Modifications\"                     \"Number.of.Proteins\"               \n[13] \"Master.Protein.Accessions\"         \"Master.Protein.Descriptions\"      \n[15] \"Protein.Accessions\"                \"Protein.Descriptions\"             \n[17] \"Number.of.Missed.Cleavages\"        \"Charge\"                           \n[19] \"Original.Precursor.Charge\"         \"Delta.Score\"                      \n[21] \"Delta.Cn\"                          \"Rank\"                             \n[23] \"Search.Engine.Rank\"                \"Concatenated.Rank\"                \n[25] \"mz.in.Da\"                          \"MHplus.in.Da\"                     \n[27] \"Theo.MHplus.in.Da\"                 \"Delta.M.in.ppm\"                   \n[29] \"Delta.mz.in.Da\"                    \"Ions.Matched\"                     \n[31] \"Matched.Ions\"                      \"Total.Ions\"                       \n[33] \"Intensity\"                         \"Activation.Type\"                  \n[35] \"NCE.in.Percent\"                    \"MS.Order\"                         \n[37] \"Isolation.Interference.in.Percent\" \"SPS.Mass.Matches.in.Percent\"      \n[39] \"Average.Reporter.SN\"               \"Ion.Inject.Time.in.ms\"            \n[41] \"RT.in.min\"                         \"First.Scan\"                       \n[43] \"Last.Scan\"                         \"Master.Scans\"                     \n[45] \"Spectrum.File\"                     \"File.ID\"                          \n[47] \"Quan.Info\"                         \"Peptides.Matched\"                 \n[49] \"XCorr\"                             \"Number.of.Protein.Groups\"         \n[51] \"Contaminant\"                       \"Percolator.q.Value\"               \n[53] \"Percolator.PEP\"                    \"Percolator.SVMScore\"              \n\n\nLet’s examine the columns Rank and Search.Engine.Rank,\n\ncc_qf[[\"psms_filtered\"]] %&gt;%\n  rowData() %&gt;% \n  as_tibble() %&gt;%\n  count(Rank)\n\n# A tibble: 7 × 2\n   Rank     n\n  &lt;int&gt; &lt;int&gt;\n1     1 42185\n2     2   442\n3     3    95\n4     4    20\n5     5     3\n6     6     5\n7     7     1\n\ncc_qf[[\"psms_filtered\"]] %&gt;%\n  rowData() %&gt;% \n  as_tibble() %&gt;%\n  count(Search.Engine.Rank)\n\n# A tibble: 7 × 2\n  Search.Engine.Rank     n\n               &lt;int&gt; &lt;int&gt;\n1                  1 41794\n2                  2   798\n3                  3   122\n4                  4    28\n5                  5     3\n6                  6     4\n7                  7     2\n\n\nWe can also visualise this by piping the results to ggplot\n\ncc_qf[[\"psms_filtered\"]] %&gt;%\n  rowData() %&gt;% \n  as_tibble() %&gt;%\n  count(Rank) %&gt;% \n  ggplot(aes(Rank, n)) + \n  geom_col() + \n  scale_y_log10() \n\n\n\n\n\n\n\ncc_qf[[\"psms_filtered\"]] %&gt;%\n  rowData() %&gt;% \n  as_tibble() %&gt;%\n  count(Search.Engine.Rank) %&gt;% \n  ggplot(aes(Search.Engine.Rank, n)) + \n  geom_col() + \n  scale_y_log10() \n\n\n\n\n\n\n\n\nPart 2\nNow let’s keep only PSMs with a rank of 1,\n\ncc_qf &lt;- cc_qf %&gt;% \n  filterFeatures(~ Rank == 1, i = \"psms_filtered\") %&gt;%\n  filterFeatures(~ Search.Engine.Rank == 1, i = \"psms_filtered\") %&gt;%\n  filterFeatures(~ PSM.Ambiguity == \"Unambiguous\", i = \"psms_filtered\") \n\n'Rank' found in 2 out of 2 assay(s).\n\n\n'Search.Engine.Rank' found in 2 out of 2 assay(s).\n\n\n'PSM.Ambiguity' found in 2 out of 2 assay(s).\n\n## After filtering\ncc_qf\n\nAn instance of class QFeatures (type: bulk) with 2 sets:\n\n [1] psms_raw: SummarizedExperiment with 45803 rows and 10 columns \n [2] psms_filtered: SummarizedExperiment with 41794 rows and 10 columns \n\n\nWe now have 41794 PSMs.\n\n\n\n\n\n\n\n\nIn quantitative proteomics when thinking about what PSMs to consider for take forward for quantitation we consider PSM uniqueness. By definition uniqueness in this context refers to (i) PSMs corresponding to a single protein only, or it can also refer to (ii) PSMs that map to multiple proteins within a single protein group. This distinction is ultimately up to the user. We do not consider PSMs corresponding to razor and shared peptides as these are linked to multiple proteins across multiple protein groups.\nIn this workflow, the final grouping of peptides to proteins will be done based on master protein accession. Therefore, differential expression analysis will be based on protein groups, and we here consider unique as any PSM linked to only one protein group. This means removing PSMs where “Number.of.Protein.Groups” is not equal to 1.\n\ncc_qf &lt;- cc_qf %&gt;% \n  filterFeatures(~ Number.of.Protein.Groups == 1, i = \"psms_filtered\")\n\n'Number.of.Protein.Groups' found in 2 out of 2 assay(s).\n\n## After filtering\ncc_qf\n\nAn instance of class QFeatures (type: bulk) with 2 sets:\n\n [1] psms_raw: SummarizedExperiment with 45803 rows and 10 columns \n [2] psms_filtered: SummarizedExperiment with 40044 rows and 10 columns \n\n\n\n\n3.2.2 Addititional quality control filters available for TMT data\n\n\n\n\n\n\n\n\n\nAs well as the completion of standard data cleaning steps which are common to all quantitative proteomics experiments (see above), different experimental methods are accompanied by additional data processing considerations. Although we cannot provide an extensive discussion of all methods, we will draw attention to three key quality control parameters to consider for our use-case TMT data.\n\nAverage reporter ion signal-to-noise (S/N)\n\nThe quantitation of a TMT experiment is dependent upon the measurement of reporter ion signals from either the MS2 or MS3 spectra. Since reporter ion measurements derived from a small number of ions are more prone to stochastic ion effects and reduced quantitative accuracy, we want to remove PSMs that rely such quantitation to ensure high data quality. When using an Orbitrap analyser, the number of ions is proportional to the S/N ratio of a peak. Hence, removing PSMs that have a low S/N value acts as a proxy for removing low quality quantitation.\n\nIsolation interference (%)\n\nIsolation interference occurs when multiple TMT-labelled precursor peptides are co-isolated within a single data acquisition window. All co-isolated peptides go on to be fragmented together and reporter ions from all peptides contribute to the reporter ion signal. Hence, quantification for the identified peptide becomes inaccurate. To minimise the co-isolation interference problem observed at MS2, MS3-based analysis can be used McAlister et al. (2014). Nevertheless, we remove PSMs with a high percentage isolation interference to prevent the inclusion of inaccurate quantitation values.\n\nSynchronous precursor selection mass matches (%) - SPS-MM\n\nSPS-MM is a parameter unique to the Proteome Discoverer software which lets users quantify the percentage of MS3 fragments that can be explicitly traced back to the precursor peptides. This is important given that quantitation is based on the MS3 spectra.\nHere we will apply the default quality control thresholds as suggested by Thermo Fisher and keep features with:\n\nAverage reporter ion S/N &gt;= 10\nIsolation interference &lt; 75%\nSPS-MM &gt;= 65%\n\nWe also remove features that do not have information regarding these quality control parameters i.e., have an NA value in the corresponding columns of the rowData. To remove these features we include the na.rm = TRUE argument.\n\ncc_qf &lt;- cc_qf %&gt;% \n  filterFeatures(~ Average.Reporter.SN &gt;= 10, \n                 na.rm = TRUE, i = \"psms_filtered\") %&gt;%\n  filterFeatures(~ Isolation.Interference.in.Percent &lt;= 75, \n                 na.rm = TRUE, i = \"psms_filtered\") %&gt;%\n  filterFeatures(~ SPS.Mass.Matches.in.Percent &gt;= 65, \n                 na.rm = TRUE, i = \"psms_filtered\")\n\n'Average.Reporter.SN' found in 2 out of 2 assay(s).\n\n\n'Isolation.Interference.in.Percent' found in 2 out of 2 assay(s).\n\n\n'SPS.Mass.Matches.in.Percent' found in 2 out of 2 assay(s).\n\n\nIn reality, we advise that users take a look at their data to decide whether the default quality control thresholds applied above are appropriate. If there is reason to believe that the raw MS data was of lower quality than desired, it may be sensible to apply stricter thresholds here. Similarly, if users wish to carry out a more stringent or exploratory analyses, these thresholds can be altered accordingly. For code and more information please see Hutchings et al. (2024).\n\n\n3.2.3 Controlling false discovery rate (FDR)\n\n\n\n\n\n\n\n\n\nThe data cleaning steps that we have carried out so far have mainly been concerned with achieving high quality quantitation. However, we also want to be confident in our PSMs and their corresponding peptide and protein identifications.\nTo identify the peptides present in each of our samples we used a third party software to carry out a database search. Raw MS spectra are matched to theoretical spectra that are generated via in silico digestion of our desired protein database. The initial result of this search is a list of peptide spectrum matches, PSMs. A database search will identify PSMs for the majority of MS spectra, but only a minority of these will be true positives. In the same way that experiments must be conducted with controls, database search identifications need to be statistically validated to avoid false positives. The most widely used method for reducing the number of false positive identifications is to control the false discovery rate (FDR).\n\nTarget-decoy approach to FDR control\nThe most common approach to FDR control is the target-decoy approach. This involves searching the raw MS spectra against both a target and decoy database. The target database contains all protein sequences which could be in our sample (the human proteome and potential contaminants). The decoy database contains fake proteins that should not be present in the sample. For example, a decoy database may contain reversed or re-shuffled sequences. The main principle is that by carrying out these two searches we are able to estimate the the proportion of total PSMs that are false (matched to the decoy sequences) and apply score-based thresholding to shift this false discovery rate to a desired maximum. Typically expression proteomics utilises a maximum FDR of 0.01 or 0.05, that is to say 1% or 5% of PSMs are accepted false positives.\n\n\nPSM level local FDR\nThe first FDR to be calculated is that for the PSMs themselves, often referred to as local FDR. As mentioned previously, each PSM is given a score indicating how likely it is to be correct or incorrect, depending on the search engine used. Based on these scores, each candidate PSM is ranked such that rank 1 PSMs have the highest probability of being correct or lowest probability of being incorrect. To calculate the local FDR for a PSM, the score for that PSM is taken as a threshold, and all PSMs with a score equal to or better than that PSM are extracted. The proportion of false positives within this population of PSMs is taken as its local FDR. In this way, the FDR can be thought of as the proportion of false positives above the critical score threshold.\n\n\n\n\n\n\n\n\n\nAn illustration of the local FDR calculation used to assign each PSM an FDR.\n\n\n\n\nIn the parameters for our identification search using Proteome Discoverer we specified a PSM level FDR threshold. We did this by allocating PSM confidence levels as ‘high’, ‘medium’ or ‘low’ depending on whether their FDR was &lt; 0.01, &lt; 0.05 or &gt; 0.05. We then told Proteome Discoverer to only retain high confidence PSMs, those with a local FDR &lt; 0.01. The local FDR indirectly tells us the probability of a PSM being a false positive, given the score it has.\nLet’s double check that we only have high confidence PSMs.\n\ncc_qf[[\"psms_raw\"]] %&gt;%\n  rowData() %&gt;%\n  as_tibble() %&gt;%\n  pull(Confidence) %&gt;%\n  table()\n\n.\n High \n45803 \n\n\nAs expected, all of the PSMs in our data are of high confidence.\n\n\nProtein level FDR thresholding is still required\nAlthough we have already applied a PSM level FDR threshold to the data (during the identification search), it is highly recommended to set a protein level FDR threshold. This is because each peptide can have multiple PSMs and each protein can be supported by multiple peptides, thus resulting in the potential for amplification of the FDR during data aggregation. At a given PSM level score threshold, the peptide level FDR is greater than the PSM level FDR. Similarly, the protein level FDR is greater than the peptide level FDR.\nTo be absolutely confident in our final protein data we need to apply a protein level FDR. Unfortunately, third party software typically generates output tables in a sequential manner and there is currently no way to include information about the protein level FDR in the PSM level output. Therefore, we have to import the protein level output to get this information. The file is called cell_cycle_total_proteome_analysis_Proteins.txt.\nWe begin by using the read.delim function to read in the data,\n\n## Applying protein level FDR - import protein output from database search\nprotein_data_PD &lt;- read.delim(file = \"data/cell_cycle_total_proteome_analysis_Proteins.txt\")\n\nLet’s check the dimensions of the protein level output,\n\ndim(protein_data_PD)\n\n[1] 4904   72\n\n\nThe output contains 4904 proteins. Let’s compare this to the number of master protein accessions in our raw and filtered PSM data,\n\n## Number of master proteins in our raw data\ncc_qf[[\"psms_raw\"]] %&gt;%\n  rowData() %&gt;%\n  as_tibble() %&gt;%\n  pull(Master.Protein.Accessions) %&gt;%\n  unique() %&gt;%\n  length()\n\n[1] 5267\n\n## Number of master proteins in our filtered data\ncc_qf[[\"psms_filtered\"]] %&gt;%\n  rowData() %&gt;%\n  as_tibble() %&gt;%\n  pull(Master.Protein.Accessions) %&gt;%\n  unique() %&gt;%\n  length()\n\n[1] 3912\n\n\nIt looks like the protein level output has fewer proteins than the raw PSM file but more proteins than our filtered PSM data. This tells us that Proteome Discoverer has done some filtering throughout the process of aggregating from PSM to peptide to protein, but not as much filtering as we have done manually in R. This makes sense because we did not set any quality control thresholds on co-isolation interference, reporter ion S/N ratio or SPS-MM during the search.\nNow let’s look to see where information about the protein level FDR is stored.\n\nnames(protein_data_PD) %&gt;% \n  head(10)\n\n [1] \"Checked\"                         \"Tags\"                           \n [3] \"Protein.FDR.Confidence.Combined\" \"Master\"                         \n [5] \"Proteins.Unique.Sequence.ID\"     \"Protein.Group.IDs\"              \n [7] \"Accession\"                       \"Description\"                    \n [9] \"Sequence\"                        \"FASTA.Title.Lines\"              \n\n\nWe have a column called \"Protein.FDR.Confidence.Combined\". Let’s add this information to our rowData of the psms_filtered set of our QFeatures object.\nFirst let’s extract the rowData and convert it to a data.frame so we can make use of dplyr,\n\n## Extract the rowData and convert to a data.frame\npsm_data_QF &lt;- \n  cc_qf[[\"psms_filtered\"]] %&gt;% \n  rowData() %&gt;% \n  as.data.frame()\n\nNow let’s subset the protein_data_PD data to keep only accession and FDR information,\n\n## Select only the Accession and FDR info\nprotein_data_PD &lt;- \n  protein_data_PD %&gt;% \n  select(Accession, Protein.FDR.Confidence.Combined)\n\nNow let’s use dplyr and the left_join function to do the matching between the protein PD data and our PSM data for us. We need to specify how we join them so we need to specify the argument by = c(\"Master.Protein.Accessions\" = \"Accession\") to tell R we wish to join the two datasets by their protein Accessions. Note, in the rowData of the QFeatures object we have \"Master.Protein.Accessions\" and in the PD protein data the column is just called \"Accessions\". Check with your own data the explicit naming of your data files as this will differ between third party softwares.\n\n## Use left.join from dplyr to add the FDR data to PSM rowData data.frame \nfdr_confidence &lt;- left_join(x = psm_data_QF,  \n                            y = protein_data_PD,\n                            by = c(\"Master.Protein.Accessions\" = \"Accession\")) %&gt;% \n  pull(\"Protein.FDR.Confidence.Combined\")\n\n\n\n\n\n\n\nNoteJoining data.frames\n\n\n\nNote: when using left_join we specify the argument by = c(\"Master.Protein.Accessions\" = \"Accession\") (note the = equals sign between the names). This tells left_join we want to match join the two data.frames by matching accession numbers between the the Master.Protein.Accessions column in psm_data_QF and the column called Accession in the protein_data_PD data.\n\n\nNow let’s add the FDR information back to the QFeatures object,\n\n## Now add this data to the QF object\nrowData(cc_qf[[\"psms_filtered\"]])$Protein.FDR.Confidence &lt;- fdr_confidence\n\nNow we can print a table to see how many of the PSMs in our data were found to have a high confidence after protein level FDR calculations.\n\ncc_qf[[\"psms_filtered\"]] %&gt;%\n  rowData() %&gt;% \n  as_tibble() %&gt;% \n  pull(Protein.FDR.Confidence) %&gt;%\n  table()\n\n.\n  High    Low Medium \n 25699      5     84 \n\n\nMost of the PSMs in our data were found to originate from proteins with high confidence (FDR &lt; 0.01 at protein level), but there are a few that are medium or low confidence. Even though we set a PSM level FDR threshold of 0.01, some of the resulting proteins still exceed this FDR due to amplification of error during aggregation.\nWe can now use the filterFeatures function to remove PSMs that do not have a Protein_Confidence of ‘High’.\n\ncc_qf &lt;- cc_qf %&gt;%\n  filterFeatures(~ Protein.FDR.Confidence == \"High\", \n                 i = \"psms_filtered\")\n\n'Protein.FDR.Confidence' found in 1 out of 2 assay(s).\n\n\nNo filter applied to the following assay(s) because one or more\nfiltering variables are missing in the rowData: psms_raw. You can\ncontrol whether to remove or keep the features using the 'keep'\nargument (see '?filterFeature').\n\n\nNotice, we see a message output to the terminal after running the function that telling us that Protein.FDR.Confidence was only found in 1 of our experiment assays and as such no filter was applied to the raw data. This is correct and expected.\n\ncc_qf[[\"psms_filtered\"]] %&gt;%\n  rowData %&gt;%\n  as_tibble %&gt;%\n  pull(Protein.FDR.Confidence) %&gt;%\n  table()\n\n.\n High \n25699 \n\n\nFor more information about how FDR values are calculated and used see Prieto and Vázquez (2019).",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Expression proteomics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data cleaning</span>"
    ]
  },
  {
    "objectID": "materials/03_data_processing.html#management-of-missing-data",
    "href": "materials/03_data_processing.html#management-of-missing-data",
    "title": "3  Data cleaning",
    "section": "3.3 Management of missing data",
    "text": "3.3 Management of missing data\n\n\n\n\n\n\n\n\n\nHaving cleaned our data, the next step is to deal with missing values. It is important to be aware that missing values can arise for different reasons in MS data, and these reasons determine the best way to deal with the missing data.\n\nBiological reasons - a peptide may be genuinely absent from a sample or have such low abundance that it is below the limit of MS detection\nTechnical reasons - technical variation and the stochastic nature of MS (particularly using DDA) may lead to some peptides not being quantified. Some peptides have a lower ionization efficiency which makes them less compatible with MS analysis\n\nMissing values that arise for different reasons can generally be deciphered by their pattern. For example, peptides that have missing quantitation values for biological reasons tend to be low intensity or completely absent peptides. Hence, these missing values are missing not at random (MNAR) and appear in an intensity-dependent pattern. By contrast, peptides that do not have quantitation due to technical reasons are missing completely at random (MCAR) and appear in an intensity-independent manner.\n\n3.3.1 Influence of experimental design\nAll quantitative proteomics datasets will have some number of missing values, although the extent of data missingness differs between label-free and label-based DDA experiments as well as between DDA and DIA label-free experiments.\nWhen carrying out a label-free DDA experiment, all samples are analysed via independent MS runs. Since the selection of precursor ions for fragmentation is somewhat stochastic in DDA (e.g., top N most abundant precursors are selected) experiments, different peptides may be identified and quantified across the different samples. In other words, not all peptides that are analysed in one sample will be analysed in the next, thus introducing a high proportion of missing values.\nOne of the advantages of using TMT labelling is the ability to multiplex samples into a single MS run. In the use-case, 10 samples were TMT labelled and pooled together prior to DDA MS analysis. As a result, the same peptides were quantified for all samples and we expect a lower proportion of missing values than if we were to use label-free DDA.\nMore recently, DIA MS analysis of label-free samples has increased in popularity. Here, instead of selecting a limited number of precursor peptides (typically the most abundance) for subsequent fragmentation and analysis, all precursors within a selected m/z window are selected. The analysis of all precursor ions within a defined range in every run results in more consistent coverage and accuracy than DDA experiments, hence lower missing values.\n\n\n3.3.2 Exploration of missing data\nThe management of missing data can be considered in three main steps:\n\nExploration of missing data - determine the number and pattern of missing values\nRemoval of data with high levels of missingness - this could be features with missing values across too many samples or samples with an abnormally high proportion of missingness compared to the average\nImputation (optional)\n\n\n\n\n\n\n\nNoteEncoding of missing data\n\n\n\nBefore we begin managing the missing data, we first need to know what missing data looks like. In our data, missing values are notated as NA values. Alternative software may display missing values as being zero, or even infinite if normalisation has been applied during the database search. All functions used for the management of missing data within the QFeatures infrastructure use the NA notation. If we were dealing with a dataset that had an alternative encoding, we could apply the zeroIsNA() or infIsNA() functions to convert missing values into NA values.\n\n\nThe main function that facilitates the exploration of missing data in QFeatures is nNA. Let’s try to use this function. Again, we use the i = argument to specify which SummarizedExperiment within the QFeatures object that we wish to look at.\n\nnNA(cc_qf, i = \"psms_filtered\")\n\n$nNA\nDataFrame with 1 row and 3 columns\n          assay       nNA         pNA\n    &lt;character&gt; &lt;integer&gt;   &lt;numeric&gt;\n1 psms_filte...        14 5.44768e-05\n\n$nNArows\nDataFrame with 25699 rows and 4 columns\n              assay        name       nNA       pNA\n        &lt;character&gt; &lt;character&gt; &lt;integer&gt; &lt;numeric&gt;\n1     psms_filte...           7         0         0\n2     psms_filte...           8         0         0\n3     psms_filte...           9         0         0\n4     psms_filte...          16         0         0\n5     psms_filte...          18         0         0\n...             ...         ...       ...       ...\n25695 psms_filte...       45748         0         0\n25696 psms_filte...       45752         0         0\n25697 psms_filte...       45753         0         0\n25698 psms_filte...       45777         0         0\n25699 psms_filte...       45784         0         0\n\n$nNAcols\nDataFrame with 10 rows and 4 columns\n           assay        name       nNA         pNA\n     &lt;character&gt; &lt;character&gt; &lt;integer&gt;   &lt;numeric&gt;\n1  psms_filte...     Control         4 0.000155648\n2  psms_filte...         M_1         0 0.000000000\n3  psms_filte...         M_2         9 0.000350208\n4  psms_filte...         M_3         1 0.000038912\n5  psms_filte...        G1_1         0 0.000000000\n6  psms_filte...        G1_2         0 0.000000000\n7  psms_filte...        G1_3         0 0.000000000\n8  psms_filte...        DS_1         0 0.000000000\n9  psms_filte...        DS_2         0 0.000000000\n10 psms_filte...        DS_3         0 0.000000000\n\n\nThe output from this function is a list of three DFrames. The first of these (called nNA) gives us information about missing data at the global level (i.e., for the entire SummarizedExperiment). We get information about the absolute number of missing values (nNA) and the proportion of the total data set that is missing values (pNA). The next two data frames also give us nNA and pNA but this time on a per row/feature (nNArows) and per column/sample (nNAcols) basis.\nLet’s direct the output of nNA on each SummarizedExperiment to an object,\n\nmv_raw &lt;- nNA(cc_qf, i = \"psms_raw\")\nmv_filtered &lt;- nNA(cc_qf, i = \"psms_filtered\")\n\nTo access one DataFrame within a list we use the standard $ operator, followed by another $ operator if we wish to access specific columns.\nMissing values down columns (sample) Print information regarding the number of missing values per column in the \"mv_raw\" data,\n\nmv_raw$nNAcols\n\nDataFrame with 10 rows and 4 columns\n         assay        name       nNA        pNA\n   &lt;character&gt; &lt;character&gt; &lt;integer&gt;  &lt;numeric&gt;\n1     psms_raw     Control       280 0.00611314\n2     psms_raw         M_1       161 0.00351505\n3     psms_raw         M_2       388 0.00847106\n4     psms_raw         M_3       177 0.00386438\n5     psms_raw        G1_1       118 0.00257625\n6     psms_raw        G1_2       123 0.00268541\n7     psms_raw        G1_3        88 0.00192127\n8     psms_raw        DS_1       116 0.00253259\n9     psms_raw        DS_2       143 0.00312207\n10    psms_raw        DS_3       110 0.00240159\n\n\nand after filtering,\n\nmv_filtered$nNAcols\n\nDataFrame with 10 rows and 4 columns\n           assay        name       nNA         pNA\n     &lt;character&gt; &lt;character&gt; &lt;integer&gt;   &lt;numeric&gt;\n1  psms_filte...     Control         4 0.000155648\n2  psms_filte...         M_1         0 0.000000000\n3  psms_filte...         M_2         9 0.000350208\n4  psms_filte...         M_3         1 0.000038912\n5  psms_filte...        G1_1         0 0.000000000\n6  psms_filte...        G1_2         0 0.000000000\n7  psms_filte...        G1_3         0 0.000000000\n8  psms_filte...        DS_1         0 0.000000000\n9  psms_filte...        DS_2         0 0.000000000\n10 psms_filte...        DS_3         0 0.000000000\n\n\nNow, we can extract the proportion of NAs in each sample,\n\nmv_raw$nNAcols$pNA\n\n [1] 0.006113137 0.003515054 0.008471061 0.003864376 0.002576250 0.002685414\n [7] 0.001921272 0.002532585 0.003122066 0.002401589\n\n\nWe can also pass the output data from nNA to ggplot to visualise the missing data. This is particularly useful so that we can see whether any of our samples have an abnormally high proportion of missing values compared to the average. This could be the case if something had gone wrong during sample preparation. It is also useful to see whether there is any specific conditions that have a greater number of missing values.\nIn the following code chunk we visualise the proportion of missing values per sample and check for sample and condition bias.\nIn the raw data,\n\nmv_raw$nNAcols %&gt;%\n  as_tibble() %&gt;%\n  mutate(condition = colData(cc_qf)$condition) %&gt;%\n  ggplot(aes(x = name, y = pNA, group = condition, fill = condition)) +\n  geom_bar(stat = \"identity\") +\n  labs(x = \"Sample\", y = \"Proportion missing values\") + \n  theme_bw()\n\n\n\n\n\n\n\n\nIn the filtered data,\n\nmv_filtered$nNAcols %&gt;%\n  as_tibble() %&gt;%\n  mutate(condition = colData(cc_qf)$condition) %&gt;%\n  ggplot(aes(x = name, y = pNA, group = condition, fill = condition)) +\n  geom_bar(stat = \"identity\") +\n  labs(x = \"Sample\", y = \"Proportion missing values\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteMissing values - expectations\n\n\n\nIn this experiment we expect all samples to have a low proportion of missing values due to the TMT labelling strategy. We also expect that all samples should have a similar proportion of missing values because we do not expect cell cycle stage to have a large impact on the majority of the proteome. Hence, the samples here should be very similar. This is the case in most expression proteomics experiments which aim to identify differential protein abundance upon a cellular perturbation. However, in some other MS-based proteomics experiments this would not be the case. For example, proximity labelling and co-immunoprecipitation (Co-IP) experiments have control samples that are not expected to have any proteins in, although there is always a small amount of unwanted noise. In such cases it would be expected to have control samples with a large proportion of missing values and experimental samples with a much lower proportion. It is important to check that the data corresponds to the experimental setup.\n\n\nMissing values across rows (PSMs) Print information regarding the number of missing values per row (PSM):\nIn the \"psms_raw\" data,\n\nmv_raw$nNArows\n\nDataFrame with 45803 rows and 4 columns\n            assay        name       nNA       pNA\n      &lt;character&gt; &lt;character&gt; &lt;integer&gt; &lt;numeric&gt;\n1        psms_raw           1         0       0.0\n2        psms_raw           2         0       0.0\n3        psms_raw           3         0       0.0\n4        psms_raw           4         6       0.6\n5        psms_raw           5         1       0.1\n...           ...         ...       ...       ...\n45799    psms_raw       45799         3       0.3\n45800    psms_raw       45800         0       0.0\n45801    psms_raw       45801         3       0.3\n45802    psms_raw       45802         2       0.2\n45803    psms_raw       45803         1       0.1\n\n\nThe \"psms_filtered\" data,\n\nmv_filtered$nNArows\n\nDataFrame with 25699 rows and 4 columns\n              assay        name       nNA       pNA\n        &lt;character&gt; &lt;character&gt; &lt;integer&gt; &lt;numeric&gt;\n1     psms_filte...           7         0         0\n2     psms_filte...           8         0         0\n3     psms_filte...           9         0         0\n4     psms_filte...          16         0         0\n5     psms_filte...          18         0         0\n...             ...         ...       ...       ...\n25695 psms_filte...       45748         0         0\n25696 psms_filte...       45752         0         0\n25697 psms_filte...       45753         0         0\n25698 psms_filte...       45777         0         0\n25699 psms_filte...       45784         0         0\n\n\n\n\n\n\n\n\nExerciseExercise 3 - Challenge 3: Analysing missing values\n\n\n\n\n\n\nLevel: \nHow many PSMs do we have with (i) 0 missing values, (ii) 1 missing value, (iii) 2 or more missing values, across samples, before and after filtering?\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nThe nNA column gives information on the number missing values (or we can use pNA which gives information on the proportion of missing values). Calling table on the output number of missing values we have across samples,\nOn the raw data,\n\nmv_raw$nNArows$nNA %&gt;% table()\n\n.\n    0     1     2     3     4     5     6     7     8     9    10 \n45263   224    96    58    42    19    17    18    13    11    42 \n\n\nOn the filtered data,\n\nmv_filtered$nNArows$nNA %&gt;% table()\n\n.\n    0     1     3 \n25687    11     1 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.3.3 Removing data with high levels of missingness\nSince the use-case data only has a small number of missing values, it makes sense to simply remove any PSM with a missing value. This is done using the filterNA function where the pNA argument specifies the maximum proportion of missing values to allow per feature.\n\ncc_qf &lt;- cc_qf %&gt;%\n  filterNA(pNA = 0, i = \"psms_filtered\")\n\nCheck that we now have no missing values,\n\nnNA(cc_qf, i = \"psms_filtered\")$nNA\n\nDataFrame with 1 row and 3 columns\n          assay       nNA       pNA\n    &lt;character&gt; &lt;integer&gt; &lt;numeric&gt;\n1 psms_filte...         0         0\n\n\nWe already established that none of our samples have an abnormally high proportion of missing values (relative to the average). Therefore, we do not need to remove any entire samples/columns.\n\n\n3.3.4 Imputation of missing values\nThe final step when managing missing data is to consider whether to impute. Imputation involves the replacement of missing values with probable values. Unfortunately, estimating probable values is difficult and requires complex assumptions about why a value is missing. Given that imputation is difficult to get right and can have substantial effects on the results of downstream analysis, it is generally recommended to avoid it where we can. This means that if there is not a large proportion of missing values in our data, we should not impute. This is one of the reasons that it was easier in the use-case to simply remove the few missing values remaining so that we do not need to impute.\nUnfortunately, not all datasets benefit from having so few missing values. Label-free proteomics experiments in particular have many more missing values than their label-based equivalents. For a more in-depth discussion of imputation and how to complete this within the QFeatures infrastructure, please see the Adapting this workflow to label-free proteomics data section.\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nThe filterFeatures function can be used to remove data from a QFeatures object (or an SummarizedExperiment within a QFeatures object) based on filtering parameters within the rowData.\nData processing includes (i) standard proteomics data cleaning steps e.g., removal of contaminants, and (ii) data-specific quality control filtering e.g., co-isolation interference thresholding for TMT data.\nThe management of missing quantitative data in expression proteomics data is complex. The nNA function can be used to explore missing data and the filterNA function can be used to remove features with undesirably high levels of missing data. Where imputation is absolutely necessary, the impute function exists within the QFeatures infrastructure.\nProtein level thresholding on false discovery rate is required to ensure confident identifications.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Expression proteomics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data cleaning</span>"
    ]
  },
  {
    "objectID": "materials/03_data_processing.html#references",
    "href": "materials/03_data_processing.html#references",
    "title": "3  Data cleaning",
    "section": "References",
    "text": "References\n\n\n\n\nFrankenfield, Ashley M., Jiawei Ni, Mustafa Ahmed, and Ling Hao. 2022. “Protein Contaminants Matter: Building Universal Protein Contaminant Libraries for DDA and DIA Proteomics.” Journal of Proteome Research 21 (9): 2104–13. https://doi.org/10.1021/acs.jproteome.2c00145.\n\n\nHutchings, Charlotte, Charlotte S. Dawson, Thomas Krueger, Kathryn S. Lilley, and Lisa M. Breckels. 2024. “A Bioconductor Workflow for Processing, Evaluating, and Interpreting Expression Proteomics Data.” F1000Research 12 (September): 1402. https://doi.org/10.12688/f1000research.139116.2.\n\n\nMcAlister, Graeme C., David P. Nusinow, Mark P. Jedrychowski, Martin Wühr, Edward L. Huttlin, Brian K. Erickson, Ramin Rad, Wilhelm Haas, and Steven P. Gygi. 2014. “MultiNotch MS3 Enables Accurate, Sensitive, and Multiplexed Detection of Differential Expression Across Cancer Cell Line Proteomes.” Analytical Chemistry 86 (14): 7150–58. https://doi.org/10.1021/ac502040v.\n\n\nPrieto, Gorka, and Jesús Vázquez. 2019. “Calculation of False Discovery Rate for Peptide and Protein Identification.” In Methods in Molecular Biology, 145–59. Springer New York. https://doi.org/10.1007/978-1-4939-9744-2_6.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Expression proteomics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data cleaning</span>"
    ]
  },
  {
    "objectID": "materials/04_normalisation_aggregation.html",
    "href": "materials/04_normalisation_aggregation.html",
    "title": "4  Data normalisation and data aggregation",
    "section": "",
    "text": "4.1 Feature aggregation\nLet’s start by recapping which stage we have reached in the processing of our quantitative proteomics data. In the previous two lessons we have so far learnt,\nIn this next lesson we will continue processing the PSM level data, aggregate our data to protein-level intensities, explore log transformation, and finally normalise the protein-level data ready for downstream statistical testing.\nLet’s recap our data,\ncc_qf\n\nAn instance of class QFeatures containing 2 set(s):\n [1] psms_raw: SummarizedExperiment with 45803 rows and 10 columns \n [2] psms_filtered: SummarizedExperiment with 25687 rows and 10 columns\nNow that we are satisfied with our PSM quality, we need to aggregate our PSM level data upward to the protein level. In a bottom-up MS experiment we initially identify and quantify peptides. Further, each peptide can be identified and quantified on the basis of multiple matched spectra (the peptide spectrum matches, PSMs). We now want to group information from all PSMs that correspond to the same master protein accession.\nTo aggregate upwards from PSM to proteins we can either do this (i) directly (from PSM straight to protein, if we are not interested in peptide level information) or (ii) include an intermediate step of aggregating from PSM to peptides, and then from the peptide level to proteins. Which you do will depend on your biological question. For the purpose of demonstration, let’s perform the explicit step of PSM to peptide aggregation.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Expression proteomics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data normalisation and data aggregation</span>"
    ]
  },
  {
    "objectID": "materials/04_normalisation_aggregation.html#feature-aggregation",
    "href": "materials/04_normalisation_aggregation.html#feature-aggregation",
    "title": "4  Data normalisation and data aggregation",
    "section": "",
    "text": "4.1.1 Step 1: Sum aggregation of PSMs to peptides\nIn your console run the aggregateFeatures function on your QFeatures object. We wish to aggregate from PSM to peptide level so pass the argument i = \"psms_filtered\" to specify we wish to aggregate the PSM data, and then pass fcol = \"Sequence\" to specify we wish to group by the peptide amino acid sequence.\n\ncc_qf &lt;- aggregateFeatures(cc_qf, \n                           i = \"psms_filtered\", \n                           fcol = \"Sequence\",\n                           name = \"peptides\",\n                           fun = base::colSums,\n                           na.rm = TRUE)\n\ncc_qf\n\nAn instance of class QFeatures containing 3 set(s):\n [1] psms_raw: SummarizedExperiment with 45803 rows and 10 columns \n [2] psms_filtered: SummarizedExperiment with 25687 rows and 10 columns \n [3] peptides: SummarizedExperiment with 17231 rows and 10 columns \n\n\nWe see we have created a new assay called peptides and summarised 25687 PSMs into 17231 peptides.\nThere are many ways in which we can combine the quantitative values from each of the contributing PSMs into a single consensus peptide or protein quantitation. Simple methods for doing this include calculating the peptide or master protein quantitation based on the mean, median or sum PSM quantitation. Although the use of these simple mathematical functions can be effective, using colMeans or colMedians can become difficult for data sets that still contain missing values. Similarly, using colSums can result in protein quantitation values being biased by the presence of missing values.\nThat said in this case when missing values are not a problem, colSums can provide a beneficial bias towards the more abundant PSMs or peptides, which are also more likely to be accurate. If missing values are present, an alternative would be to use MsCoreUtils::robustSummary, a state-of-the art aggregation method that is able to aggregate effectively even in the presence of missing values (Sticker et al. 2020). See the extended materials for an example of robust summarisation.\n\n\n4.1.2 Step 2: Sum aggregation of peptides to proteins\nLet’s complete our aggregation by now aggregating our peptide level data to protein level data. Let’s again use the aggregateFeatures function and pass fcol = \"Master.Protein.Accessions\" to specify we wish to group by \"Master.Protein.Accessions\".\n\ncc_qf &lt;- aggregateFeatures(cc_qf, \n                           i = \"peptides\", \n                           fcol = \"Master.Protein.Accessions\",\n                           name = \"proteins\",\n                           fun = base::colSums,\n                           na.rm = TRUE)\n\ncc_qf\n\nAn instance of class QFeatures containing 4 set(s):\n [1] psms_raw: SummarizedExperiment with 45803 rows and 10 columns \n [2] psms_filtered: SummarizedExperiment with 25687 rows and 10 columns \n [3] peptides: SummarizedExperiment with 17231 rows and 10 columns \n [4] proteins: SummarizedExperiment with 3823 rows and 10 columns \n\n\nWe see we have now created a new assay with 3823 protein groups.\n\n\n\n\n\n\nNoteProtein groups\n\n\n\nSince we are aggregating all PSMs that are assigned to the same master protein accession, the downstream statistical analysis will be carried out at the level of protein groups. This is important to consider since most people will report “proteins” as displaying significantly different abundances across conditions, when in reality they are referring to protein groups.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Expression proteomics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data normalisation and data aggregation</span>"
    ]
  },
  {
    "objectID": "materials/04_normalisation_aggregation.html#logarithmic-transformation",
    "href": "materials/04_normalisation_aggregation.html#logarithmic-transformation",
    "title": "4  Data normalisation and data aggregation",
    "section": "4.2 Logarithmic transformation",
    "text": "4.2 Logarithmic transformation\nWe have now reached the point where we are ready to log2 transform the quantitative data. If we take a look at our current (raw) quantitative data we will see that our abundance values are dramatically skewed towards zero.\n\n## Look at distribution of abundance values in untransformed data\ncc_qf[[\"proteins\"]] %&gt;%\n  assay() %&gt;%\n  longFormat() %&gt;%\n  ggplot(aes(x = value)) +\n  geom_histogram() + \n  theme_bw() +\n  xlab(\"Abundance (raw)\")\n\n\n\n\n\n\n\n\nThis is to be expected since the majority of proteins exist at low abundances within the cell and only a few proteins are highly abundant. However, if we leave the quantitative data in a non-Gaussian distribution then we will not be able to apply parametric statistical tests later on. Consider the case where we have a protein with abundance values across three samples A, B and C. If the abundance values were 0.1, 1 and 10, we can tell from just looking at the numbers that the protein is 10-fold more abundant in sample B compared to sample A, and 10-fold more abundant in sample C than sample B. However, even though the fold-changes are equal, the abundance values in A and B are much closer together on a linear scale than those of B and C. A parametric test would not account for this bias and would not consider A and B to be as equally different as B and C. By applying a logarithmic transformation we can convert our skewed asymmetrical data distribution into a symmetrical, Gaussian distribution.\n\n\n\n\n\n\nNoteWhy use base-2?\n\n\n\nAlthough there is no mathematical reason for applying a log2 transformation rather than using a higher base such as log10, the log2 scale provides an easy visualisation tool. Any protein that halves in abundance between conditions will have a 0.5 fold change, which translates into a log2 fold change of -1. Any protein that doubles in abundance will have a fold change of 2 and a log2 fold change of +1.\n\n\n\n\n\n\n\n\nNoteAt which stage of the processing should I perform log transformation?\n\n\n\nLogarithmic transformation can be applied at any stage of the data processing. This decision will depend upon the other data processing steps being completed and the methods used to do so. For example, many imputation methods work on log transformed data. We have chosen to log transform the data in this example after protein aggregation as we will use the colSums method for summarisation and this method requires the data to not be log transformed.\n\n\n\n## Look at distribution of abundance values in untransformed data\ncc_qf[[\"proteins\"]] %&gt;%\n  assay() %&gt;%\n  longFormat() %&gt;%\n  ggplot(aes(x = log2(value))) +\n  geom_histogram() + \n  theme_bw() +\n  xlab(\"(Log2) Abundance\")\n\n\n\n\n\n\n\n\nTo apply this log2 transformation to our data we use the logTransform function and specify base = 2.\n\ncc_qf &lt;- logTransform(object = cc_qf, \n                      base = 2, \n                      i = \"proteins\", \n                      name = \"log_proteins\")\n\nLet’s take a look again at our QFeatures object,\n\ncc_qf\n\nAn instance of class QFeatures containing 5 set(s):\n [1] psms_raw: SummarizedExperiment with 45803 rows and 10 columns \n [2] psms_filtered: SummarizedExperiment with 25687 rows and 10 columns \n [3] peptides: SummarizedExperiment with 17231 rows and 10 columns \n [4] proteins: SummarizedExperiment with 3823 rows and 10 columns \n [5] log_proteins: SummarizedExperiment with 3823 rows and 10 columns",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Expression proteomics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data normalisation and data aggregation</span>"
    ]
  },
  {
    "objectID": "materials/04_normalisation_aggregation.html#normalisation-of-quantitative-data",
    "href": "materials/04_normalisation_aggregation.html#normalisation-of-quantitative-data",
    "title": "4  Data normalisation and data aggregation",
    "section": "4.3 Normalisation of quantitative data",
    "text": "4.3 Normalisation of quantitative data\n\n\n\n\n\n\n\n\n\nWe now have log protein level abundance data to which we could apply a parametric statistical test. However, to perform a statistical test and discover whether any proteins differ in abundance between conditions (here cell cycle stages), we first need to account for non-biological variance that may contribute to any differential abundance. Such variance can arise from experimental error or technical variation, although the latter is much more prominent when dealing with label-free DDA data.\nNormalisation is the process by which we account for non-biological variation in protein abundance between samples and attempt to return our quantitative data back to its ‘normal’ condition i.e., representative of how it was in the original biological system. There are various methods that exist to normalise expression proteomics data and it is necessary to consider which of these to apply on a case-by-case basis. Unfortunately, there is not currently a single normalisation method which performs best for all quantitative proteomics datasets.\nIn QFeatures we can use the normalize function. To see which other normalisation methods are supported within this function, type ?normalize to access the function’s help page.\nOf the supported methods, median-based methods work well for most quantitative proteomics data. Unlike using the mean, median-based methods are less sensitive to the outliers which we often have in proteomics datasets. Let’s apply a center median normalisation approach,\n\ncc_qf &lt;- normalize(cc_qf, \n                   i = \"log_proteins\", \n                   name = \"log_norm_proteins\",\n                   method = \"center.median\")\n\nLet’s verify the normalisation by viewing the QFeatures object. We can call experiments to view all the assays we have created,\n\nexperiments(cc_qf)\n\nExperimentList class object of length 6:\n [1] psms_raw: SummarizedExperiment with 45803 rows and 10 columns\n [2] psms_filtered: SummarizedExperiment with 25687 rows and 10 columns\n [3] peptides: SummarizedExperiment with 17231 rows and 10 columns\n [4] proteins: SummarizedExperiment with 3823 rows and 10 columns\n [5] log_proteins: SummarizedExperiment with 3823 rows and 10 columns\n [6] log_norm_proteins: SummarizedExperiment with 3823 rows and 10 columns\n\n\n\n\n\n\n\n\nExerciseExercise 1 - Challenge 2: Visualising the data prior and post-normalisation\n\n\n\n\n\n\nLevel: \nCreate two boxplots pre- and post-normalisation to visualise the effect it has had on the data and add colour to distinguish between conditions.\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nUsing ggplot2,\n\npre_norm &lt;- cc_qf[[\"log_proteins\"]] %&gt;%\n  assay() %&gt;%\n  longFormat() %&gt;%\n  ggplot(aes(x = colname, y = value)) +\n  geom_boxplot() +\n  labs(x = \"Sample\", y = \"log2(abundance)\", title = \"Pre-normalization\") \n\npost_norm &lt;- cc_qf[[\"log_norm_proteins\"]] %&gt;%\n  assay() %&gt;%\n  longFormat() %&gt;%\n  ggplot(aes(x = colname, y = value)) +\n  geom_boxplot() +\n  labs(x = \"Sample\", y = \"log2(abundance)\", title = \"Post-normalization\") \n\npre_norm  + post_norm \n\n\n\n\n\n\n\n\nColour coding by condition,\n\npre_norm &lt;- cc_qf[[\"log_proteins\"]] %&gt;%\n  assay() %&gt;%\n  longFormat() %&gt;%\n  mutate(Condition = strsplit(as.character(colname), split = \"_\") %&gt;% \n           sapply(\"[[\", 1)) %&gt;%\n  ggplot(aes(x = colname, y = value, fill = Condition))  +\n  geom_boxplot() +\n  labs(x = \"Sample\", y = \"log2(abundance)\", title = \"Pre-normalization\") +\n  theme(legend.position = \"none\")\n\npost_norm &lt;- cc_qf[[\"log_norm_proteins\"]] %&gt;%\n  assay() %&gt;%\n  longFormat() %&gt;%\n  mutate(Condition = strsplit(as.character(colname), split = \"_\") %&gt;% \n           sapply(\"[[\", 1)) %&gt;% \n  ggplot(aes(x = colname, y = value, fill = Condition))  +\n  geom_boxplot() +\n  labs(x = \"Sample\", y = \"log2(abundance)\", title = \"Post-normalization\") \n\npre_norm + post_norm \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo visualise the effect that log transformation followed by normalisation has had on our data, we can also generate density plots. Density plots allow us to visualise the distribution of quantitative values in our data and to see where the majority of intensities lie.\n\n\n\n\n\n\nExerciseExercise 2 - Challenge 3: Visualising the impact of data transformation and normalisation\n\n\n\n\n\n\nLevel: \nCreate three density plots to visualise the distribution of intensities in (1) the raw protein data, (2) the log transformed protein data and (3) the log normalised protein data.\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nUsing plotDensities from the Limma package,\n\npar(mfrow = c(1, 3))  ## Set up panel to hold three figures\n\ncc_qf[[\"proteins\"]] %&gt;%\n  assay() %&gt;%\n  plotDensities(legend = FALSE, \n                main = \"Raw proteins\") \n\ncc_qf[[\"log_proteins\"]] %&gt;%\n  assay() %&gt;%\n  plotDensities(legend = FALSE, \n                main = \"Log2 proteins\") \n\ncc_qf[[\"log_norm_proteins\"]] %&gt;%\n  assay() %&gt;%\n  plotDensities(legend = FALSE, \n                main = \"Log2 norm proteins\") \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom our plots we can see that the center median normalisation has shifted the curves according to their median such that all of the final peaks are overlapping. This is what we would expect given that all of our samples come from the same cells and our treatment/conditions don’t case massive changes in the proteome.\nTo explore the use of alternative normalisation strategies, the NormalyzerDE Willforss, Chawade, and Levander (2018) package can be used to compare normalisation approaches. Please refer to the Using NormalyzerDE to explore normalisation methods section.\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nAggregation from lower level data (e.g., PSM) to high level identification and quantification (e.g., protein) is achieved using the aggregateFeatures function, which also creates explicit links between the original and newly created assays.\nExpression proteomics data should be log2 transformed to generate a Gaussian distribution which is suitable for parametric statistical testing. This is done using the logTransform function.\nTo remove non-biological variation, data normalisation should be completed using the normalize function. To help users decide which normalisation method is appropriate for their data we recommend using the normalyzer function to create a report containing a comparison of methods.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Expression proteomics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data normalisation and data aggregation</span>"
    ]
  },
  {
    "objectID": "materials/04_normalisation_aggregation.html#references",
    "href": "materials/04_normalisation_aggregation.html#references",
    "title": "4  Data normalisation and data aggregation",
    "section": "References",
    "text": "References\n\n\n\n\nSticker, Adriaan, Ludger Goeminne, Lennart Martens, and Lieven Clement. 2020. “Robust Summarization and Inference in Proteome-Wide Label-Free Quantification.” Molecular and Cellular Proteomics 19 (7): 1209–19. https://doi.org/10.1074/mcp.ra119.001624.\n\n\nWillforss, Jakob, Aakash Chawade, and Fredrik Levander. 2018. “NormalyzerDE: Online Tool for Improved Normalization of Omics Expression Data and High-Sensitivity Differential Expression Analysis.” Journal of Proteome Research 18 (2): 732–40. https://doi.org/10.1021/acs.jproteome.8b00523.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Expression proteomics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data normalisation and data aggregation</span>"
    ]
  },
  {
    "objectID": "materials/05_protein_exploration.html",
    "href": "materials/05_protein_exploration.html",
    "title": "5  Exploration and visualisation of protein data",
    "section": "",
    "text": "5.1 Adding assay links\nBefore we carry out statistical analysis to determine which of our proteins show significantly differential abundance across conditions (cell cycle stages), we first want to do some exploration of the protein level data. This includes determining some information that may be required for reporting and publication purposes as well as information corresponding to quality control.\nOne of the main benefits of using QFeatures is that the hierarchical links between quantitative levels are maintained whilst allowing easy access to all data levels for individual features (PSMs, peptides and proteins) of interest. These links are generated and maintained when aggregating using the aggregateFeatures function, as well as the logTransform and normalize functions - all functions that take one SummarizedExperiment within a QFeatures object and create a new SummarizedExperiment in the same QFeatures object. However, before we started filtering the data, we created a copy of the raw data and added this back to our QFeatures object using the addAssay function, which does not maintain the links. Hence, our \"raw_psms\" dataset is not currently linked to any of our higher level data.\nplot(cc_qf)\nIt may be beneficial to add a link between our raw and filtered PSM data. This can be achieved using the addAssayLink function as demonstrated below.\n## Create assay link\ncc_qf &lt;- addAssayLink(object = cc_qf, \n                      from = \"psms_raw\", \n                      to = \"psms_filtered\")\n\n## Verify\nplot(cc_qf)\nAdding a relation between these two sets ensures traceability.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Expression proteomics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploration and visualisation of protein data</span>"
    ]
  },
  {
    "objectID": "materials/05_protein_exploration.html#determining-the-dimensions-of-our-final-protein-data",
    "href": "materials/05_protein_exploration.html#determining-the-dimensions-of-our-final-protein-data",
    "title": "5  Exploration and visualisation of protein data",
    "section": "5.2 Determining the dimensions of our final protein data",
    "text": "5.2 Determining the dimensions of our final protein data\nGiven that we started from the PSM level and did extensive data cleaning, filtering and management of missing data, it would be useful to know how much data we have left. We may want to know how many PSMs, peptides and proteins the log_norm_proteins assay contains, given that this is the data to which statistical analysis will be applied.\nWe can easily find the number master proteins by printing our QFeatures object\n\ncc_qf\n\nAn instance of class QFeatures containing 6 set(s):\n [1] psms_raw: SummarizedExperiment with 45803 rows and 10 columns \n [2] psms_filtered: SummarizedExperiment with 25687 rows and 10 columns \n [3] peptides: SummarizedExperiment with 17231 rows and 10 columns \n [4] proteins: SummarizedExperiment with 3823 rows and 10 columns \n [5] log_proteins: SummarizedExperiment with 3823 rows and 10 columns \n [6] log_norm_proteins: SummarizedExperiment with 3823 rows and 10 columns \n\n\nWe can see we have 3823 master proteins, each representing a protein group.\n\ncc_qf[[\"log_norm_proteins\"]] %&gt;%\n  nrow()\n\n[1] 3823\n\n\n\n\n\n\n\n\nExerciseExercise 1 - Challenge 1: Final PSM, peptide and protein count\n\n\n\n\n\n\nLevel: \nDetermine how many PSMs, peptides and proteins were lost during processing of the raw data to our final protein list?\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nWe started with,\n\npsm_count &lt;- cc_qf[[\"psms_raw\"]] %&gt;% nrow()\n\npeptide_count &lt;- \n  cc_qf[[\"psms_raw\"]] %&gt;%\n  rowData() %&gt;%\n  as_tibble() %&gt;%\n  pull(Sequence) %&gt;%\n  unique() %&gt;%\n  length() \n\nprot_count &lt;- \n  cc_qf[[\"psms_raw\"]] %&gt;%\n  rowData() %&gt;%\n  as_tibble() %&gt;%\n  pull(Master.Protein.Accessions) %&gt;%\n  unique() %&gt;%\n  length() \n\nmessage(psm_count, \" PSMs, \", \n        peptide_count, \" peptides and \", \n        prot_count, \" protein groups\")\n\n45803 PSMs, 26738 peptides and 5267 protein groups\n\n\nAfter filtering we have,\n\npsm_final &lt;- cc_qf[[\"psms_filtered\"]] %&gt;% nrow()\n\npeptide_final &lt;- cc_qf[[\"peptides\"]] %&gt;% nrow()\n\nprot_final &lt;- cc_qf[[\"log_norm_proteins\"]] %&gt;% nrow()\n\nmessage(psm_final, \" PSMs, \", \n        peptide_final, \" peptides and \", \n        prot_final, \" protein groups\")\n\n25687 PSMs, 17231 peptides and 3823 protein groups\n\n\nDuring the course of data processing we have lost,\n\nmessage(psm_count - psm_final, \" PSMs, \", \n        peptide_count - peptide_final, \" peptides and \", \n        prot_count - prot_final, \" protein groups\")\n\n20116 PSMs, 9507 peptides and 1444 protein groups",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Expression proteomics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploration and visualisation of protein data</span>"
    ]
  },
  {
    "objectID": "materials/05_protein_exploration.html#the-.n-column-created-by-aggregatefeatures",
    "href": "materials/05_protein_exploration.html#the-.n-column-created-by-aggregatefeatures",
    "title": "5  Exploration and visualisation of protein data",
    "section": "5.3 The .n column created by aggregateFeatures",
    "text": "5.3 The .n column created by aggregateFeatures\nIf we look at the names of the columns within our “peptides\" and \"proteins\" datasets we see that there is a column called .n. This column was not present in the PSM level datasets.\nFor example,\n\n## Check columns in the log normalised peptide assay\ncc_qf[[\"peptides\"]] %&gt;%\n  rowData() %&gt;%\n  names()\n\n [1] \"Checked\"                     \"Tags\"                       \n [3] \"Confidence\"                  \"Identifying.Node.Type\"      \n [5] \"Identifying.Node\"            \"Search.ID\"                  \n [7] \"Identifying.Node.No\"         \"PSM.Ambiguity\"              \n [9] \"Sequence\"                    \"Number.of.Proteins\"         \n[11] \"Master.Protein.Accessions\"   \"Master.Protein.Descriptions\"\n[13] \"Protein.Accessions\"          \"Protein.Descriptions\"       \n[15] \"Number.of.Missed.Cleavages\"  \"Delta.Cn\"                   \n[17] \"Rank\"                        \"Search.Engine.Rank\"         \n[19] \"Ions.Matched\"                \"Matched.Ions\"               \n[21] \"Total.Ions\"                  \"Quan.Info\"                  \n[23] \"Number.of.Protein.Groups\"    \"Contaminant\"                \n[25] \"Protein.FDR.Confidence\"      \".n\"                         \n\n\nThe .n column is created during the aggregation process that is completed via the aggregateFeatures function. This column stores information about how many child features (PSMs/peptides) were aggregated into each parent (peptides/protein) feature. Since we aggregated completed two steps of aggregation (1) PSMs to peptides, (2) peptides to proteins, the .n column in \"peptides\" tells us how many PSMs we have in support of each peptide, and in \"proteins\" how many peptides we have in support of each master protein.\nLet’s examine peptide support,\n\ncc_qf[[\"log_norm_proteins\"]] %&gt;%\n  rowData() %&gt;%\n  as_tibble() %&gt;%\n  pull(.n) %&gt;%\n  table()\n\n.\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n1228  710  449  313  232  162  149  110   67   54   58   53   33   26   20   25 \n  17   18   19   20   21   22   23   24   25   26   27   28   29   30   31   32 \n  19   12    8   14    9    8    7    5    6    2    1    2    1    3    1    1 \n  33   34   35   36   40   41   44   45   48   49   52   54   59   60   69   86 \n   5    3    2    4    2    2    1    2    1    1    1    3    1    1    1    1 \n  87   90  173  223 \n   1    1    1    1 \n\n\nThe output tells us that we have 1228 proteins with 1 peptides, 710 proteins with support from 2 peptides, and so forth.\n\n\n\n\n\n\nExerciseExercise 2 - Challenge 2: Examining peptide support\n\n\n\n\n\n\nLevel: \n\nUsing the information we have in the .n column create a graph to visualise peptide support.\n\n\n\nInspiration\n\nThe “from Data to Viz project” provides some great ideas for visualisation in R and a brilliant platform for exploring your data. The R Graph Gallery is another great source of inspiration with coded examples to follow.\n\n\nWhat is,\n\n\nthe maximum number of peptides we have available for one given protein?\nthe most common number of peptides available for any given protein?\nthe median number of peptides available for any given protein?\n\n\n\nHint\n\nThe functions table and summary may help.\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nTask 1: Graph to visualise peptide support\nThere are many ways we can visualise peptide support. The first thing we could do is plot a histogram.\n\ncc_qf[[\"log_norm_proteins\"]] %&gt;%\n  rowData() %&gt;%\n  as_tibble() %&gt;%\n  ggplot(aes(x = .n)) +\n  geom_histogram(binwidth = 1)\n\n\n\n\n\n\n\n\nIs this a good visualisation for our dataset? It is perhaps not the easiest plot to read if the aim is to get an overview of how many peptides are available per protein group.\nLet’s bin peptides with &gt; 8 peptides per protein group into one category and then plot the data.\nIn the next code chunk we create a new tibble which tells us how many proteins we have which have n number of peptides.\n\n## Summarise the number of peptides per protein if we have greater than 8  \npeptide_df &lt;- cc_qf[[\"log_norm_proteins\"]] %&gt;%\n  rowData() %&gt;%\n  as_tibble() %&gt;%\n  select(.n) %&gt;% \n  mutate(peptide_n = ifelse(.n &lt;= 7, .n, \"8+\")) %&gt;% \n  count(peptide_n) \n\npeptide_df\n\n# A tibble: 8 × 2\n  peptide_n     n\n  &lt;chr&gt;     &lt;int&gt;\n1 1          1228\n2 2           710\n3 3           449\n4 4           313\n5 5           232\n6 6           162\n7 7           149\n8 8+          580\n\n\nNow let’s plot this data.\n\n## Plot the data using a lollipop\nggplot(peptide_df, aes(x = peptide_n, y = n)) +\n  geom_segment(aes(x = peptide_n, xend = peptide_n, y=0, yend = n)) +\n  geom_point(color = \"red\", size = 4) +\n  ylab(\"Frequency\") +\n  xlab(\"Number of peptides per protein group\") +\n  theme_light() \n\n\n\n\n\n\n\n\nWe can also plot as a percentage.\n\n## Plot the data using a lollipop\npeptide_df %&gt;% \n  mutate(n, n_percent = n/sum(n)*100) %&gt;% \n  ggplot(aes(x = peptide_n, y = n_percent)) +\n  geom_segment(aes(x = peptide_n, xend = peptide_n, y = 0, yend = n_percent)) +\n  geom_point(color=\"red\", size=4) +\n  ylab(\"Frequency (%)\") +\n  xlab(\"Number of peptides per protein group\") +\n  theme_light()\n\n\n\n\n\n\n\n\nTask 2: Peptides support, summary statistics\nLet’s again pull the column .n and tabulate the output\n\ncc_qf[[\"log_norm_proteins\"]] %&gt;%\n  rowData() %&gt;%\n  as_tibble() %&gt;%\n  pull(.n) %&gt;% \n  table()\n\n.\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n1228  710  449  313  232  162  149  110   67   54   58   53   33   26   20   25 \n  17   18   19   20   21   22   23   24   25   26   27   28   29   30   31   32 \n  19   12    8   14    9    8    7    5    6    2    1    2    1    3    1    1 \n  33   34   35   36   40   41   44   45   48   49   52   54   59   60   69   86 \n   5    3    2    4    2    2    1    2    1    1    1    3    1    1    1    1 \n  87   90  173  223 \n   1    1    1    1 \n\n\nLet’s now calculate summary statistics to find the median number of peptides,\n\ncc_qf[[\"log_norm_proteins\"]] %&gt;%\n  rowData() %&gt;%\n  as_tibble() %&gt;%\n  pull(.n) %&gt;% \n  summary()\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   1.000   2.000   4.507   5.000 223.000 \n\n\n\nWe have one instance which has 223 peptides for one given protein.\nFrom the above output we see the most common number of peptides available to support a given protein is 1. Single peptide hits most frequently occur in the data.\nThe median number of peptides is 2.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Expression proteomics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploration and visualisation of protein data</span>"
    ]
  },
  {
    "objectID": "materials/05_protein_exploration.html#the-subsetbyfeature-function",
    "href": "materials/05_protein_exploration.html#the-subsetbyfeature-function",
    "title": "5  Exploration and visualisation of protein data",
    "section": "5.4 The subsetByFeature function",
    "text": "5.4 The subsetByFeature function\nAs well as determining the dimensions of our entire dataset, both in its raw state and its final state, sometimes we may wish to find out information about a specific feature e.g., a protein of interest. The QFeatures infrastructure provides a convenient function called subsetByFeature to extract all data levels corresponding to a particular feature.\nThe subsetByFeature function take a QFeatures object as its input and an additional argument specifying one or more features of interest. The output is a new QFeatures object with only data corresponding to the specified features.\nLet’s take a look at O43583, the human density-regulated protein.\n\nO43583 &lt;- subsetByFeature(cc_qf, \"O43583\")\n\nexperiments(O43583)\n\nExperimentList class object of length 6:\n [1] psms_raw: SummarizedExperiment with 5 rows and 10 columns\n [2] psms_filtered: SummarizedExperiment with 5 rows and 10 columns\n [3] peptides: SummarizedExperiment with 4 rows and 10 columns\n [4] proteins: SummarizedExperiment with 1 rows and 10 columns\n [5] log_proteins: SummarizedExperiment with 1 rows and 10 columns\n [6] log_norm_proteins: SummarizedExperiment with 1 rows and 10 columns\n\n\nFrom this we can see that the O43583 protein is supported by 4 peptides derived from 5 PSMs.\nWe can use our new QFeatures object to create a plot which displays how the PSM data was aggregated to protein for this particular feature. To do so, we extract the assays of interest from our \"O43583\" QFeatures object and pass to the longFormat function which will covert the subset QFeatures object to a long format DataFrame. We can then use the standard ggplot2 functions to visualise the processing of this protein.\n\nO43583[, , c(\"psms_filtered\", \"peptides\", \"proteins\")] %&gt;%\n  longFormat() %&gt;%\n  as_tibble() %&gt;%\n  mutate(assay_order = factor(assay, \n                              levels = c(\"psms_filtered\", \n                                         \"peptides\", \n                                         \"proteins\"))) %&gt;%\n  ggplot(aes(x = colname, y = log2(value), colour = assay)) + \n  geom_point() +\n  geom_line(aes(group = rowname)) +\n  theme(axis.text.x = element_text(angle = 45, size = 7)) +\n  facet_wrap(~ assay_order)\n\n\n\n\n\n\n\n\nOther useful functions that we do not have time to cover today include subsetByAssay, subsetByColData, subsetByColumn, subsetByFilter, subsetByRow, subsetByOverlap, and many more. To find out more about these functions you can execute a single question mark (?) followed by the function name. If you have the QFeatures package installed you should be able to access a help and information page for the function of interest.\nFor example:\n\n?subsetByAssay",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Expression proteomics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploration and visualisation of protein data</span>"
    ]
  },
  {
    "objectID": "materials/05_protein_exploration.html#principal-component-analysis-pca",
    "href": "materials/05_protein_exploration.html#principal-component-analysis-pca",
    "title": "5  Exploration and visualisation of protein data",
    "section": "5.5 Principal Component Analysis (PCA)",
    "text": "5.5 Principal Component Analysis (PCA)\nThe final protein level exploration that we will do is Principal Component Analysis (PCA).\nPCA is a statistical method that can be applied to condense complex data from large data tables into a smaller set of summary indices, termed principal components. This process of dimensionality reduction makes it easier to understand the variation observed in a dataset, both how much variation there is and what the primary factors driving the variation are. This is particularly important for multivariate datasets in which experimental factors can contribute differentially or cumulatively to variation in the observed data. PCA allows us to observe any trends, clusters and outliers within the data thereby helping to uncover the relationships between observations and variables.\n\n5.5.1 The process of PCA\nThe process of PCA can be considered in several parts:\n\nScaling and centering the data\n\nFirstly, all continuous variables are standardized into the same range so that they can contribute equally to the analysis. This is done by centering each variable to have a mean of 0 and scaling its standard deviation to 1.\n\nGeneration of a covariance matrix\n\nAfter the data has been standardized, the next step is to calculate a covariance matrix. The term covariance refers to a measure of how much two variables vary together. For example, the height and weight of a person in a population will be somewhat correlated, thereby resulting in covariance within the population. A covariance matrix is a square matrix of dimensions p x p (where p is the number of dimensions in the original dataset i.e., the number of variables). The matrix contains an entry for every possible pair of variables and describes how the variables are varying with respect to each other.\nOverall, the covariance matrix is essentially a table which summarises the correlation between all possible pairs of variables in the data. If the covariance of a pair is positive, the two variables are correlated in some direction (increase or decrease together). If the covariance is negative, the variables are inversely correlated with one increasing when the other decreases. If the covariance is near-zero, the two variables are not expected to have any relationship.\n\nEigendecomposition - calculating eigenvalues and eigenvectors\n\nEigendecomposition is a concept in linear algebra whereby a data matrix is represented in terms of eigenvalues and eigenvectors. In this case, the the eigenvalues and eigenvectors are calculated based on the covariance matrix and will inform us about the magnitude and direction of our data. Each eigenvector represents a direction in the data with a corresponding eigenvalue telling us how much variation in our data occurs in that direction.\n\nEigenvector = informs about the direction of variation\nEigenvalue = informs about the magnitude of variation\n\nThe number of eigenvectors and eigenvalues will always be the same as the number of dimensions (variables) in the initial dataset. In our use-case, we have 10 samples, so we will have a covariance matrix of dimensions 10 x 10, and this will give rise to 10 eigenvectors and 10 associated eigenvalues.\n\nThe calculation of principal components\n\nPrincipal components are calculated by multiplying the original data by a corresponding eigenvector. As a result, the principal components themselves represent directionality of data. The order of the principal components is determined by the corresponding eigenvector such that the first principal component is that which explains the most variation in the data (i.e., has the largest eigenvalue).\nBy having the first principal components explain the largest proportion of variation in the data, the dimension of the data can be reduced by focusing on these principal components and ignoring those which explain very little in the data.\n\n\n5.5.2 Completing PCA with prcomp\nTo carry out PCA on our data we will use the prcomp function from the stats package. We first extract the quantitative matrix (assay) corresponding to the log normalised protein level data. To make this matrix compatible with prcomp we also need to transpose the data such that the samples become rows and proteins become columns. This is easily achieved using the t function.\nOur protein data does not contain missing values. However, if there were any missing values in the data, these would need to be removed using filterNA to facilitate compatibility with PCA.\n\nprotein_pca &lt;- cc_qf[[\"log_norm_proteins\"]] %&gt;%\n  filterNA() %&gt;%\n  assay() %&gt;%\n  t() %&gt;%\n  prcomp(scale = TRUE, center = TRUE)\n\nsummary(protein_pca)\n\nImportance of components:\n                           PC1     PC2     PC3     PC4      PC5      PC6\nStandard deviation     36.5002 26.5765 22.1495 19.9970 17.26112 14.16447\nProportion of Variance  0.3485  0.1847  0.1283  0.1046  0.07794  0.05248\nCumulative Proportion   0.3485  0.5332  0.6616  0.7662  0.84410  0.89658\n                           PC7      PC8      PC9      PC10\nStandard deviation     11.9731 11.30108 11.14872 1.091e-13\nProportion of Variance  0.0375  0.03341  0.03251 0.000e+00\nCumulative Proportion   0.9341  0.96749  1.00000 1.000e+00\n\n\nWe now have a simplified representation of our quantitative data in the form of principle components (PC). The prcomp function outputs a list of 5 different information sources, each of which can be accessed using the $ sign nomenclature.\n\nsdev - holds the standard deviation values for each of the principle components\nrotation - a matrix which contains each of our proteins as a row and the corresponding PC values as columns\nx - a matrix which contains each of our samples as a row and the corresponding PC values as columns\ncenter - if center = TRUE then contains the centering values, otherwise FALSE\nscale - if scale = TRUE then contains the scaling values, otherwise FALSE\n\nTo visualise the resulting PCs and how much of the data variation they explain we can plot a scree plot using the fviz_screeplot function. The resulting plot displays the proportion of total data variation explained by each of PC.\n\nfviz_screeplot(protein_pca)\n\n\n\n\n\n\n\n\nLooking at a scree plot can be useful when deciding which principle components to plot and investigate further. We now want to plot each of our samples in PCA space. To do this we will use the protein_pca$x data. Typically a 2D PCA plot will display PC1 and PC2, since these are the PCs that explain the most variation within the dataset, but it can also be useful to plot later PCs if they also explain a large proportion of variation.\n\nprotein_pca$x %&gt;%\n  as_tibble() %&gt;%\n  ggplot(aes(x = PC1, y = PC2)) +\n  geom_point(size = 3) + \n  theme_bw()\n\n\n\n\n\n\n\n\nIt is generally advisable to colour each point based on all possible explanatory variables that may have contributed to the observed variation. In our case we only have one - the cell cycle stage.\n\n\n\n\n\n\nExerciseExercise 3 - Challenge 3: PCA plot\n\n\n\n\n\n\nLevel: \n\nGenerate a PCA plot of the data and colour by condition.\n\n\n\nHint\n\nTo colour the points based on this condition we can use the tidyverse mutate function to add a column defining the condition of each sample and then use colour = condition within our ggplot aesthetics.\n\n\nWhat does this plot tell us?\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nTask 1\n\nprotein_pca$x %&gt;%\n  as_tibble() %&gt;%\n  mutate(condition = cc_qf[[\"log_norm_proteins\"]]$condition) %&gt;%\n  ggplot(aes(x = PC1, y = PC2, colour = condition)) +\n  geom_point(size = 3) + \n  theme_bw()\n\n\n\n\n\n\n\n\n\nTask 2\nThis PCA plot shows clear clustering of samples based on their condition, which is what we would hope to see. This indicates that the observed variation could indeed be explained by cell cycle stage.\n\n\n\n\n\n\n\n\nFor more complicated multivariate experiments all possible explanatory variables should be visualised. For example, if multiple batches of samples have been prepared separately or several TMTplexes were used, these factors should be visualised (e.g., by colour) on the PCA plot to see whether they are contributing the the observed variation. If the samples do cluster based on unwanted factors such as batch or TMTplex, additional normalisation may be required.\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nThe .n column created by aggregateFeatures is a useful way to trace how many child features have been aggregated into a single parent feature\nThe subsetByFeature function can be used to generate a QFeatures object with all levels of data corresponding to one or more features of interest\nPrincipal Component Analysis (PCA) is a dimensionality reduction method that can be used to visualise the relationship between explanatory variables and observed data. If samples cluster together based on a particular factor, this indicates that the factor",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Expression proteomics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploration and visualisation of protein data</span>"
    ]
  },
  {
    "objectID": "materials/05_protein_exploration.html#references",
    "href": "materials/05_protein_exploration.html#references",
    "title": "5  Exploration and visualisation of protein data",
    "section": "References",
    "text": "References",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Expression proteomics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploration and visualisation of protein data</span>"
    ]
  },
  {
    "objectID": "materials/06_statistical_analysis.html",
    "href": "materials/06_statistical_analysis.html",
    "title": "6  Statistical analysis",
    "section": "",
    "text": "6.1 Differential expression analysis\nHaving cleaned our data, aggregated from PSM to protein level, completed a log2 transformation and normalised the data, we are now ready to carry out statistical analysis.\nTo simply the statistical analysis, we will focus on just two conditions here: M and G1 cell cycle stages. For a more complete statistical analysis of all cell cycles stages, please see the Statistical analysis of all 3 cell cycle stages section.\nThe aim of this section is to answer the question: “Which proteins show a significant change in abundance between M and G1?”.\nOur null hypothesis is: (H0) The change in abundance for a protein between cell cycle stages is 0.\nOur alternative hypothesis is: (H1) The change in abundance for a protein between cell cycle stages is not 0.\nWe want to perform a statistical test to determine if there is sufficient evidence to reject the null hypothesis. This is carried out on each protein separately, though as we will see, all proteins are tested simultaneously and there is some sharing of information between the proteins.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Expression proteomics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical analysis</span>"
    ]
  },
  {
    "objectID": "materials/06_statistical_analysis.html#selecting-a-statistical-test",
    "href": "materials/06_statistical_analysis.html#selecting-a-statistical-test",
    "title": "6  Statistical analysis",
    "section": "6.2 Selecting a statistical test",
    "text": "6.2 Selecting a statistical test\nThere are a few aspects of our data that we need to consider prior to deciding which statistical test to apply.\n\nThe protein abundances in this data are not normally distributed (i.e., they do not follow a Gaussian distribution). However, they are approximately normal following a log-transformation.\nThe cell cycle is not expected to have a large impact on biological variability. We can assume that the variance is approximately equal across the groups.\nThe samples are independent not paired. For example, M_1 is not derived from the same cells as G1_1 and DS_1.\n\nThe first point relates to a key assumption that is made when carrying out Gaussian Linear modeling, which assumes that the residuals (difference between the observed values and the values predicted by the model) are Gaussian distributed. If this assumption is not met, then it is not appropriate to use a Gaussian Linear model. For quantitative proteomics data, it’s reasonable to assume the residuals will be approximately Gaussian distributed if we first log-transform the abundances.\nMany different R packages can be used to carry out differential expression (abundance) analysis on proteomics data. Here we will use limma, a package that is widely used for omics analysis and can be used in single comparisons or multifactorial experiments using an empirical Bayes-moderated linear model. A simple example of the empirical Bayes-moderated linear model is provided in Hutchings et al. (2024).\nHere, we will perform a comparison between two groups (M and G1 phases) for each protein. For a multifactorial comparison of all cell cycle stages, see Statistical analysis of all 3 cell cycle stages.\n\n6.2.1 What does the empirical Bayes part mean?\nWhen carrying out high throughput omics experiments we not only have a population of samples but also a population of features - here we have several thousand proteins. Proteomics experiments are typically lowly replicated (e.g n &lt; 10), therefore, the per-protein variance estimates are relatively inaccurate. The empirical Bayes method borrows information across features (proteins) and shifts the per-protein variance estimates towards an expected value based on the variance estimates of other proteins with a similar abundance. This improves the accuracy of the variance estimates, thus reducing false negatives for proteins with over-estimated variance and reducing false positives from proteins with under-estimated variance. For more detail about the empirical Bayes methods, see here.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Expression proteomics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical analysis</span>"
    ]
  },
  {
    "objectID": "materials/06_statistical_analysis.html#extracting-the-required-data",
    "href": "materials/06_statistical_analysis.html#extracting-the-required-data",
    "title": "6  Statistical analysis",
    "section": "6.3 Extracting the required data",
    "text": "6.3 Extracting the required data\nWe subset to our log2 transformed protein-level data and retain just the M and G1 phase samples\n\n# extract the log-normalised experiment from our QFeatures object\nall_proteins &lt;- cc_qf[[\"log_norm_proteins\"]]\n\n# subset to retain only M and G1 samples\nall_proteins &lt;- all_proteins[, all_proteins$condition %in% c(\"M\", \"G1\")]\n                             \n## Ensure that conditions are stored as levels of a factor with\n## explicitly defined levels\nall_proteins$condition &lt;- factor(all_proteins$condition, \n                                 levels = c(\"M\", \"G1\"))",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Expression proteomics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical analysis</span>"
    ]
  },
  {
    "objectID": "materials/06_statistical_analysis.html#defining-the-statistical-model",
    "href": "materials/06_statistical_analysis.html#defining-the-statistical-model",
    "title": "6  Statistical analysis",
    "section": "6.4 Defining the statistical model",
    "text": "6.4 Defining the statistical model\nBefore we apply our empirical Bayes moderated linear model, we first need to set up a model. To define the model design we use model.matrix. A model matrix, also called a design matrix, is a matrix in which rows represent individual samples and columns correspond to explanatory variables, in our case the cell cycle stages. Simply put, the model design is determined by how samples are distributed across conditions.\nBelow, we define the model matrix with condition as the explanatory variable.\n\n## Design a matrix containing all factors that we wish to model\ncondition &lt;- all_proteins$condition\n\nm_design &lt;- model.matrix(~condition) # Model with intercept\n\nInspecting the design matrix, we can see that we have a coefficient called (Intercept) and a coefficient called conditionG1. The first level of the variable (here, M) is considered the ‘baseline’ and modeled by the intercept. The second level of the variable (here, G1) is then modeled by an additional term in the model, which captures the difference between M and G1. This is the most appropriate way to model the data since the term conditionG1 captures the difference we are interested in.\n\n## Inspect the design matrix\nm_design\n\n  (Intercept) conditionG1\n1           1           0\n2           1           0\n3           1           0\n4           1           1\n5           1           1\n6           1           1\nattr(,\"assign\")\n[1] 0 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$condition\n[1] \"contr.treatment\"\n\n\n\n\n\n\n\n\nNoteWhat happens if we don’t include an intercept?\n\n\n\nWhen investigating the effect of a single explanatory variable, the design matrix should be created using model.matrix(~variable), such that an intercept term is included and the other model term captures the difference we are interested in.\nIf we specified a model without an intercept (model.matrix(~0 + variable)), the resultant coefficients in the model will capture the difference between each group (M or G1) and zero. Given our null hypothesis relates to the difference between the groups, not between each group and zero, this is not want we want! We could still use contrasts to explore the differences between the groups an intercept term in our model, but it’s simpler to work with a model which inherently estimates this difference.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Expression proteomics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical analysis</span>"
    ]
  },
  {
    "objectID": "materials/06_statistical_analysis.html#running-an-empirical-bayes-moderated-test-using-limma",
    "href": "materials/06_statistical_analysis.html#running-an-empirical-bayes-moderated-test-using-limma",
    "title": "6  Statistical analysis",
    "section": "6.5 Running an empirical Bayes-moderated test using limma",
    "text": "6.5 Running an empirical Bayes-moderated test using limma\nAfter we have specified the design matrix, the next step is to apply the statistical model\n\n## Fit linear model using the design matrix and desired contrasts\nfit_model &lt;- lmFit(object = assay(all_proteins), design = m_design)\n\nThe initial model has now been applied to each of the proteins in our data. We now update the model using the eBayes function. When we do this we include two other arguments: trend = TRUE and robust = TRUE.\n\ntrend - takes a logical value of TRUE or FALSE to indicate whether an intensity-dependent trend should be allowed for the prior variance (i.e., the population level variance prior to empirical Bayes moderation). This means that when the empirical Bayes moderation is applied the protein variances are not squeezed towards a global mean but rather towards an intensity-dependent trend.\nrobust - takes a logical value of TRUE or FALSE to indicate whether the parameter estimation of the priors should be robust against outlier sample variances.\n\nSee (Phipson et al. (2016) and Smyth (2004)) for further details.\n\n## Update the model using the limma eBayes algorithm\n\nfinal_model &lt;- eBayes(fit = fit_model, \n                      trend = TRUE,\n                      robust = TRUE)\n\n\n6.5.1 Accessing the model results\nThe topTable function extracts a table of the top-ranked proteins from our fitted linear model. By default, topTable outputs a table of the top 10 ranked proteins, that is the 10 proteins with the highest log-odds of being differentially abundant. To get the results for all of our proteins we use the number = Inf argument.\n\n## Format results\nlimma_results &lt;- topTable(fit = final_model,\n                          coef = 'conditionG1',\n                          adjust.method = \"BH\",    # Method for multiple hypothesis testing\n                          number = Inf) %&gt;%        # Print results for all proteins\n  rownames_to_column(\"Protein\") \n\n## Verify\nhead(limma_results)\n\n  Protein     logFC   AveExpr         t      P.Value    adj.P.Val        B\n1  Q9NQW6 -2.077921  2.641270 -40.40812 1.139582e-15 4.356623e-12 25.79468\n2  Q9BW19 -2.343592  1.430210 -36.79774 4.073530e-15 7.786553e-12 24.71272\n3  P49454 -1.991326  2.598481 -34.88623 8.411188e-15 9.266757e-12 24.07905\n4  Q9ULW0 -2.152503  2.476087 -34.52309 9.695796e-15 9.266757e-12 23.95341\n5  Q562F6 -3.205603 -1.304111 -32.91865 1.849860e-14 1.414403e-11 23.37677\n6  O14965 -2.098029  0.797902 -31.22180 3.791132e-14 2.415583e-11 22.72604\n\n\nDepending on whether the linear model is used to perform single comparisons or multifactorial comparisons, the test statistic for each protein will either be a t-value or an F-value, respectively. Here, we are performing a single comparison (M vs G1), so we obtain a t-value. We also obtain the p-value from the comparison of the t-value with a t-distribution.\n\n\n\n\n\n\nNoteWhat is a t-value?\n\n\n\nA t-value is a parametric statistical value used to compare the mean values of two groups. The t-value is the ratio of the difference in means to the standard error of the difference in means. The further away from zero that a t-value lies, the more significant the difference between the groups is.\n\n\n\n\n\n\n\n\nNoteHow is the p-value obtained?\n\n\n\nA p-value may be obtained from a t-value by comparing the value against a t-distribution with the appropriate degrees of freedom.\n\ndegrees of freedom = the number of observations minus the number of independent variables in the model\np-value = the probability of achieving the t-value under the null hypothesis i.e., by chance\n\n\n\nWe also see an adjusted p-value (adj.P.Val) column. This provides p-values adjusted to account for the multiple hypothesis tests performed.\n\n\n6.5.2 Multiple hypothesis testing and correction\nUsing the linear model defined above, we have carried out a statistical test for each protein.\nMultiple testing describes the process of separately testing multiple null hypothesis i.e., carrying out many statistical tests at a time, each to test a null hypothesis on different data. Here we have carried out 3823 hypothesis tests. If we were to use the typical p &lt; 0.05 significance threshold for each test, we would expect a 5% chance of incorrectly rejecting the null hypothesis per test. Here, we would expect approximately 191 p-values &lt;= 0.05 by chance.\nIf we do not account for the fact that we have carried out multiple hypothesis, we risk including false positives in our data. Many methods exist to correct for multiple hypothesis testing and these mainly fall into two categories:\n\nControl of the Family-Wise Error Rate (FWER)\nControl of the False Discovery Rate (FDR)\n\nAbove we specified the “BH” method for adjusting p-values in our topTable function call. This is shorthand for the Benjamini-Hochberg procedure, to control the FDR.\n\n\n\n\n\n\nTipThe False Discovery Rate\n\n\n\nThe False Discovery Rate (FDR) defines the fraction of false discoveries that we are willing to tolerate in our list of differential proteins. For example, an FDR threshold of 0.05 means that approximately 5% of the proteins deemed differentially abundant will be false positives. It is up to you to decide what this threshold should be, but conventionally a value between 0.01 (1% FPs) and 0.1 (10% FPs) is chosen.\n\n\n\n\n6.5.3 Diagnostic plots to verify suitability of our statistical model\nAs with all statistical analysis, it is crucial to do some quality control and to check that the statistical test that has been applied was indeed appropriate for the data. As mentioned above, statistical tests typically come with several assumptions. To check that these assumptions were met and that our model was suitable, we create some diagnostic plots.\nThe first plot that we generate is an SA plot to display the residual standard deviation (sigma) versus log abundance for each protein to which our model was fitted. We can use the plotSA function to do this.\n\nplotSA(fit = final_model,\n       cex = 0.5,\n       xlab = \"Average log2 abundance\")\n\n\n\n\n\n\n\n\nIt is recommended that an SA plot be used as a routine diagnostic plot when applying a limma-trend pipeline. From the SA plot we can visualise the intensity-dependent trend that has been incorporated into our linear model. It is important to verify that the trend line fits the data well. If we had not included the trend = TRUE argument in our eBayes function, then we would instead see a straight horizontal line that does not follow the trend of the data. Further, the plot also colours any outliers in red. These are the outliers that are only detected and excluded when using the robust = TRUE argument.\nNext, we plot a histogram of the raw p-values (not adjusted p-values). This can be done by passing our results data into standard ggplot2 plotting functions.\n\nlimma_results %&gt;%\n  as_tibble() %&gt;%\n  ggplot(aes(x = P.Value)) + \n  geom_histogram()\n\n\n\n\n\n\n\n\nThe histogram we have plotted shows an anti-conservative distribution, which is good. The near-flat distribution across the bottom corresponds to null p-values which are distributed approximately uniformly between 0 and 1. The peak close to 0 contains a combination of our significantly changing proteins (true positives) and proteins with a low p-value by chance (false positives).\nOther examples of how a p-value histogram could look are shown below. Whilst in some experiments a uniform p-value distribution may arise due to an absence of significant alternative hypotheses, other distribution shapes can indicate that something was wrong with the model design or statistical test. For more detail on how to interpret p-value histograms there is a great blog post by David Robinson, from which the examples below are taken.\n\n\n\n\n\n\n\n\n\nExamples of p-value histograms.\n\n\n\n\n\n\n6.5.4 Interpreting the output of our statistical model\nHaving checked that the model we fitted was appropriate for the data, we can now take a look at the results of our test\n\nhead(limma_results)\n\n  Protein     logFC   AveExpr         t      P.Value    adj.P.Val        B\n1  Q9NQW6 -2.077921  2.641270 -40.40812 1.139582e-15 4.356623e-12 25.79468\n2  Q9BW19 -2.343592  1.430210 -36.79774 4.073530e-15 7.786553e-12 24.71272\n3  P49454 -1.991326  2.598481 -34.88623 8.411188e-15 9.266757e-12 24.07905\n4  Q9ULW0 -2.152503  2.476087 -34.52309 9.695796e-15 9.266757e-12 23.95341\n5  Q562F6 -3.205603 -1.304111 -32.91865 1.849860e-14 1.414403e-11 23.37677\n6  O14965 -2.098029  0.797902 -31.22180 3.791132e-14 2.415583e-11 22.72604\n\n\nInterpreting the output of topTable:\n\nlogFC = The observed log2FC for G1 vs M cell cycle stages\nAveExpr = the average log abundance of the protein across samples\nt = eBayes moderated t-value. Interpreted in the same way as a normal t-value (see above).\nP.Value = Unadjusted p-value\nadj.P.Val = FDR-adjusted p-value (note that this adjustment is only for multiple proteins, not multiple contrasts i.e., separate rather than global correction)\n\nWe have used the statistical test to ask “Does this protein show a significant change in abundance between M and G1 cell cycle stages?” for each protein.\nOur null hypothesis is: (H0) The change in abundance for a protein between cell cycle stages is 0.\nOur alternative hypothesis is: (H1) The change in abundance for a protein between cell cycle stages is greater than 0.\nFrom our output we can see that some of our proteins have high t-values and low adjusted p-values (below any likely threshold of significance). These adjusted p-values tell us that these protein have a significantly different abundance across M and G1 cell cycle stages.\n\nAdding user-defined significance thresholds\nThe output of our statistical test will provide us with key information for each protein, including its p-value, BH-adjusted p-value and logFC. However, it is up to us to decide what we consider to be significant. The first parameter to consider is the adj.P.Val threshold that we wish to apply. The second parameter which is sometimes used to define significance is the logFC. This is mainly because larger fold changes are deemed more likely to be ‘biologically relevant’.\nHere we are going to define significance based on an adj.P.Val &lt; 0.01. We can add a column to our results to indicate significance as well as the direction of change.\n\n## Add direction and significance information\nlimma_results &lt;- limma_results %&gt;%\n  mutate(direction = ifelse(logFC &gt; 0, \"up\", \"down\"),\n         significance = ifelse(adj.P.Val &lt; 0.01, \"sig\", \"not.sig\"))\n\n\n## Verify\nhead(limma_results)\n\n  Protein     logFC   AveExpr         t      P.Value    adj.P.Val        B\n1  Q9NQW6 -2.077921  2.641270 -40.40812 1.139582e-15 4.356623e-12 25.79468\n2  Q9BW19 -2.343592  1.430210 -36.79774 4.073530e-15 7.786553e-12 24.71272\n3  P49454 -1.991326  2.598481 -34.88623 8.411188e-15 9.266757e-12 24.07905\n4  Q9ULW0 -2.152503  2.476087 -34.52309 9.695796e-15 9.266757e-12 23.95341\n5  Q562F6 -3.205603 -1.304111 -32.91865 1.849860e-14 1.414403e-11 23.37677\n6  O14965 -2.098029  0.797902 -31.22180 3.791132e-14 2.415583e-11 22.72604\n  direction significance\n1      down          sig\n2      down          sig\n3      down          sig\n4      down          sig\n5      down          sig\n6      down          sig",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Expression proteomics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical analysis</span>"
    ]
  },
  {
    "objectID": "materials/06_statistical_analysis.html#visualising-the-results-of-our-statistical-model",
    "href": "materials/06_statistical_analysis.html#visualising-the-results-of-our-statistical-model",
    "title": "6  Statistical analysis",
    "section": "6.6 Visualising the results of our statistical model",
    "text": "6.6 Visualising the results of our statistical model\n\n\n\n\n\n\n\n\n\nThe final step in any statistical analysis is to visualise the results. This is important for ourselves as it allows us to check that the data looks as expected.\nThe most common visualisation used to display the results of expression proteomics experiments is a volcano plot. This is a scatterplot that shows statistical significance (p-values) against the magnitude of fold change. Of note, when we plot the statistical significance we use the raw unadjusted p-value (-log10(P.Value)). This is because it is better to plot the statistical test results in their ‘raw’ form and not values derived from them (the adjusted p-value is derived from each p-value using the BH-method of correction). Furthermore, the process of FDR correction can result in some points that previously had distinct p-values having the same adjusted p-value. Finally, different methods of correction will generate different adjusted p-values, making the comparison and interpretation of values more difficult.\n\nlimma_results %&gt;%\n  ggplot(aes(x = logFC, y = -log10(P.Value), fill = significance)) +\n  geom_point(shape = 21, stroke = 0.25, size = 3) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExerciseExercise 1 - Challenge: Volcano plots\n\n\n\n\n\n\nLevel: \nRe-generate your volcano plot defining significance based on an adjusted P-value &lt; 0.01 and a log2 fold-change of &gt; 1.\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\n\nmy_results &lt;- \n  limma_results %&gt;%\n  mutate(direction = ifelse(logFC &gt; 0, \"up\", \"down\"), \n         significance = ifelse(adj.P.Val &lt; 0.01 & abs(logFC) &gt; 1, \"sig\", \"not.sig\"))\n\nmy_results %&gt;%\n  ggplot(aes(x = logFC, y = -log10(P.Value), fill = significance)) +\n  geom_point(shape = 21, stroke = 0.25, size = 3) +\n  theme_bw()",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Expression proteomics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical analysis</span>"
    ]
  },
  {
    "objectID": "materials/06_statistical_analysis.html#a-more-statistically-valid-way-to-include-a-fold-change-threshold",
    "href": "materials/06_statistical_analysis.html#a-more-statistically-valid-way-to-include-a-fold-change-threshold",
    "title": "6  Statistical analysis",
    "section": "6.7 A more statistically valid way to include a fold-change threshold",
    "text": "6.7 A more statistically valid way to include a fold-change threshold\nAlthough it is commonplace to see a threshold being applied to the point estimate for the log fold-change (logFC) to determine the ‘biologically significant’ changes, there is a drawback. The point estimate does not take into account the confidence interval for the logFC. As such, proteins with poorly estimated fold-changes are more likely to pass the logFC threshold by chance, while proteins with very well estimated fold-changes which fall just below the threshold would not be deemed biologically significant.\nThankfully, limma has in-built functions which allows us to specify a different null hypothesis and more appropriately test whether a protein has a fold-change greater than a given value. This test whether the fold-change is greater than a specific value is more stringent than a post-hoc threshold on the point estimate and it thus makes sense to use a slightly lower threshold. Here we will use a threshold of absolute logFC &gt; 0.5 (&gt;1.4 fold-change).\n\nfinal_model_treat &lt;- treat(final_model,\n                           lfc = 0.5, # null hypothesis is 'absolute logFC &lt; 0.5'\n                           trend = TRUE, \n                           robust = TRUE)\n\n# We now need to use TopTreat in place of TopTable\nlimma_results_treat &lt;- topTreat(final_model_treat,\n                                coef = \"conditionG1\",\n                                n = Inf) %&gt;%\n  rownames_to_column(\"Protein\")\n\nAgain, we add columns specifying the direction of change and significance (using the adjusted p-value alone).\n\n## Add direction and significance information\nlimma_results_treat &lt;- limma_results_treat %&gt;%\n  mutate(direction = ifelse(logFC &gt; 0, \"up\", \"down\"),\n         significance = ifelse(adj.P.Val &lt; 0.01, \"sig\", \"not.sig\"))\n\nFinally, we visualise the volcano plot.\n\nlimma_results_treat %&gt;%\n  ggplot(aes(x = logFC, y = -log10(P.Value), fill = significance)) +\n  geom_point(shape = 21, stroke = 0.25, size = 3) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExerciseExercise 2 - Challenge: Compare logFC thresholding post-hoc with LogFC null hypothesis\n\n\n\n\n\n\nLevel: \n\nCompare the overall results for each logFC thresholding approach by creating a 2 x 2 table with the number of proteins with increased/decreased abundance and significant/not significant change, for each approach.\nIdentify the proteins which are significant when using the TREAT functions to define a logFC threshold for the null hypothesis but not when thresholding on the logFC post-hoc. You can use the existing my_results and limma_results_treat objects for this.\nRe-make the volcano plots for the two logFC thresholding approaches, but this time with the proteins identified above highlighted by the point shape. &gt; 1.\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\n\n# Tabulate direction of change and significance\n# for both logFC threshold approaches\ntable(my_results$direction,\n      my_results$significance)\n\n      \n       not.sig  sig\n  down    1792   35\n  up      1961   35\n\ntable(limma_results_treat$direction,\n      limma_results_treat$significance)\n\n      \n       not.sig  sig\n  down    1791   36\n  up      1974   22\n\n\n\n# Identify the proteins which are significant using \n# TREAT, but not with the post hoc threshold on fold-change\npost_hoc_not_sig &lt;- my_results %&gt;%\n  filter(significance == 'not.sig') %&gt;%\n  pull(Protein)\n\ntreat_sig &lt;- limma_results_treat %&gt;% \n  filter(significance == 'sig') %&gt;%\n  pull(Protein)\n\nsig_treat_only &lt;- intersect(post_hoc_not_sig, treat_sig)\n\n\n# Make a volcano plot highlighting these proteins\nmy_results %&gt;%\n  # Add a new column to annotate the proteins to highlight\n  mutate(highlight = Protein %in% sig_treat_only) %&gt;% \n  # Use the highlight column to control the shape aesthetic\n  ggplot(aes(x = logFC, y = -log10(P.Value),\n             fill = significance,\n             shape = highlight)) + \n  geom_point(stroke = 0.25, size = 3) +\n  # Define the shapes\n  scale_shape_manual(values = c(21, 8),\n                     name = 'Post-hoc logFC\\nthresh. sig. only') + \n  guides(fill = guide_legend(override.aes = list(shape=21))) +\n  theme_bw()\n\n\n\n\n\n\n\nlimma_results_treat %&gt;%\n  mutate(highlight = Protein %in% sig_treat_only) %&gt;% \n  ggplot(aes(x = logFC, y = -log10(P.Value),\n             fill = significance,\n             shape = highlight)) + \n  geom_point(stroke = 0.25, size = 3) +\n  scale_shape_manual(values = c(21, 8),\n                     name = 'Post-hoc logFC\\nthresh. sig. only') + \n  guides(fill = guide_legend(override.aes = list(shape=21))) +\n  theme_bw()",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Expression proteomics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical analysis</span>"
    ]
  },
  {
    "objectID": "materials/06_statistical_analysis.html#visualising-the-protein-abundances-in-a-heatmap",
    "href": "materials/06_statistical_analysis.html#visualising-the-protein-abundances-in-a-heatmap",
    "title": "6  Statistical analysis",
    "section": "6.8 Visualising the protein abundances in a heatmap",
    "text": "6.8 Visualising the protein abundances in a heatmap\nAnother widely used visualisation tool is a heatmap. A heatmap is a two-dimensional representation of our quantitative data where the magnitude of values are depicted by colour. These visualisations are commonly combined with clustering tools to facilitate the identification of groups of features, here proteins, that display similar quantitative behaviour. Here, we will use the pheatmap function from the pheatmap package to plot a heatmap of proteins that display a significant difference in abundance between M-phase and G1-phase cells. Note that we will plot all samples though, not just M and G1 phase samples.\nWe first extract the accessions of proteins with significant differences. We use these accessions to subset the original quantification data which is currently stored in the assay of our cp_qf object.\n\n## Extract accessions of significant proteins\nsig_proteins &lt;- limma_results %&gt;%\n  filter(significance == \"sig\") %&gt;%\n  pull(Protein)\n\n## Subset quantitative data corresponding to significant proteins\nquant_data &lt;- cc_qf[[\"log_norm_proteins\"]]\n\nquant_data &lt;- quant_data[sig_proteins, ] %&gt;% assay() \n\nNow we use the quantitative data to plot a heatmap using pheatmap. We will normalise each row of protein abundances to Z-scores (standard deviations away from the mean).\n\npheatmap(mat = quant_data,\n         scale = 'row', # Z-score normalise across the rows (proteins)\n         show_rownames = FALSE)  # Too many proteins to show all their names!\n\n\n\n\n\n\n\n\nA more in-depth overview of pheatmap and how to customise these plots further can be found in the documentation (?pheatmap) and here.\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nThe limma package provides a statistical pipeline for the analysis of differential expression (abundance) experiments\nEmpirical Bayes moderation involves borrowing information across proteins to squeeze the per-protein variance estimates towards an expected value based on the behavior of other proteins with similar abundances. This method increases the statistical power and reduces the number of false positives.\nSince proteomics data typically shows an intensity-dependent trend, it is recommended to apply empirical Bayes moderation with trend = TRUE and robust = TRUE. The validity of this approach can be assessed by plotting an SA plot.\nSignificance thresholds are somewhat arbitrary and must be selected by the user. However, correction must be carried out for multiple hypothesis testing so significance thresholds should be based on adjusted p-values rather than raw p-values.\nThe statistically appropriate way to threshold based on a log fold-change is to use the TREAT functions in limma and define a null hypothesis that the change is below a given value.\nThe results of differential expression and abundance analyses are often summarised with volcano plots and heatmaps.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Expression proteomics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical analysis</span>"
    ]
  },
  {
    "objectID": "materials/06_statistical_analysis.html#references",
    "href": "materials/06_statistical_analysis.html#references",
    "title": "6  Statistical analysis",
    "section": "References",
    "text": "References\n\n\n\n\nHutchings, Charlotte, Charlotte S. Dawson, Thomas Krueger, Kathryn S. Lilley, and Lisa M. Breckels. 2024. “A Bioconductor Workflow for Processing, Evaluating, and Interpreting Expression Proteomics Data.” F1000Research 12 (September): 1402. https://doi.org/10.12688/f1000research.139116.2.\n\n\nPhipson, Belinda, Stanley Lee, Ian J. Majewski, Warren S. Alexander, and Gordon K. Smyth. 2016. “Robust Hyperparameter Estimation Protects Against Hypervariable Genes and Improves Power to Detect Differential Expression.” The Annals of Applied Statistics 10 (2). https://doi.org/10.1214/16-aoas920.\n\n\nSmyth, Gordon K. 2004. “Linear Models and Empirical Bayes Methods for Assessing Differential Expression in Microarray Experiments.” Statistical Applications in Genetics and Molecular Biology 3 (1): 1–25. https://doi.org/10.2202/1544-6115.1027.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Expression proteomics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical analysis</span>"
    ]
  },
  {
    "objectID": "materials/07_biological_interpretation.html",
    "href": "materials/07_biological_interpretation.html",
    "title": "7  Biological interpretation",
    "section": "",
    "text": "7.1 Adding metadata to our results using dplyr\nBefore we can look any further into the biological meaning of any protein abundance changes we need to extract these proteins from our overall results. It is also useful to re-add information about the master protein descriptions since this is lost in the output of limma analysis.\nIt is important to note that the results table we have generated from limma is not in the same order as the input data. Therefore, to add information from our original data e.g., from the rowData such as the Master.Protein.Descriptions we must match the protein names between them.\nTo do this, let’s extract information the Master.Protein.Descriptions from the original data we have created called all_proteins.\nRecall that all_proteins is a SummarizedExperiment object,\nall_proteins\n\nclass: SummarizedExperiment \ndim: 3823 6 \nmetadata(0):\nassays(2): assay aggcounts\nrownames(3823): A0A0B4J2D5 A0A2R8Y4L2 ... Q9Y6X9 Q9Y6Y8\nrowData names(21): Checked Tags ... Protein.FDR.Confidence .n\ncolnames(6): M_1 M_2 ... G1_2 G1_3\ncolData names(4): sample rep condition tag\nWe wish to extract information from the rowData regarding the Master.Protein.Descriptions,\n## Add master protein descriptions back\nprotein_info &lt;- all_proteins %&gt;%\n  rowData() %&gt;%\n  as_tibble() %&gt;%\n  select(Protein = Master.Protein.Accessions, \n         Protein_description = Master.Protein.Descriptions)\n\nprotein_info %&gt;% head()\n\n# A tibble: 6 × 2\n  Protein    Protein_description                                                \n  &lt;chr&gt;      &lt;chr&gt;                                                              \n1 A0A0B4J2D5 Putative glutamine amidotransferase-like class 1 domain-containing…\n2 A0A2R8Y4L2 Heterogeneous nuclear ribonucleoprotein A1-like 3 OS=Homo sapiens …\n3 A0AVT1     Ubiquitin-like modifier-activating enzyme 6 OS=Homo sapiens OX=960…\n4 A0MZ66     Shootin-1 OS=Homo sapiens OX=9606 GN=SHTN1 PE=1 SV=4               \n5 A1L0T0     2-hydroxyacyl-CoA lyase 2 OS=Homo sapiens OX=9606 GN=ILVBL PE=1 SV…\n6 A1X283     SH3 and PX domain-containing protein 2B OS=Homo sapiens OX=9606 GN…\nNote, we also extract the Master.Protein.Accessions column so we can use this to match to the protein column in our limma results.\nNow we can use the left_join function from dplyr to match the protein descriptions to the protein IDs,\nlimma_results &lt;- limma_results %&gt;% \n  left_join(protein_info, by = \"Protein\", suffix = c(\".left\", \".right\"))\n\n# Verify\nlimma_results %&gt;%\n  head()\n\n  Protein     logFC   AveExpr         t      P.Value    adj.P.Val        B\n1  Q9NQW6 -2.077921  2.641270 -40.40812 1.139582e-15 4.356623e-12 25.79468\n2  Q9BW19 -2.343592  1.430210 -36.79774 4.073530e-15 7.786553e-12 24.71272\n3  P49454 -1.991326  2.598481 -34.88623 8.411188e-15 9.266757e-12 24.07905\n4  Q9ULW0 -2.152503  2.476087 -34.52309 9.695796e-15 9.266757e-12 23.95341\n5  Q562F6 -3.205603 -1.304111 -32.91865 1.849860e-14 1.414403e-11 23.37677\n6  O14965 -2.098029  0.797902 -31.22180 3.791132e-14 2.415583e-11 22.72604\n  direction significance\n1      down          sig\n2      down          sig\n3      down          sig\n4      down          sig\n5      down          sig\n6      down          sig\n                                                    Protein_description\n1                     Anillin OS=Homo sapiens OX=9606 GN=ANLN PE=1 SV=2\n2 Kinesin-like protein KIFC1 OS=Homo sapiens OX=9606 GN=KIFC1 PE=1 SV=2\n3       Centromere protein F OS=Homo sapiens OX=9606 GN=CENPF PE=1 SV=3\n4 Targeting protein for Xklp2 OS=Homo sapiens OX=9606 GN=TPX2 PE=1 SV=2\n5                 Shugoshin 2 OS=Homo sapiens OX=9606 GN=SGO2 PE=1 SV=2\n6            Aurora kinase A OS=Homo sapiens OX=9606 GN=AURKA PE=1 SV=3",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Expression proteomics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Biological interpretation</span>"
    ]
  },
  {
    "objectID": "materials/07_biological_interpretation.html#adding-metadata-to-our-results-using-dplyr",
    "href": "materials/07_biological_interpretation.html#adding-metadata-to-our-results-using-dplyr",
    "title": "7  Biological interpretation",
    "section": "",
    "text": "NoteManipulating data with dplyr and tidyverse\n\n\n\nThere is lots of information online about getting started with dplyr and using the tidyverse. We really like this lesson from the Data Carpentry if you are new to the tidyverse.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Expression proteomics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Biological interpretation</span>"
    ]
  },
  {
    "objectID": "materials/07_biological_interpretation.html#subset-differentially-abundant-proteins",
    "href": "materials/07_biological_interpretation.html#subset-differentially-abundant-proteins",
    "title": "7  Biological interpretation",
    "section": "7.2 Subset differentially abundant proteins",
    "text": "7.2 Subset differentially abundant proteins\nLet’s subset our results and only keep proteins which have been flagged as exhibiting significant abundance changes,\n\nsig_changing &lt;- limma_results %&gt;% \n  as_tibble() %&gt;%\n  filter(significance == \"sig\")\n\nsig_up &lt;- sig_changing %&gt;%\n  filter(direction == \"up\")\n\nsig_down &lt;- sig_changing %&gt;%\n  filter(direction == \"down\")",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Expression proteomics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Biological interpretation</span>"
    ]
  },
  {
    "objectID": "materials/07_biological_interpretation.html#biological-interpretation-of-differentially-abundant-proteins",
    "href": "materials/07_biological_interpretation.html#biological-interpretation-of-differentially-abundant-proteins",
    "title": "7  Biological interpretation",
    "section": "7.3 Biological interpretation of differentially abundant proteins",
    "text": "7.3 Biological interpretation of differentially abundant proteins\nOur statistical analyses provided us with a list of proteins that are present with significantly different abundances between M-phase and G1-phase of the cell cycle. We can get an initial idea about what these proteins are and do by looking at the protein descriptions.\n\n## Look at descriptions of proteins upregulated in M relative to G1\nsig_up %&gt;%\n  pull(Protein_description) %&gt;%\n  head()\n\n[1] \"Golgi reassembly-stacking protein 2 OS=Homo sapiens OX=9606 GN=GORASP2 PE=1 SV=3\"\n[2] \"Selenoprotein S OS=Homo sapiens OX=9606 GN=SELENOS PE=1 SV=3\"                    \n[3] \"Anamorsin OS=Homo sapiens OX=9606 GN=CIAPIN1 PE=1 SV=2\"                          \n[4] \"Krueppel-like factor 16 OS=Homo sapiens OX=9606 GN=KLF16 PE=1 SV=1\"              \n[5] \"Histone H1.10 OS=Homo sapiens OX=9606 GN=H1-10 PE=1 SV=1\"                        \n[6] \"Importin subunit alpha-4 OS=Homo sapiens OX=9606 GN=KPNA3 PE=1 SV=2\"             \n\n\nWhilst we may recognise some of the changing proteins, this might be the first time that we are coming across others. Moreover, some protein descriptions contain useful information, but this is very limited. We still want to find out more about the biological role of the statistically significant proteins so that we can infer the potential effects of their abundance changes.\nThere are many functional analyses that could be done on the proteins with differential abundance:\n\nInvestigate the biological pathways that the proteins function within (KEGG etc.)\nIdentify potential interacting partners (IntAct, STRING)\nDetermine the subcellular localisation in which the changing proteins are found\nUnderstand the co-regulation of their mRNAs (Expression Atlas)\nCompare our changing proteins to those previously identified in other proteomic studies of the cell cycle\n\n\n7.3.1 Gene Ontology (GO) enrichment analysis\nOne of the common methods used to probe the biological relevance of proteins with significant changes in abundance between conditions is to carry out Gene Ontology (GO) enrichment, or over-representation, analysis.\nThe Gene Ontology consortium have defined a set of hierarchical descriptions to be assigned to genes and their resulting proteins. These descriptions are split into three categories: cellular components (CC), biological processes (BP) and molecular function (MF). The idea is to provide information about a protein’s subcellular localisation, functionality and which processes it contributes to within the cell. Hence, the overarching aim of GO enrichment analysis is to answer the question:\n“Given a list of proteins found to be differentially abundant in my phenotype of interest, what are the cellular components, molecular functions and biological processes involved in this phenotype?”.\nUnfortunately, just looking at the GO terms associated with our differentially abundant proteins is insufficient to draw any solid conclusions. For example, if we find that 120 of the 453 proteins significantly downregulated in M phase are annotated with the GO term “kinase activity”, it may seem intuitive to conclude that reducing kinase activity is important for the M-phase phenotype. However, if 90% of all proteins in the cell were kinases (an extreme example), then we might expect to discover a high representation of the “kinase activity” GO term in any protein list we end up with.\nThis leads us to the concept of an over-representation analysis. We wish to ask whether any GO terms are over-represented (i.e., present at a higher frequency than expected by chance) in our lists of differentially abundant proteins. In other words, we need to know how many proteins with a GO term could have shown differential abundance in our experiment vs. how many proteins with this GO term did show differential abundance in our experiment.\nWe are going to use a function in R called enrichGO from the the clusterProfiler Yu et al. (2012) Bioconductor R package to perform GO enrichment analysis. The package vignette can be found here. and full tutorials for using the package here\n\n\n\n\n\n\nNoteAnnotation packages in Bioconductor\n\n\n\nThe enrichGO function uses the org.Hs.eg.db Bioconductor package that has genome wide annotation for human. It also uses the GO.db package which is a set of annotation maps describing the entire Gene Ontology assembled using data from GO.\nUnfortunately, because GO annotations are by design at the gene level, performing analyses with proteomics data can be more tricky.\n\n\nIn the next code chunk we call the enrichGO function.\n\nego_down &lt;- enrichGO(gene = sig_down$Protein,               # list of down proteins\n                     universe = limma_results$Protein,      # all proteins \n                     OrgDb = org.Hs.eg.db,                  # database to query\n                     keyType = \"UNIPROT\",                   # protein ID encoding \n                     qvalueCutoff = 0.05,\n                     ont = \"BP\",                            # can be CC, MF, BP, or ALL\n                     readable = TRUE)\n\nLet’s take a look at the output.\n\nhead(ego_down@result)\n\n                   ID                                   Description GeneRatio\nGO:0030198 GO:0030198             extracellular matrix organization    24/443\nGO:0043062 GO:0043062          extracellular structure organization    24/443\nGO:0045229 GO:0045229 external encapsulating structure organization    24/443\nGO:0055085 GO:0055085                       transmembrane transport    61/443\nGO:0045333 GO:0045333                          cellular respiration    35/443\nGO:0030199 GO:0030199                  collagen fibril organization    13/443\n            BgRatio RichFactor FoldEnrichment   zScore       pvalue\nGO:0030198  52/3673  0.4615385       3.826706 7.601862 8.298786e-10\nGO:0043062  52/3673  0.4615385       3.826706 7.601862 8.298786e-10\nGO:0045229  52/3673  0.4615385       3.826706 7.601862 8.298786e-10\nGO:0055085 244/3673  0.2500000       2.072799 6.422166 5.339347e-09\nGO:0045333 107/3673  0.3271028       2.712074 6.655422 9.470160e-09\nGO:0030199  19/3673  0.6842105       5.672924 7.561939 1.326766e-08\n               p.adjust       qvalue\nGO:0030198 7.753832e-07 6.563320e-07\nGO:0043062 7.753832e-07 6.563320e-07\nGO:0045229 7.753832e-07 6.563320e-07\nGO:0055085 3.741547e-06 3.167076e-06\nGO:0045333 5.308972e-06 4.493840e-06\nGO:0030199 6.198208e-06 5.246545e-06\n                                                                                                                                                                                                                                                                                                                                                                                                                                 geneID\nGO:0030198                                                                                                                                                                                                                                                                      CCN1/QSOX1/COL12A1/ITGB1/SERPINH1/APP/LAMC1/PLOD2/COL23A1/P4HA1/FLOT1/LOXL2/COL4A2/FKBP10/PLOD1/MMP2/CRTAP/PRDX4/COLGALT1/PLOD3/TGFB2/ERO1A/COL1A1/DAG1\nGO:0043062                                                                                                                                                                                                                                                                      CCN1/QSOX1/COL12A1/ITGB1/SERPINH1/APP/LAMC1/PLOD2/COL23A1/P4HA1/FLOT1/LOXL2/COL4A2/FKBP10/PLOD1/MMP2/CRTAP/PRDX4/COLGALT1/PLOD3/TGFB2/ERO1A/COL1A1/DAG1\nGO:0045229                                                                                                                                                                                                                                                                      CCN1/QSOX1/COL12A1/ITGB1/SERPINH1/APP/LAMC1/PLOD2/COL23A1/P4HA1/FLOT1/LOXL2/COL4A2/FKBP10/PLOD1/MMP2/CRTAP/PRDX4/COLGALT1/PLOD3/TGFB2/ERO1A/COL1A1/DAG1\nGO:0055085 THBS1/ITGB1/HSPA5/RACGAP1/TOMM70/MAGT1/ATP5F1A/SLC3A2/ATP1B1/SLC39A14/SLC2A1/HSPD1/SLC29A1/TIMM50/HSPA9/ASPH/TIMM21/ITGAV/NDUFS2/AIFM1/USP9X/TMEM165/SLC12A2/NNT/FLNA/PEX14/AFG3L2/UQCRC1/SLC25A3/NDUFS7/RALBP1/YES1/ATP5F1B/ERO1A/ARPP19/FMR1/ABCB7/GHITM/MT-CO2/ATP2A2/GRPEL1/ATP2B1/SLC25A24/TIMM44/ATP1A1/ATP6AP2/LETM1/SLC7A1/GOPC/ATP5PB/RPS6KB1/NDUFB3/UQCRFS1/TOMM40/ACTN4/COX4I1/SLC30A1/PMPCB/NDUFB8/STXBP3/SLC1A5\nGO:0045333                                                                                                                                                                                                            PLEC/ATP5F1A/CS/SOD2/SDHA/ACO2/SHMT2/NDUFS2/NNT/NDUFB11/ETFA/MDH2/SDHB/SUCLA2/UQCRC1/UQCRC2/CDK1/NDUFS7/SUCLG2/ATP5F1B/GHITM/MT-CO2/DLST/DLD/OXA1L/IDH3A/ATP5PB/STOML2/NDUFB3/CAT/HCCS/UQCRFS1/PDHB/COX4I1/NDUFB8\nGO:0030199                                                                                                                                                                                                                                                                                                                                      COL12A1/SERPINH1/PLOD2/P4HA1/LOXL2/FKBP10/PLOD1/CRTAP/COLGALT1/PLOD3/TGFB2/ERO1A/COL1A1\n           Count\nGO:0030198    24\nGO:0043062    24\nGO:0045229    24\nGO:0055085    61\nGO:0045333    35\nGO:0030199    13\n\n\nThe output of the enrichGO function is an object of class enrichResult that contains the ID and Description of all enriched GO terms. There is also information about which geneIDs from our significantly downregulated proteins are annotated with each of the enriched GO terms. Let’s take a look at the descriptions.\n\nego_down$Description %&gt;% \n  head(10)\n\n [1] \"extracellular matrix organization\"            \n [2] \"extracellular structure organization\"         \n [3] \"external encapsulating structure organization\"\n [4] \"transmembrane transport\"                      \n [5] \"cellular respiration\"                         \n [6] \"collagen fibril organization\"                 \n [7] \"cell adhesion\"                                \n [8] \"mitochondrial transmembrane transport\"        \n [9] \"cell division\"                                \n[10] \"aerobic respiration\"                          \n\n\nThere is a long list because of the hierarchical nature of GO terms. The results of GO enrichment analysis can be visualised in many different ways. For a full overview of GO enrichment visualisation tools see Visualization of functional enrichment result.\nHere, we’ll use a ‘dotplot’ first\n\np &lt;- dotplot(ego_down, \n        x = \"Count\", \n        showCategory = 20, \n        font.size = 10,\n        label_format = 100,\n        color = \"p.adjust\")\n\nprint(p)\n\n\n\n\n\n\n\n\nThe dotplot gives a good overview of the over-representation results for the top GO terms, but it’s not clear how much overlap there is between the proteins annotated with each term. For this, we can use an ‘upset’ plot.\n\nupsetplot(ego_down, n=10)\n\n\n\n\n\n\n\n\nIt’s usually informative to explore the proteins with the over-represented GO terms further. For this, we need to know which proteins have a particular GO term. This is where is gets a little more tricky, since we need to map between Uniprot IDs and Entrez gene IDs for this.\nWe start by using the toTable method to turn the UNIPROT map into a data.frame.\n\n## Obtain a gene to protein mapping for the proteins in our QFeatures\n## org.Hs.egUNIPROT is part of org.Hs.eg.db\ng2p_map_df &lt;- toTable(org.Hs.egUNIPROT) %&gt;%\n  filter(uniprot_id %in% rownames(cc_qf[['log_norm_proteins']]))\n\n## Obtain the GO terms for each gene\ngo_submap &lt;- org.Hs.egGO[g2p_map_df$gene_id]\n\n## Merge the gene to protein map and GO terms for\n## genes to get GO terms for proteins\np2go &lt;- toTable(go_submap) %&gt;%\n  left_join(g2p_map_df, by='gene_id')\n\nWarning in left_join(., g2p_map_df, by = \"gene_id\"): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 33120 of `x` matches multiple rows in `y`.\nℹ Row 186 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\nNow we can visualise the proteins with a particular GO term. Here, we will use a heatmap to see the patterns in protein abundance differences between the conditions for a given GO term.\n\n\n\n\n\n\nExerciseExercise 1 - Challenge: Plot a heatmap for the significant cell division proteins\n\n\n\n\n\n\nLevel: \nMake a heatmap with the protein abundances in all samples for those proteins with ‘cell division’ GO annotation. You can use the ego_down@result data.frame to get the GO term for ‘cell division’, or, failing that, your favourite search engine. For the heatmap plotting, you can refer back to the end of the previous session where we used pheatmap.\n\nHow does the protein abundance in the desyncronised cells compare to the M/G1 phases? How does this inform your interpretation?\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\n\n## Get the GO ID for 'cell division'\ncell_division_go &lt;- ego_down@result %&gt;%\n  filter(Description=='cell division') %&gt;%\n  rownames()\n\n## Get the Uniprot IDs for cell division annotated proteins\ncell_division_uniprot_ids &lt;- p2go %&gt;%\n  filter(go_id==cell_division_go) %&gt;%\n  pull(uniprot_id) %&gt;%\n  unique()\n\n## Extract quant data\nquant_data &lt;- cc_qf[[\"log_norm_proteins\"]] %&gt;% assay()\n\n## Plot heatmap\npheatmap(mat = quant_data, \n         scale = \"row\",\n         show_rownames = FALSE)\n\n\n\n\n\n\n\n## Extract quant data for the significant proteins\nquant_data_cd &lt;- quant_data[intersect(sig_changing$Protein, cell_division_uniprot_ids), ]\n\n## Plot heatmap\npheatmap(mat = quant_data_cd, \n         scale = \"row\",\n         show_rownames = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nGene ontology (GO) terms described the molecular function (MF), biological processes (BP) and cellular component (CC) of genes (and their protein products).\nGO terms are hierarchical and generic. They do not relate to specific biological systems e.g., cell type or condition.\nGO enrichment analysis aims to identify GO terms that are present in a list of proteins of interest (foreground) at a higher frequency than expected by chance based on their frequency in a background list of proteins (universe). The universe should be a list of all proteins included identified and quantified in your experiment.\nThe enrichGO function from the clusterProfiler package provides a convenient way to carry out reproducible GO enrichment analysis in R.\nThere are many ways to visualise the results from functional enrichment analyses. With GO terms in particular, it’s important to consider the relationships between the gene sets we are testing for over-representation.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Expression proteomics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Biological interpretation</span>"
    ]
  },
  {
    "objectID": "materials/07_biological_interpretation.html#references",
    "href": "materials/07_biological_interpretation.html#references",
    "title": "7  Biological interpretation",
    "section": "References",
    "text": "References\n\n\n\n\nYu, Guangchuang, Li-Gen Wang, Yanyan Han, and Qing-Yu He. 2012. “clusterProfiler: An r Package for Comparing Biological Themes Among Gene Clusters.” OMICS: A Journal of Integrative Biology 16 (5): 284–87. https://doi.org/10.1089/omi.2011.0118.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Expression proteomics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Biological interpretation</span>"
    ]
  },
  {
    "objectID": "materials/08_maxquant_notes.html",
    "href": "materials/08_maxquant_notes.html",
    "title": "8  Adapting this workflow to MaxQuant processed data",
    "section": "",
    "text": "8.1 Using data processed with MaxQuant\nThis course was written for proteomics data processed by the Proteome Discoverer software, as that is what the Cambridge Centre for Proteomics core facility uses to process DDA data (TMT and LFQ). Nevertheless, the workflow and basic principles discussed are also applicable to the output of any similar proteomics raw data processing software, including MaxQuant, among others.\nHere we have outlined the differences to be aware of when following this course using MaxQuant output text files. The code as written will require some minor modifications to work properly with MaxQuant formatted data.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Extended materials",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Adapting this workflow to MaxQuant processed data</span>"
    ]
  },
  {
    "objectID": "materials/08_maxquant_notes.html#using-data-processed-with-maxquant",
    "href": "materials/08_maxquant_notes.html#using-data-processed-with-maxquant",
    "title": "8  Adapting this workflow to MaxQuant processed data",
    "section": "",
    "text": "The rough equivalent of the PSMs.txt file output by Proteome Discoverer is the evidence.txt file output by MaxQuant.\nDecoy PSMs (known false discoveries which are used to calculate false discovery rate) are automatically filtered out by Proteome Discoverer, but this is not the case with MaxQuant. Hence when working with MaxQuant outputs it is important to filter out rows with ‘+’ in the Reverse column.\nEquivalent column names and the type of data contained are described below. Ellipses are put where no equivalent column exists. \n\n\n\n\n\n\n\nNoteNaming of column headers in third party softwares\n\n\n\nColumn names are not only different between different softwares but also between different versions of the same software. Always check the column names and that they correspond to what you expect.\n\n\n\nPSM or peptide level files\n\n\n\n\n\n\n\nProteome Discoverer (PSMs.txt)\nMaxQuant (evidence.txt)\n\n\n\n\nAbundance (float)\nReporter.intensity.corrected (integer)\n\n\nSequence (string)\nSequence (string)\n\n\nMaster.Protein.Accessions (string)\nLeading.proteins (string)\n\n\nContaminants (string, True or False)\nPotential.contaminant (string, + or blank)\n\n\n…\nReverse (string, + or blank)\n\n\nRank (integer)\n…\n\n\nSearch.Engine.Rank (integer)\n…\n\n\nPSM.Ambiguity (string)\n…\n\n\nNumber.of.Protein.Groups (integer)\n… (you might calculate this by counting the number of ; in the Leading.proteins column and adding 1)\n\n\nAverage.Reporter.SN (float)\n… (you might calculate the average reporter ion intensity and threshold based on that instead)\n\n\nIsolation.Interference.in.Percent (float)\nPIF (float, to get the data in exactly the same format you have to calculate (1 - PIF)*100)\n\n\nSPS.Mass.Matches.in.Percent (integer)\n…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProtein level files\n\n\n\n\n\n\n\nProteome Discoverer (Proteins.txt)\nMaxQuant (proteinGroups.txt)\n\n\n\n\nAccession (string)\nMajority.protein.IDs (string)\n\n\nProtein.FDR.Confidence.Combined (string; High, Medium, or Low)\nQ.value (float, a Proteome Discoverer protein FDR of ‘High’ is equivalent to a Q.value &lt; 0.01)",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Extended materials",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Adapting this workflow to MaxQuant processed data</span>"
    ]
  },
  {
    "objectID": "materials/09_normalyzer.html",
    "href": "materials/09_normalyzer.html",
    "title": "9  Exploring normalisation methods",
    "section": "",
    "text": "9.1 Using NormalyzerDE\nSelecting an appropriate and optimal normalisation method will depend on the exact experimental design and data structure. Within the R Bioconductor packages, however, exists NormalyzerDE Willforss, Chawade, and Levander (2018), a tool for evaluating different normalisation methods.\nThe NormalyzerDE package provides a function called normalyzer which is useful for getting an overview of how different normalisation methods perform on a dataset. The normalyzer function however requires a raw intensity matrix as input, prior to any log transformation. Therefore, to use this function we need a non-log protein-level dataset.\nnorm_qf &lt;- cc_qf\n\nnorm_qf\n\nAn instance of class QFeatures (type: bulk) with 2 sets:\n\n [1] psms_raw: SummarizedExperiment with 45803 rows and 10 columns \n [2] psms_filtered: SummarizedExperiment with 25687 rows and 10 columns\nnorm_qf &lt;- aggregateFeatures(norm_qf, \n                             i = \"psms_filtered\",\n                             fcol = \"Master.Protein.Accessions\",\n                             name = \"proteins_direct\",\n                             fun = base::colSums,\n                             na.rm = TRUE)\n\n## Verify\nexperiments(norm_qf)\n\nExperimentList class object of length 3:\n [1] psms_raw: SummarizedExperiment with 45803 rows and 10 columns\n [2] psms_filtered: SummarizedExperiment with 25687 rows and 10 columns\n [3] proteins_direct: SummarizedExperiment with 3823 rows and 10 columns\nNote: To run normalyzer on this data we need to pass requireReplicates = FALSE as we have only one sample of the control. We pass the un-transformed data as the normalyzer function does an internal log2 transformation as part of its pipeline. For more details on using the NormalyzerDE package take a look at the package vignette.\nnormalyzer(jobName = \"normalyzer\",\n           experimentObj = norm_qf[[\"proteins_direct\"]],\n           sampleColName = \"sample\",\n           groupColName = \"condition\",\n           outputDir = \"normalyzer_output\",\n           requireReplicates = FALSE)\nIf your job is successful a new folder will be created in your working directory under outputs called normalyzer. Take a look at the PDF report.\nThe output report contains:",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Extended materials",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exploring normalisation methods</span>"
    ]
  },
  {
    "objectID": "materials/09_normalyzer.html#using-normalyzerde",
    "href": "materials/09_normalyzer.html#using-normalyzerde",
    "title": "9  Exploring normalisation methods",
    "section": "",
    "text": "Create a copy of your cc_qf dataset for testing normalisation methods. Let’s call this norm_qf.\n\n\n\nTake the your data from the psms_filtered level and create a new assay in your QFeatures object (norm_qf) that aggregates the data from this level directly to protein level. Call this assay \"proteins_direct\".\n\n\n\nRun the normalyzer function on the newly created (untransformed) protein level data using the below code.\n\n\n\n\n\n\nTotal intensity plot: Barplot showing the summed intensity in each sample for the log2-transformed data\nTotal missing plot: Barplot showing the number of missing values found in each sample for the log2-transformed data\nLog2-MDS plot: MDS plot where data is reduced to two dimensions allowing inspection of the main global changes in the data\nScatterplots: The first two samples from each dataset are plotted.\nQ-Q plots: QQ-plots are plotted for the first sample in each normalized dataset.\nBoxplots: Boxplots for all samples are plotted and colored according to the replicate grouping.\nRelative Log Expression (RLE) plots: Relative log expression value plots. Ratio between the expression of the variable and the median expression of this variable across all samples. The samples should be aligned around zero. Any deviation would indicate discrepancies in the data.\nDensity plots: Density distributions for each sample using the density function. Can capture outliers (if single densities lies far from the others) and see if there is batch effects in the dataset (if for instance there is two clear collections of lines in the data).\nMDS plots: Multidimensional scaling plot using the cmdscale() function from the stats package. Is often able to show whether replicates group together, and whether there are any clear outliers in the data.\nDendograms: Generated using the hclust function. Data is centered and scaled prior to analysis. Coloring of replicates is done using as.phylo from the ape package.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Extended materials",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exploring normalisation methods</span>"
    ]
  },
  {
    "objectID": "materials/09_normalyzer.html#interpreting-the-results-of-normalyzer",
    "href": "materials/09_normalyzer.html#interpreting-the-results-of-normalyzer",
    "title": "9  Exploring normalisation methods",
    "section": "9.2 Interpreting the results of Normalyzer",
    "text": "9.2 Interpreting the results of Normalyzer\nWhen interpreting our normalyzer output we need to consider our experimental design. If all samples come from the same cells and we don’t expect the treatment/conditions to cause huge changes to the proteome, we expect the distributions of the intensities to be roughly the same. We can compare across samples by plotting the distribution in each sample together. When doing this you should get an idea of where the majority of the intensities lie. We expect samples from the same condition to have intensities that lie in the same range and if they do not then we can assume that this is due to technical noise, and we want to normalise for this technical variability.\n\n\n\n\n\n\n\n\nFigure 9.1\n\n\n\n\n\nFigure 9.1 shows a screenshot of the PDF report output from running the normalyzer pipeline. We see that the data before normalisation (log2, topleft) has curves/peaks at different locations and what we want to do is try and register the curves at the same location.\nWhich method to choose? For the use-case data, there is no clear differences when applying different normalisation methods within normalyzer. Really you need to look at the underlying summary statistics. For example, the mean is very sensitive to outliers and in proteomics we often have outliers, so this is not a method we would choose. The median (or median-based methods) are a good choice for most quantitative proteomics data. Quantile normalisation is not recommended for quantitative proteomics data. Quantile methods will not change the median but will change all quantiles of the distribution so that all distributions coincide. We could do this but this often causes problems due to the fact that we have missing data in proteomics. This makes the normalisation even more challenging than in other omics types of data.\nThe decision is ultimately up to the user, but it is often best to explore different normalisation methods and their impact on the data.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Extended materials",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exploring normalisation methods</span>"
    ]
  },
  {
    "objectID": "materials/09_normalyzer.html#references",
    "href": "materials/09_normalyzer.html#references",
    "title": "9  Exploring normalisation methods",
    "section": "References",
    "text": "References\n\n\n\n\nWillforss, Jakob, Aakash Chawade, and Fredrik Levander. 2018. “NormalyzerDE: Online Tool for Improved Normalization of Omics Expression Data and High-Sensitivity Differential Expression Analysis.” Journal of Proteome Research 18 (2): 732–40. https://doi.org/10.1021/acs.jproteome.8b00523.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Extended materials",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exploring normalisation methods</span>"
    ]
  },
  {
    "objectID": "materials/10_lfq_vs_tmt.html",
    "href": "materials/10_lfq_vs_tmt.html",
    "title": "10  Adapting this workflow to label-free proteomics data",
    "section": "",
    "text": "10.1 Data import\nThe processing and analysis of label-free quantitative proteomics data to discover differentially abundant proteins follows the same overall workflow presented in this course for our TMT use-case.\nHowever, some of the decisions made and exact approaches to each stage may differ when considering label-free data. These decisions are discussed below and summarised in Table 10.1.\nAs outlined in Import and infrastructure, we prefer to import the data into R at the lowest possible level. This allows us to have more control over and understanding of our data processing and analysis. For the TMT-labelled use-case data, the lowest possible level of data for import was the PSM-level. However, the analysis of label-free data often requires us to start one level up at the peptide-level. This is because most identification searches carried out on label-free MS data utilise an algorithm called retention time (RT) alignment, which uses the match between runs (MBR) function.\nWhat problem does retention time alignment address?\nRetention time alignment aims to deal with the problem of missing values in label-free data-dependent acquisition (DDA) MS data. Since label-free samples are all analysed by independent MS runs, the stochastic nature of DDA means that different peptides are identified and quantified between samples, hence there are a high number of missing values.\nHow does retention time alignment work?\nQuantification of label-free samples is achieved at the MS1 level. This means that we have potentially useful quantitative information before we have any peptide identification (MS1 before MS2). In cases where a peptide is identified in some samples but not others, it is possible to align the retention times of each sample run and then compare the MS1 spectra. In this way, information can be shared across runs and a peptide identification made in one run can be assigned to an MS1 spectra from a completely independent run, even if this spectrum does not have a corresponding MS2 spectrum.\nWhy does retention time alignment prevent analysis from PSM level?\nThe process of RT alignment and MBRs occurs after the process of peptide spectrum matching. First, PSMs are derived from an identification search. This is done independently for each sample. The remaining spectra for which no PSM was identified are then included in the RT alignment algorithm in an attempt to assign an identification. If successful, this means that there may be peptide level data in the absence of PSM level data. Hence, if we used PSM level data for the processing and analysis of label-free data then we would lose out on the benefit of RT alignment.\nWhen to use peptide-level data for label-free analysis?\nLabel-free data processed using Proteome Discoverer software should be processed from the peptide level. This means that we would use the file called cell_cycle_total_proteome_analysis_PeptideGroups.txt and import using readQFeatures, as outlined in Import and infrastructure. Other third party software, however, may still allow for label-free data to be processed from the PSM level. For example, MaxQuant users can still use the evidence.txt file.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Extended materials",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Adapting this workflow to label-free proteomics data</span>"
    ]
  },
  {
    "objectID": "materials/10_lfq_vs_tmt.html#data-import",
    "href": "materials/10_lfq_vs_tmt.html#data-import",
    "title": "10  Adapting this workflow to label-free proteomics data",
    "section": "",
    "text": "Note\n\n\n\nThese notes on data import are an adjunct to the Import and infrastructure section. Please first read through that material, since it includes background and clarifications which are not repeated here.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Extended materials",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Adapting this workflow to label-free proteomics data</span>"
    ]
  },
  {
    "objectID": "materials/10_lfq_vs_tmt.html#data-cleaning-quality-control-filtering-and-fdr-control",
    "href": "materials/10_lfq_vs_tmt.html#data-cleaning-quality-control-filtering-and-fdr-control",
    "title": "10  Adapting this workflow to label-free proteomics data",
    "section": "10.2 Data cleaning, quality control filtering and FDR control",
    "text": "10.2 Data cleaning, quality control filtering and FDR control\n\n\n\n\n\n\nNote\n\n\n\nThese notes on data import are an adjunct to the Data processing section. Please first read through that material, since it includes background and clarifications which are not repeated here.\n\n\nMany of the basic data cleaning steps that we apply to TMT-labelled quantitative proteomics data are still applicable to label-free data. The following steps should still be completed using the filterFeatures function, as demonstrated in the Data processing section.\nRemoval of features:\n\nWithout a master protein accession = filterFeatures(~ Master.Protein.Accessions != \"\")\nAssociated with contaminant accessions = filterFeatures(~ Contaminant == \"False\")\nLacking quantitative data = filterFeatures(~ Quan.Info != \"NoQuanValues\")\nWhich are not unique (based on user’s definition) = filterFeatures(~ Number.of.Protein.Groups == 1)\nWhich are not allocated as rank 1 = filterFeatures(~ Rank == 1) and filterFeatures(~ Search.Engine.Rank == 1)\nWhich are not unambiguous matches = filterFeatures(~ PSM.Ambiguity == \"Unambiguous\")\n\nIn addition to these data cleaning steps, users may wish to remove features (peptides) which were not quantified based on a monoisotopic peak from their label-free dataset. This can be achieved using filterFeatures(~ Quan.Info != \"NoneMonoisotopic\").\nThe three quality control filters applied to the TMT use-case data (Isolation.Interference.in.Percent, Average.Reporter.SN and SPS.Mass.Matches.in.Percent) are TMT-specific and cannot be applied to label-free data.\nProtein-level FDR control should be carried out on label-free data in the same way as was demonstrated in the main course materials.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Extended materials",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Adapting this workflow to label-free proteomics data</span>"
    ]
  },
  {
    "objectID": "materials/10_lfq_vs_tmt.html#managing-missing-data",
    "href": "materials/10_lfq_vs_tmt.html#managing-missing-data",
    "title": "10  Adapting this workflow to label-free proteomics data",
    "section": "10.3 Managing missing data",
    "text": "10.3 Managing missing data\n\n\n\n\n\n\nNote\n\n\n\nThese notes on management of missing data are an adjunct to the Data processing section which demonstrates the exploration of missing values within QFeatures. Please first read through that material, since it includes background and clarifications which are not repeated here.\n\n\nLabel-free DDA proteomics data suffers from a greater number of missing values than multiplexed label-based approaches (e.g., TMT). Indeed, this is one of the advantages of multi-plexing samples using TMT as multiple samples (10 in the use-case) can be run simultaneously on the MS and, therefore, the same peptides are selected for analysis across all samples. Since label-free samples are each analysed via independent MS runs, the stochastic nature of DDA MS means that different peptides may be identified and quantified across different runs, thus leading to a higher percentage of missing values.\nManagement of missing data should still follow the same three steps as discussed in the Data processing section:\n\nExplore the presence and distribution of missing values\nFilter data to remove features (rows) or samples (columns) with excessive missing values\nConsider the use of imputation\n\nSteps 1 and 2 were outlined in the main course content. For the use-case TMT data we decided to remove all missing values rather than impute, since this would not represent a drastic data loss. For datasets with a higher proportion of missing data, imputation can be considered. However, protein summarisation using an approach that can handle missing values appropriately is likely to be the optimal approach (see Section 10.4).\nHow can I impute using QFeatures?\nImputation can be achieved within the QFeatures infrastructure using the impute function. To see what imputation methods this function facilitates we can use MsCoreUtils::imputeMethods().\n\nMsCoreUtils::imputeMethods()\n\n [1] \"bpca\"    \"knn\"     \"QRILC\"   \"MLE\"     \"MLE2\"    \"MinDet\"  \"MinProb\"\n [8] \"min\"     \"zero\"    \"mixed\"   \"nbavg\"   \"with\"    \"RF\"      \"none\"   \n\n\nFor example, to impute using a k-NN method, we would use the following code,\n\ncc_qf &lt;- impute(object = cc_qf,\n                method = \"knn\", \n                i = \"psms_filtered\",\n                name = \"psms_imputed\")\n\ncc_qf\n\nWhich imputation method should I use for my data?\nMissing values exist in the data for different reasons and these reasons dictate the best way in which to impute. For example, if a value is missing because a peptide is completely absent or present at an abundance below the limit of detection then the most suitable replacement value is arguably the lowest abundance value recorded in the data set (since this represents the limit of detection). Alternatively, if a value is missing because of stochastic technical reasons then it might be more appropriate to replace it with a value derived from a similar peptide. Overall, left-censored methods such as minimal value and limit of detection approaches work best for data that is MNAR (intensity-dependent missing values). Hot deck methods such as k-nearest neighbors, random forest and maximum likelihood methods work better for data that is MCAR (intensity-independent). To confuse the situation further, most data sets contain missing values that are a mixture of MNAR and MCAR, so mixed imputation methods can be applied.\nAt what stage of the workflow should I impute?\nThere are two aspects to consider when deciding when to impute:\n\nWhich data level should be imputed - PSM, peptide or protein\nWhether the selected imputation method requires raw or log transformed quantitation data\n\nMissing values can be imputed at any data level e.g., PSM, peptide or protein. However, if missing values are not imputed in lower data levels then users should be aware of how their missing values are treated during summarisation. Data summarisation methods deal with missing values in different ways, either ignoring them, removing them, considering missing values to be zero, or propagating them. Thus, a combined strategy for imputation and summarisation must be arrived at. In general, we advise that where imputation is necessary, it should be completed at the lowest possible data level to maintain transparency and allow users to check that the data structure has not been drastically altered (e.g., by checking summary statistics or plotting density plots pre- and post-imputation). For LFQ, we advise summarisation using the robustSummary method (see Section 10.4), which negates the need to impute missing values.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Extended materials",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Adapting this workflow to label-free proteomics data</span>"
    ]
  },
  {
    "objectID": "materials/10_lfq_vs_tmt.html#sec-robust",
    "href": "materials/10_lfq_vs_tmt.html#sec-robust",
    "title": "10  Adapting this workflow to label-free proteomics data",
    "section": "10.4 Summarisation to protein level",
    "text": "10.4 Summarisation to protein level\nSummarisation of label-free data can still be achieved using aggregateFeatures. If imputation has been completed, there should be no missing values left to influence summarisation.\nHere we will use robustSummary, a state-of-the art summarisation method that is able to summarise effectively even in the presence of missing values Sticker et al. (2020). robustSummary directly models the log-transformed peptide-level quantification as being dependent upon the protein-level abundance of the sample plus a peptide-level effect. Thus robustSummary estimates the protein-level abundances within the modelling. This modelling-based approach to protein summarisation can handle relatively sparse data as it only considers the finite data. The only requirement for a peptide to be informative for estimating protein-level abundances using robustSummary is that the peptide be quantified in at least two samples.\nSince the robustSummary can handle missing values, it negates the need to impute missing values. Indeed, for LFQ experiments, we recommend not imputing and using robustSummary to summarise to protein-level abundance. As expected, protein-level abundance estimates are less accurate the more sparse the data is, so removal of peptides with excessive missing values may be worthwhile.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Extended materials",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Adapting this workflow to label-free proteomics data</span>"
    ]
  },
  {
    "objectID": "materials/10_lfq_vs_tmt.html#logarithmic-transformation",
    "href": "materials/10_lfq_vs_tmt.html#logarithmic-transformation",
    "title": "10  Adapting this workflow to label-free proteomics data",
    "section": "10.5 Logarithmic transformation",
    "text": "10.5 Logarithmic transformation\nAs discussed in the Normalisation and data aggregation section, logarithmic transformation of the data is required to give our data a Gaussian distribution, as required for downstream differential abundance analysis. This step can happen at any stage of the workflow, depending upon which imputation and summarisation methods are selected. If you impute or summarise the data using a method which requires log transformation, then this step should have been done above. If not, log2 transformation can be completed now.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Extended materials",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Adapting this workflow to label-free proteomics data</span>"
    ]
  },
  {
    "objectID": "materials/10_lfq_vs_tmt.html#normalisation",
    "href": "materials/10_lfq_vs_tmt.html#normalisation",
    "title": "10  Adapting this workflow to label-free proteomics data",
    "section": "10.6 Normalisation",
    "text": "10.6 Normalisation\nThe rules when normalising label-free data are the same as the use-case TMT data. See Data normalisation and data aggregation and Using NormalyzerDE to explore normalisation methods for more discussion.\n\n\n\n\nSticker, Adriaan, Ludger Goeminne, Lennart Martens, and Lieven Clement. 2020. “Robust Summarization and Inference in Proteome-Wide Label-Free Quantification.” Molecular and Cellular Proteomics 19 (7): 1209–19. https://doi.org/10.1074/mcp.ra119.001624.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Extended materials",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Adapting this workflow to label-free proteomics data</span>"
    ]
  },
  {
    "objectID": "materials/11_statistical_analysis_all_stages.html",
    "href": "materials/11_statistical_analysis_all_stages.html",
    "title": "11  Statistical analysis of all cell cycle stages",
    "section": "",
    "text": "11.1 Differential expression analysis\nHaving cleaned our data, aggregated from PSM to protein level, completed a log2 transformation and normalised the data, we are now ready to carry out statistical analysis.\nThe aim of this section is to answer the question: “Which proteins show a significant change in abundance between the multiple cell cycle stages?”.\nOur null hypothesis is: (H0) The mean protein abundance is the same across cell cycle stages.\nOur alternative hypothesis is: (H1) The mean protein abundance differs across cell cycle stages.\nWe will use limma to perform the statistical tests using an empirical Bayes-moderated linear model. Please see the Statistical analysis section for more details about limma and its application in quantitative proteomics.\nDepending on whether the linear model is used to perform single comparisons or multifactorial comparisons, the test statistic will either be a t-value or an F-value, respectively. Here, we will perform both, starting with assessing the overall effect of cell cycle with an F-test. The F-test does not tell us which groups are different to one another, only that the cell cycle stage does affect protein abundance. Later we will see how to perform all the pairwise comparisons with t-tests.\nHere, we will start by performing a comparison between three groups (M phase, G1 phase and desynchronised) for each protein and obtain an F-value and p-value for each protein.\nFirst, we extract the suitable protein experiment. We will remove the pre-treatment sample, since this condition is not replicated and so not amenable to statistical testing.\n# extract the log-normalised experiment from our QFeatures object\nall_proteins &lt;- cc_qf[[\"log_norm_proteins\"]]\n\n# subset to exclude the pre-treatment sample\nall_proteins &lt;- all_proteins[, all_proteins$condition != 'Pre-treatment']",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Extended materials",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Statistical analysis of all cell cycle stages</span>"
    ]
  },
  {
    "objectID": "materials/11_statistical_analysis_all_stages.html#differential-expression-analysis",
    "href": "materials/11_statistical_analysis_all_stages.html#differential-expression-analysis",
    "title": "11  Statistical analysis of all cell cycle stages",
    "section": "",
    "text": "NoteWhat is an F-value?\n\n\n\nAn F-value is a parametric statistical value used to compare the mean values of three or more groups. The F-value is the ratio of the between-group variation (explained variance) and the within-group variance (unexplained variance). The higher the F-value, the more significant the difference between the groups is.\n\n\n\n\n\n\n\n\nNoteWhat is a t-value?\n\n\n\nA t-value is a parametric statistical value used to compare the mean values of two groups. The t-value is the ratio of the difference in means to the standard error of the difference in means. The further away from zero that a t-value lies, the more significant the difference between the groups is.\n\n\n\n\n\n\n\n\nNoteHow is the p-value obtained?\n\n\n\nA p-value may be obtained from a t-value/F-value by comparing the value against an t-value/F-distribution with the appropriate degrees of freedom.\n\ndegrees of freedom = the number of observations minus the number of independent variables in the model\np-value = the probability of achieving the t-value/F-value under the null hypothesis i.e., by chance\n\n\n\n\n\n\n\n11.1.1 Modelling with or without an intercept\nWhen investigating the effect of a single explanatory categorical variable with more than two levels, a design matrix can be created using either model.matrix(~variable) or model.matrix(~0 + variable). The former will create a model that includes an intercept term, whilst the latter will not. If an intercept is included, the first level of the variable (here, M) is considered the ‘baseline’ and modeled by the intercept. The subsequent levels of the variable (here, G1 and Desynch) are modeled by additional terms in the model, which capture the difference between each other level and the baseline (M). This approach makes intuitive sense if one group of samples are control samples to which all other groups should be compared. Although comparisons can be made between any pair of groups when using a model with an intercept, it’s less intuitive than a model without an intercept. In this experiment, none of the groups are a control group to compare to, and we wish to compare between all samples. Thus, we will model without an intercept here.\nFor further guidance on generating design matrices for covariates or continuous explanatory variables, see A guide to creating design matrices for gene expression experiments.\nBelow, we define the model matrix without any intercept term using the model.matrix(~0 + variable) formulation.\n\n## Ensure that conditions are stored as levels of a factor \n## Explicitly define level order by cell cycle stage\nall_proteins$condition &lt;- factor(all_proteins$condition, \n                                 levels = c(\"M\", \"G1\", \"Desynch\"))\n\n## Design a matrix containing all factors that we wish to model\ncondition &lt;- all_proteins$condition\nm_design &lt;- model.matrix(~0 + condition)\n\n# Rename design matrix columns to make them easier to refer to\ncolnames(m_design) &lt;- levels(condition)\n\n## Verify\nm_design\n\n  M G1 Desynch\n1 1  0       0\n2 1  0       0\n3 1  0       0\n4 0  1       0\n5 0  1       0\n6 0  1       0\n7 0  0       1\n8 0  0       1\n9 0  0       1\nattr(,\"assign\")\n[1] 1 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$condition\n[1] \"contr.treatment\"\n\n\n\n\n11.1.2 What are contrasts?\nWe also define contrasts. The contrasts represent comparisons are of interest to us, e.g M phase vs Desynchronised. This is important since we are not directly interested in the parameter (mean) estimates for each group but rather the differences in these parameter (mean) estimates between groups. The makeContrasts function is used by passing the name we wish to give each contrast and how this contrast should be calculated using column names from the design matrix. We also pass the levels argument to tell R where the column names come from i.e., which design matrix the contrasts are being applied to.\n\n## Specify contrasts of interest\ncontrasts &lt;- makeContrasts(G1_M = G1 - M, \n                           M_Des = M - Desynch, \n                           G1_Des = G1 - Desynch,\n                           levels = m_design)\n\n## Verify\ncontrasts\n\n         Contrasts\nLevels    G1_M M_Des G1_Des\n  M         -1     1      0\n  G1         1     0      1\n  Desynch    0    -1     -1\n\n\n\n\n11.1.3 Running an empirical Bayes moderated test using limma\nAfter we have specified the design matrix and contrasts we wish to make, the next step is to apply the statistical model.\n\n## Fit linear model using the design matrix and desired contrasts\nfit_model &lt;- lmFit(object = assay(all_proteins), design = m_design)\nfit_contrasts &lt;- contrasts.fit(fit = fit_model, contrasts = contrasts)\n\nThe initial model has now been applied to each of the proteins in our data. We now update the model using the eBayes function.\n\n## Update the model using the limma eBayes algorithm\nfinal_model &lt;- eBayes(fit = fit_contrasts, \n                      trend = TRUE,\n                      robust = TRUE)\n\n\n\n11.1.4 Accessing the model results\nTo get the results for all of our proteins we use the topTable function with the number = Inf argument.\n\n## Format results\nlimma_results_all_contrasts &lt;- topTable(\n  fit = final_model,   \n  coef = NULL, \n  adjust.method = \"BH\",    # Method for multiple hypothesis testing\n  number = Inf) %&gt;%        # Print results for all proteins\n  rownames_to_column(\"Protein\") \n\n## Verify\nhead(limma_results_all_contrasts)\n\n  Protein      G1_M      M_Des     G1_Des    AveExpr        F      P.Value\n1  Q9NQW6 -2.077921  0.8130706 -1.2648507  2.7165663 794.7797 5.148901e-15\n2  Q9ULW0 -2.152503  0.7854924 -1.3670105  2.5730063 685.1572 1.425086e-14\n3  P49454 -1.991326  1.1442558 -0.8470698  2.5489502 598.3086 3.606132e-14\n4  O14965 -2.098029  0.9147903 -1.1832391  0.8426435 551.3458 6.308302e-14\n5  Q562F6 -3.205603  1.4313091 -1.7742942 -1.2469471 546.9525 6.662987e-14\n6  Q15004 -1.355585 -1.0359248 -2.3915102  0.8346698 542.7099 7.027329e-14\n     adj.P.Val\n1 1.968425e-11\n2 2.724051e-11\n3 4.477580e-11\n4 4.477580e-11\n5 4.477580e-11\n6 4.477580e-11\n\n\n\n\n11.1.5 QC plots for statistical test assumptions\nFirst we examine whether there is the expected relationship between abundance and variance, and that the trend line captures this.\n\nplotSA(fit = final_model,\n       cex = 0.5,\n       xlab = \"Average log2 abundance\")\n\n\n\n\n\n\n\n\nNext, we check that the p-value distribution is as expected.\n\nlimma_results_all_contrasts %&gt;%\n  as_tibble() %&gt;%\n  ggplot(aes(x = P.Value)) + \n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Extended materials",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Statistical analysis of all cell cycle stages</span>"
    ]
  },
  {
    "objectID": "materials/11_statistical_analysis_all_stages.html#interpreting-the-overall-linear-model-output",
    "href": "materials/11_statistical_analysis_all_stages.html#interpreting-the-overall-linear-model-output",
    "title": "11  Statistical analysis of all cell cycle stages",
    "section": "11.2 Interpreting the overall linear model output",
    "text": "11.2 Interpreting the overall linear model output\nHaving checked that the model we fitted was appropriate for the data, we can now take a look at the results of our test.\nAs we saw above, topTable will give us the overall output of our linear model. We previously used this function to generate our limma_results_all_contrasts without specifying any value for the coef argument.\n\nhead(limma_results_all_contrasts)\n\n  Protein      G1_M      M_Des     G1_Des    AveExpr        F      P.Value\n1  Q9NQW6 -2.077921  0.8130706 -1.2648507  2.7165663 794.7797 5.148901e-15\n2  Q9ULW0 -2.152503  0.7854924 -1.3670105  2.5730063 685.1572 1.425086e-14\n3  P49454 -1.991326  1.1442558 -0.8470698  2.5489502 598.3086 3.606132e-14\n4  O14965 -2.098029  0.9147903 -1.1832391  0.8426435 551.3458 6.308302e-14\n5  Q562F6 -3.205603  1.4313091 -1.7742942 -1.2469471 546.9525 6.662987e-14\n6  Q15004 -1.355585 -1.0359248 -2.3915102  0.8346698 542.7099 7.027329e-14\n     adj.P.Val\n1 1.968425e-11\n2 2.724051e-11\n3 4.477580e-11\n4 4.477580e-11\n5 4.477580e-11\n6 4.477580e-11\n\n\nInterpreting the output of topTable for a multi-contrast model:\n\nG1_M, M_Des and G1_Des = the estimated log2FC for each model contrast\nAveExpr = the average log abundance of the protein across samples\nF = eBayes moderated F-value. Interpreted in the same way as a normal F-value (see above).\nP.Value = Unadjusted p-value\nadj.P.Val = FDR-adjusted p-value (adjusting across proteins but not multiple contrasts)\n\n\n11.2.1 Interpreting the results of a single contrast\nWe can look at individual contrasts by passing the contrast name to the coef argument in the topTable function. For example, let’s look at the pairwise comparison between M-phase and desynchronised cells. We use the topTable function to get the results of the \"M_Des\" contrast. We use the argument confint = TRUE so that the our output reports the 95% confidence interval of the calculated log2FC.\n\nM_Desynch_results &lt;- topTable(fit = final_model, \n                              coef = \"M_Des\", \n                              number = Inf,\n                              adjust.method = \"BH\",\n                              confint = TRUE) %&gt;%\n  rownames_to_column(\"Protein\")\n\n## Verify\nhead(M_Desynch_results)\n\n  Protein      logFC       CI.L       CI.R   AveExpr         t      P.Value\n1  P31350 -2.8479188 -3.0751121 -2.6207254 -1.113925 -26.91319 2.350968e-13\n2  O00622  1.3935764  1.2583727  1.5287801  1.822783  22.12969 3.333718e-12\n3  P46013  0.7490389  0.6686160  0.8294618  4.944857  19.99669 1.303459e-11\n4  P11388  1.0847590  0.9671981  1.2023198  4.047165  19.81088 1.477276e-11\n5  P49454  1.1442558  1.0202037  1.2683079  2.548950  19.80397 1.484201e-11\n6  P11021  0.6622147  0.5840995  0.7403298  5.655878  18.20106 4.586280e-11\n     adj.P.Val        B\n1 8.987752e-10 20.71754\n2 6.372403e-09 18.32184\n3 1.134820e-08 17.03986\n4 1.134820e-08 16.92071\n5 1.134820e-08 16.91625\n6 2.922225e-08 15.83230\n\n\nInterpreting the output of topTable for a single contrast:\n\nlogFC = the fold change between the mean log abundance in group A and the mean log abundance in group B\nCI.L = the left limit of the 95% confidence interval for the reported log2FC\nCI.R = the right limit of the 95% confidence interval for the reported log2FC\nAveExpr = the average log abundance of the protein across samples\nt = t-statistic derived from the original statistical test (not a t-test)\nP.Value = Unadjusted p-value\nadj.P.Val = FDR-adjusted p-value (adjusted across proteins but not multiple contrasts)\nB = B-statistic representing the log-odds that a protein is differentially abundant between conditions\n\nThis time the output of topTable contains a t-statistic rather than an F-value. This is because we only told the function to compare two conditions, so the corresponding t-statistic from our linear test is reported. Importantly, however, the p-value adjustment in this case only accounts for multiple tests across our 3823 proteins, not the three different contrasts/comparisons we used the data for. As a result, we could over-estimate the number of statistically significant proteins within this contrast, although this this effect is only likely to become problematic when we have a larger number of contrasts to account for.\n\n\n11.2.2 Visualising the results of our single contrast test\nBelow, we produce a volcano plot to visualisation the statistical test results for the M vs Desynchronised contrast.\n\nM_Desynch_results %&gt;%\n  mutate(significance = ifelse(adj.P.Val &lt; 0.01, \"sig\", \"not.sig\")) %&gt;%\n  ggplot(aes(x = logFC, y = -log10(P.Value), fill = significance)) +\n  geom_point(shape = 21, stroke = 0.25, size = 3) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n11.2.3 Interpreting the results of all contrasts\nTo understand the impact of adjusting for multiple hypothesis testing across our comparisons, we can use the decideTests function. This function provides a matrix of values -1, 0 and +1 to indicate whether a protein is significantly downregulated, unchanged or significantly upregulated in a given contrast. For the function to determine significance we have to provide a p-value adjustment method and threshold, here we use the standard Benjamini-Hochberg procedure for FDR adjustment and set a threshold of adj.P.Value &lt; 0.01 for significance.\nThe decideTests function also takes an argument called method. This argument specifies whether p-value adjustment should account only for multiple hypothesis tests across proteins (\"separate\") or across both proteins and contrasts (\"global\").\nLet’s first look at the results when we apply the \"separate\" method i.e., consider each contrast separately.\n\ndt &lt;- decideTests(object = final_model,\n            adjust.method = \"BH\", \n            p.value = 0.01, \n            method = \"separate\")\n\nsummary(dt)\n\n       G1_M M_Des G1_Des\nDown    429   438     86\nNotSig 3002  2757   3610\nUp      392   628    127\n\n\nFrom this table we can see the number of significantly changing proteins per contrast. For the M_Des comparison the total number of significantly changing proteins is 1066. This should be the same as the number of proteins with an adjusted p-value &lt; 0.01 in our M_Desynch_results object. Let’s check.\n\nM_Desynch_results %&gt;%\n  as_tibble() %&gt;%\n  filter(adj.P.Val &lt; 0.01) %&gt;% \n  nrow()\n\n[1] 1066\n\n\nHowever, if we use the \"global\" method for p-value adjustment and therefore adjust for both per protein and per contrast hypotheses we may see slightly fewer significant proteins in our M_Des contrast.\n\ndecideTests(object = final_model,\n            adjust.method = \"BH\", \n            p.value = 0.01, \n            method = \"global\") %&gt;%\n  summary()\n\n       G1_M M_Des G1_Des\nDown    413   403    110\nNotSig 3034  2835   3517\nUp      376   585    196\n\n\nUnfortunately, there is no way to specify global p-value adjustment accounting for all contrasts when using topTable to look at a single contrast. Instead, we can merge the results from our globally adjusted significance summary (generated using decideTests with method = \"global\") with the results of our overall linear model test (generated using topTable with coef = NULL). We demonstrate how to do this in the code below.\n\n## Determine global significance using decideTests\nglobal_sig &lt;- decideTests(object = final_model, \n                          adjust.method = \"BH\", \n                          p.value = 0.01, \n                          method = \"global\") %&gt;%\n  as.data.frame() %&gt;% \n  rownames_to_column(\"protein\")\n\n\n## Change column names to avoid conflict when binding\ncolnames(global_sig) &lt;- paste0(\"sig_\", colnames(global_sig))\n\n## Add the results of global significance test to overall linear model results\nlimma_results_all_contrasts &lt;- dplyr::left_join(limma_results_all_contrasts, \n                                  global_sig, \n                                  by = c(\"Protein\" = \"sig_protein\"))\n\n## Verify\nlimma_results_all_contrasts %&gt;% head()\n\n  Protein      G1_M      M_Des     G1_Des    AveExpr        F      P.Value\n1  Q9NQW6 -2.077921  0.8130706 -1.2648507  2.7165663 794.7797 5.148901e-15\n2  Q9ULW0 -2.152503  0.7854924 -1.3670105  2.5730063 685.1572 1.425086e-14\n3  P49454 -1.991326  1.1442558 -0.8470698  2.5489502 598.3086 3.606132e-14\n4  O14965 -2.098029  0.9147903 -1.1832391  0.8426435 551.3458 6.308302e-14\n5  Q562F6 -3.205603  1.4313091 -1.7742942 -1.2469471 546.9525 6.662987e-14\n6  Q15004 -1.355585 -1.0359248 -2.3915102  0.8346698 542.7099 7.027329e-14\n     adj.P.Val sig_G1_M sig_M_Des sig_G1_Des\n1 1.968425e-11       -1         1         -1\n2 2.724051e-11       -1         1         -1\n3 4.477580e-11       -1         1         -1\n4 4.477580e-11       -1         1         -1\n5 4.477580e-11       -1         1         -1\n6 4.477580e-11       -1        -1         -1\n\n\nWe now have three additional column, one per contrast, called sig_G1_M, sig_M_Des, and sig_G1_Des. These columns contain -1, 0 or 1 meaning that the protein is significantly downregulated, non-significant or significantly upregulated in the given contrast.\n\n\n11.2.4 Visualising the protein abundances in a heatmap\n\n## Extract accessions of significant proteins\n\n# Summarise how often each protein passes the significance threshold\n# across the 3 contrasts\nn_sig &lt;- decideTests(object = final_model, \n                          adjust.method = \"BH\", \n                          p.value = 0.01, \n                          method = \"global\") %&gt;%\n  apply(MARGIN = 1, FUN = function(x) sum(x != 0))\n\n# Identify the proteins significant at least once\nsig_proteins &lt;- names(n_sig)[n_sig &gt; 0]  \n\n## Subset quantitative data corresponding to significant proteins\nquant_data &lt;- cc_qf[[\"log_norm_proteins\"]][sig_proteins, ] %&gt;%\n  assay() \n\nNow we use the quantitative data to plot a heatmap using pheatmap.\n\npheatmap(mat = quant_data,\n         scale = 'row', # Z-score normalise across the rows (proteins)\n         show_rownames = FALSE) # Too many proteins to show all their names!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nThe limma package provides a statistical pipeline for the analysis of differential expression (abundance) experiments and can be used for categorical variables with more than two levels.\nWhen performing multiple contrasts, the p-value adjustment for multiple testing should take this into account. decideTests with method = \"global\" can be used to this end.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Extended materials",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Statistical analysis of all cell cycle stages</span>"
    ]
  },
  {
    "objectID": "materials/11_statistical_analysis_all_stages.html#references",
    "href": "materials/11_statistical_analysis_all_stages.html#references",
    "title": "11  Statistical analysis of all cell cycle stages",
    "section": "References",
    "text": "References\n\n\n\n\nSmyth, Gordon K. 2004. “Linear Models and Empirical Bayes Methods for Assessing Differential Expression in Microarray Experiments.” Statistical Applications in Genetics and Molecular Biology 3 (1): 1–25. https://doi.org/10.2202/1544-6115.1027.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Extended materials",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Statistical analysis of all cell cycle stages</span>"
    ]
  },
  {
    "objectID": "versions.html",
    "href": "versions.html",
    "title": "Archived Versions",
    "section": "",
    "text": "Select a version to view the course materials as they were at that time.\nEach version represents a snapshot of the course materials at a specific point in time. This allows participants to access the exact version of the materials they used during their course, even as the content continues to evolve.\nThe latest version always contains the most up-to-date content and improvements.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='Cambridge Centre for Proteomics GitHub' >}}",
      "Appendices",
      "Archived Versions"
    ]
  }
]